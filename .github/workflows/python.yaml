name: Python CI

on:
  push:
    branches:
      - '*'
  pull_request:
    types: ['opened', 'reopened', 'synchronize']

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  lint:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ "ubuntu-latest" ]
        python-version: [ "3.10" ]
    steps:
      - name: Check out code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
          submodules: recursive
      - name: Set up Python environment
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
      - name: flake8 Lint
        uses: py-actions/flake8@v2
        with:
          path: "xinference"
          args: "--config setup.cfg"
      - name: black
        uses: psf/black@stable
        with:
          src: "xinference"
          options: "--check"
          version: "25.1.0"
      - uses: isort/isort-action@master
        with:
          sortPaths: "xinference"
          configuration: "--check-only --diff --sp setup.cfg"
      - name: mypy
        run: pip install 'mypy==1.18.1' && mypy --install-types --non-interactive xinference
      - name: codespell
        run: pip install codespell && codespell --ignore-words-list thirdparty xinference
      - name: Set up Node.js
        uses: actions/setup-node@v1
        with:
          node-version: 16
      # ESLint and Prettier must be in `package.json`
      - name: Install Node.js dependencies
        run: cd xinference/ui/web/ui && npm ci
      - name: ESLint Check
        run: cd xinference/ui/web/ui && npx eslint .
      - name: Prettier Check
        run: cd xinference/ui/web/ui && ./node_modules/.bin/prettier --check .

  build_test_job:
    runs-on: ${{ matrix.os }}
    needs: lint
    env:
      CONDA_ENV: test
      SELF_HOST_PYTHON: /root/miniconda3/envs/inference_test/bin/python
      SELF_HOST_CONDA: /root/miniconda3/condabin/conda
    defaults:
      run:
        shell: bash -l {0}
    strategy:
      fail-fast: false
      max-parallel: 2
      matrix:
        os: [ "ubuntu-latest", "macos-latest", "windows-latest" ]
        python-version: [ "3.10", "3.11", "3.12", "3.13" ]
        module: [ "xinference" ]
        exclude:
          - { os: macos-latest, python-version: 3.11 }
          - { os: macos-latest, python-version: 3.12 }
          - { os: windows-latest, python-version: 3.11 }
          - { os: windows-latest, python-version: 3.12 }
        include:
          - { os: self-hosted, module: gpu, python-version: "3.11"}
          - { os: macos-latest, module: metal, python-version: "3.10" }

    steps:
      - name: Check out code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
          submodules: recursive

      - name: Set up conda ${{ matrix.python-version }}
        uses: conda-incubator/setup-miniconda@v3
        if: ${{ matrix.module != 'gpu' }}
        with:
          python-version: ${{ matrix.python-version }}
          activate-environment: ${{ env.CONDA_ENV }}

      # Important for python == 3.12 and 3.13
      - name: Update pip and setuptools
        if: ${{ matrix.python-version == '3.12' || matrix.python-version == '3.13' }}
        run: |
          python -m pip install -U pip "setuptools<82"

      # Install torch for Python 3.13 using nightly builds
      - name: Install torch for Python 3.13
        if: ${{ matrix.python-version == '3.13'}}
        run: |
          python -m pip install torch torchvision torchaudio

      - name: Install numpy
        if: |
          (startsWith(matrix.os, 'macos') && (matrix.python-version == '3.13')) || 
          (startsWith(matrix.os, 'windows'))
        run: |
          python -m pip install "numpy<2"

      - name: Install dependencies
        env:
          MODULE: ${{ matrix.module }}
          OS: ${{ matrix.os }}
        if: ${{ matrix.module != 'gpu' }}
        run: |
          if [ "$OS" == "ubuntu-latest" ]; then
            sudo rm -rf /usr/share/dotnet
            sudo rm -rf /opt/ghc
            sudo rm -rf "/usr/local/share/boost"
            sudo rm -rf "$AGENT_TOOLSDIRECTORY"
          fi
          pip install -e ".[dev]"
          pip install "xllamacpp>=0.2.0"
          if [ "$MODULE" == "metal" ]; then
            conda install -c conda-forge "ffmpeg<7"
            pip install "mlx>=0.22.0"
            pip install mlx-lm
            pip install "mlx-vlm>=0.3.4"
            pip install mlx-whisper
            pip install f5-tts-mlx
            pip install qwen-vl-utils!=0.0.9
            pip install tomli 
          else
            pip install "transformers<4.49"
            pip install attrdict
            pip install "timm>=0.9.16"
            if [ "${{ matrix.python-version }}" != "3.13" ]; then
              pip install torch torchvision
            fi
            pip install accelerate
            pip install sentencepiece
            pip install transformers_stream_generator
            pip install bitsandbytes
            pip install "sentence-transformers>=5.1.1"
            pip install modelscope
            pip install diffusers
            pip install protobuf
            pip install FlagEmbedding
            pip install "tenacity>=8.2.0,<8.4.0"
            pip install "jinja2==3.1.2"
            pip install jj-pytorchvideo
            pip install qwen-vl-utils!=0.0.9
            pip install datamodel_code_generator
            pip install jsonschema
          fi
        working-directory: .

      - name: Clean up disk
        if: |
          (startsWith(matrix.os, 'ubuntu'))
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          sudo apt-get clean
          sudo rm -rf /var/lib/apt/lists/*
          df -h 

      - name: Test with pytest
        env:
          MODULE: ${{ matrix.module }}
          PYTORCH_MPS_HIGH_WATERMARK_RATIO: 1.0
          PYTORCH_MPS_LOW_WATERMARK_RATIO: 0.2
          XFORMERS_FORCE_DISABLE_TRITON: 1
          TORCH_DISABLE_FLASH_ATTENTION: 1
        run: |
          if [ "$MODULE" == "gpu" ]; then
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U -e ".[audio,dev]"
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U "openai>1"
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U modelscope
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U gguf
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U uv
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U sse_starlette
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U xoscar
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U "python-jose[cryptography]"
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U "passlib[bcrypt]"
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U "aioprometheus[starlette]"
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U "pynvml"
            ${{ env.SELF_HOST_PYTHON }} -m pip install "transformers==4.53.2"
            ${{ env.SELF_HOST_PYTHON }} -m pip install "funasr==1.2.7"
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U nemo_text_processing<1.1.0
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U omegaconf~=2.3.0
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U WeTextProcessing<1.0.4
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U librosa
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U xxhash
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U "ChatTTS>=0.2.1"
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U HyperPyYAML
            ${{ env.SELF_HOST_PYTHON }} -m pip uninstall -y matcha-tts
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U onnxruntime-gpu==1.16.0; sys_platform == 'linux'
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U openai-whisper
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U "torch==2.7.0" "torchaudio==2.7.0" "torchvision==0.22.0"
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U "loguru"
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U "natsort"
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U "loralib"
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U "ormsgpack"
            ${{ env.SELF_HOST_PYTHON }} -m pip uninstall -y opencc
            ${{ env.SELF_HOST_PYTHON }} -m pip uninstall -y "faster_whisper"
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U accelerate
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U verovio
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U cachetools
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U silero-vad
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U pydantic
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U diffusers
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U onnx
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U onnxconverter_common
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U torchdiffeq
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U "x_transformers>=1.31.14"
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U pypinyin
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U tomli
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U vocos
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U jieba
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U soundfile
            ${{ env.SELF_HOST_PYTHON }} -m pip install tensorizer
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U sentence-transformers
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U FlagEmbedding
            ${{ env.SELF_HOST_PYTHON }} -m pip install -U "peft<=0.17.1"
            ${{ env.SELF_HOST_PYTHON }} -m pip install "xllamacpp>=0.2.0" --index-url https://xorbitsai.github.io/xllamacpp/whl/cu124 --extra-index-url https://pypi.org/simple
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              --disable-warnings \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/core/tests/test_continuous_batching.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/embedding/tests/test_qwen3_vl_engine_params.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/rerank/tests/test_qwen3_vl_reranker_virtualenv.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/image/tests/test_stable_diffusion.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/image/tests/test_got_ocr2.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/audio/tests/test_whisper.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/audio/tests/test_funasr.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/audio/tests/test_chattts.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/audio/tests/test_cosyvoice.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/audio/tests/test_f5tts.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/audio/tests/test_f5tts.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/audio/tests/test_melotts.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/audio/tests/test_kokoro.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/audio/tests/test_fish_speech.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/audio/tests/test_megatts.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/embedding/tests/test_integrated_embedding.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/embedding/vllm/tests/test_vllm_embedding.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/llm/transformers/tests/test_tensorizer.py && \
            ${{ env.SELF_HOST_PYTHON }} -m pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/llm/tests/test_llm_model.py
          elif [ "$MODULE" == "metal" ]; then
            pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/llm/mlx/tests/test_mlx.py && \
            pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/audio/tests/test_whisper_mlx.py && \
            pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/audio/tests/test_f5tts_mlx.py && \
            pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              --cov-config=setup.cfg --cov-report=xml --cov=xinference xinference/model/llm/mlx/tests/test_distributed_model.py
          else
            pytest --timeout=3000 \
              -W ignore::PendingDeprecationWarning \
              -vv \
              --cov-config=setup.cfg \
              --cov-report=xml \
              --cov=xinference \
              --ignore xinference/core/tests/test_continuous_batching.py \
              --ignore xinference/model/image/tests/test_stable_diffusion.py \
              --ignore xinference/model/image/tests/test_got_ocr2.py \
              --ignore xinference/model/audio/tests \
              --ignore xinference/model/embedding/tests/test_integrated_embedding.py \
              --ignore xinference/model/llm/transformers/tests/test_tensorizer.py \
              --ignore xinference/model/llm/tests/test_llm_model.py \
              --ignore xinference/model/llm/vllm \
              --ignore xinference/model/llm/sglang \
              --ignore xinference/client/tests/test_client.py \
              --ignore xinference/client/tests/test_async_client.py \
              --ignore xinference/model/llm/mlx \
              xinference
          
          fi
        working-directory: .
