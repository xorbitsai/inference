# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-01-21 14:27+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.14.0\n"

#: ../../source/getting_started/using_xinference.rst:5
msgid "Using Xinference"
msgstr "使用"

#: ../../source/getting_started/using_xinference.rst:9
msgid "Run Xinference Locally"
msgstr "本地运行 Xinference"

#: ../../source/getting_started/using_xinference.rst:11
msgid ""
"Let's start by running Xinference on a local machine and running a "
"classic LLM model: ``qwen2.5-instruct``."
msgstr ""
"让我们以一个经典的大语言模型 ``qwen2.5-instruct`` 来展示如何在本地用 "
"Xinference 运行大模型。"

#: ../../source/getting_started/using_xinference.rst:13
msgid ""
"After this quickstart, you will move on to learning how to deploy "
"Xinference in a cluster environment."
msgstr ""
"在这个快速入门之后，可以继续学习如何在一个分布式集群环境下部署 Xinference"
"。"

#: ../../source/getting_started/using_xinference.rst:16
msgid "Start Local Server"
msgstr "拉起本地服务"

#: ../../source/getting_started/using_xinference.rst:18
msgid ""
"First, please ensure that you have installed Xinference according to the "
"instructions provided :ref:`here <installation>`. To start a local "
"instance of Xinference, run the following command:"
msgstr ""
"首先，请根据这个 :ref:`文档 <installation>` 的指导确保本地安装了 "
"Xinference。使用以下命令拉起本地的 Xinference 服务："

#: ../../source/getting_started/using_xinference.rst:23
#: ../../source/getting_started/using_xinference.rst:65
msgid "shell"
msgstr "shell"

#: ../../source/getting_started/using_xinference.rst:29
#: ../../source/getting_started/using_xinference.rst:71
msgid "output"
msgstr "输出"

#: ../../source/getting_started/using_xinference.rst:39
msgid ""
"By default, Xinference uses ``<HOME>/.xinference`` as home path to store "
"necessary files such as logs and models, where ``<HOME>`` is the home "
"path of current user."
msgstr ""
"默认情况下，Xinference 会使用 ``<HOME>/.xinference`` 作为主目录来存储一些"
"必要的信息，比如日志文件和模型文件，其中 ``<HOME>`` 就是当前用户的主目录"
"。"

#: ../../source/getting_started/using_xinference.rst:42
msgid ""
"You can change this directory by configuring the environment variable "
"``XINFERENCE_HOME``. For example:"
msgstr "你可以通过配置环境变量 ``XINFERENCE_HOME`` 修改主目录， 比如："

#: ../../source/getting_started/using_xinference.rst:49
msgid ""
"Congrats! You now have Xinference running on your local machine. Once "
"Xinference is running, there are multiple ways we can try it: via the web"
" UI, via cURL, via the command line, or via the Xinference's python "
"client."
msgstr ""
"恭喜！你已经在本地拉起了 Xinference 服务。一旦 Xinference 服务运行起来，"
"可以有多种方式来使用，包括使用网页、cURL 命令、命令行或者是 Xinference 的"
" Python SDK。"

#: ../../source/getting_started/using_xinference.rst:52
msgid ""
"You can visit the web UI at `http://127.0.0.1:9997/ui "
"<http://127.0.0.1:9997/ui>`_ and visit `http://127.0.0.1:9997/docs "
"<http://127.0.0.1:9997/docs>`_ to inspect the API docs."
msgstr ""
"可以通过访问 `http://127.0.0.1:9997/ui <http://127.0.0.1:9997/ui>`_ 来"
"使用 UI，访问 `http://127.0.0.1:9997/docs <http://127.0.0.1:9997/docs>`_ "
"来查看 API 文档。"

#: ../../source/getting_started/using_xinference.rst:55
msgid ""
"You can install the Xinference command line tool and Python client using "
"the following command:"
msgstr "可以通过以下命令安装后，利用 Xinference 命令行工具或者 Python 代码来使用："

#: ../../source/getting_started/using_xinference.rst:61
msgid ""
"The command line tool is ``xinference``. You can list the commands that "
"can be used by running:"
msgstr "命令行工具是 ``xinference``。可以通过以下命令查看有哪些可以使用的命令："

#: ../../source/getting_started/using_xinference.rst:102
msgid ""
"You can install the Xinference Python client with minimal dependencies "
"using the following command. Please ensure that the version of the client"
" matches the version of the Xinference server."
msgstr ""
"如果只需要安装 Xinference 的 Python SDK，可以使用以下命令安装最少依赖。"
"需要注意的是版本必须和 Xinference 服务的版本保持匹配。"

#: ../../source/getting_started/using_xinference.rst:112
msgid "About Model Engine"
msgstr "关于模型的推理引擎"

#: ../../source/getting_started/using_xinference.rst:113
msgid ""
"Since ``v0.11.0`` , before launching the LLM model, you need to specify "
"the inference engine you want to run. Currently, xinference supports the "
"following inference engines:"
msgstr ""
"自 ``v0.11.0`` 版本开始，在加载 LLM 模型之前，你需要指定具体的推理引擎。"
"当前，Xinference 支持以下推理引擎："

#: ../../source/getting_started/using_xinference.rst:116
msgid "``vllm``"
msgstr ""

#: ../../source/getting_started/using_xinference.rst:117
msgid "``sglang``"
msgstr ""

#: ../../source/getting_started/using_xinference.rst:118
msgid "``llama.cpp``"
msgstr ""

#: ../../source/getting_started/using_xinference.rst:119
msgid "``transformers``"
msgstr ""

#: ../../source/getting_started/using_xinference.rst:120
msgid "``MLX``"
msgstr ""

#: ../../source/getting_started/using_xinference.rst:122
msgid ""
"About the details of these inference engine, please refer to :ref:`here "
"<inference_backend>`."
msgstr "关于这些推理引擎的详细信息，请参考 :ref:`这里 <inference_backend>` 。"

#: ../../source/getting_started/using_xinference.rst:124
msgid ""
"Note that when launching a LLM model, the ``model_format`` and "
"``quantization`` of the model you want to launch is closely related to "
"the inference engine."
msgstr ""
"注意，当加载 LLM 模型时，所能运行的引擎与 ``model_format`` 和 ``"
"quantization`` 参数息息相关。"

#: ../../source/getting_started/using_xinference.rst:127
msgid ""
"You can use ``xinference engine`` command to query the combination of "
"parameters of the model you want to launch. This will demonstrate under "
"what conditions a model can run on which inference engines."
msgstr "Xinference 提供了 ``xinference engine`` 命令帮助你查询相关的参数组合。"

#: ../../source/getting_started/using_xinference.rst:130
msgid "For example:"
msgstr "例如："

#: ../../source/getting_started/using_xinference.rst:132
msgid ""
"I would like to query about which inference engines the ``qwen-chat`` "
"model can run on, and what are their respective parameters."
msgstr ""
"我想查询与 ``qwen-chat`` 模型相关的参数组合，以决定它能够怎样跑在各种推理"
"引擎上。"

#: ../../source/getting_started/using_xinference.rst:138
msgid ""
"I want to run ``qwen-chat`` with ``VLLM`` as the inference engine, but I "
"don't know how to configure the other parameters."
msgstr ""
"我想将 ``qwen-chat`` 跑在 ``VLLM`` 推理引擎上，但是我不知道什么样的其他"
"参数符合这个要求。"

#: ../../source/getting_started/using_xinference.rst:144
msgid ""
"I want to launch the ``qwen-chat`` model in the ``GGUF`` format, and I "
"need to know how to configure the remaining parameters."
msgstr "我想加载 ``GGUF`` 格式的 ``qwen-chat`` 模型，我需要知道其余的参数组合。"

#: ../../source/getting_started/using_xinference.rst:151
msgid ""
"In summary, compared to previous versions, when launching LLM models, you"
" need to additionally pass the ``model_engine`` parameter. You can "
"retrieve information about the supported inference engines and their "
"related parameter combinations through the ``xinference engine`` command."
msgstr ""
"总之，相比于之前的版本，当加载 LLM 模型时，需要额外传入 ``model_engine`` "
"参数。你可以通过 ``xinference engine`` 命令查询你想运行的推理引擎与其他"
"参数组合的关系。"

#: ../../source/getting_started/using_xinference.rst:158
msgid "Here are some recommendations on when to use which engine:"
msgstr ""
"关于何时使用什么引擎，以下是一些建议："

#: ../../source/getting_started/using_xinference.rst:160
msgid "**Linux**"
msgstr ""

#: ../../source/getting_started/using_xinference.rst:162
msgid ""
"When possible, prioritize using **vLLM** or **SGLang** for better "
"performance."
msgstr ""
"在能使用的情况下，优先使用 **vLLM** 或 **SGLang**，因为他们有更好的性能。"

#: ../../source/getting_started/using_xinference.rst:163
msgid ""
"If resources are limited, consider using **llama.cpp**, as it offers more"
" quantization options."
msgstr ""
"如果资源有限，可以考虑使用 **llama.cpp**，因为他提供了更多的量化选项。"

#: ../../source/getting_started/using_xinference.rst:164
msgid ""
"For other cases, consider using **Transformers**, which supports nearly "
"all models."
msgstr ""
"其他使用考虑使用 **Transformers**，它几乎支持所有的模型。"

#: ../../source/getting_started/using_xinference.rst:166
msgid "**Windows**"
msgstr ""

#: ../../source/getting_started/using_xinference.rst:168
msgid ""
"It is recommended to use **WSL**, and in this case, follow the same "
"choices as Linux."
msgstr ""
"推荐使用 **WSL**，这个时候选择和 Linux 一致。"

#: ../../source/getting_started/using_xinference.rst:169
msgid ""
"Otherwise, prefer **llama.cpp**, and for unsupported models, opt for "
"**Transformers**."
msgstr ""
"其他时候推荐使用 **llama.cpp**，对于不支持的模型，选择使用 **Transformers**。"

#: ../../source/getting_started/using_xinference.rst:171
msgid "**Mac**"
msgstr ""

#: ../../source/getting_started/using_xinference.rst:173
msgid ""
"If supported by the model, use the **MLX engine**, as it delivers the "
"best performance."
msgstr ""
"在模型支持的情况下，推荐使用 **MLX 引擎**，它有着最好的性能"

#: ../../source/getting_started/using_xinference.rst:174
msgid ""
"For other cases, prefer **llama.cpp**, and for unsupported models, choose"
" **Transformers**."
msgstr ""
"其他时候推荐使用 **llama.cpp**，对于不支持的模型，选择使用 **Transformers**。"

#: ../../source/getting_started/using_xinference.rst:178
msgid "Run qwen2.5-instruct"
msgstr "运行 qwen2.5-instruct"

#: ../../source/getting_started/using_xinference.rst:180
msgid ""
"Let's start by running a built-in model: ``qwen2.5-instruct``. When you "
"start a model for the first time, Xinference will download the model "
"parameters from HuggingFace, which might take a few minutes depending on "
"the size of the model weights. We cache the model files locally, so "
"there's no need to redownload them for subsequent starts."
msgstr ""
"让我们来运行一个内置的 ``qwen2.5-instruct`` 模型。当你需要运行一个模型时，"
"第一次运行是要从HuggingFace 下载模型参数，一般来说需要根据模型大小下载10"
"到30分钟不等。当下载完成后，Xinference本地会有缓存的处理，以后再运行相同"
"的模型不需要重新下载。"

#: ../../source/getting_started/using_xinference.rst:185
msgid ""
"Xinference also allows you to download models from other sites. You can "
"do this by setting an environment variable when launching Xinference. For"
" example, if you want to download models from `modelscope "
"<https://modelscope.cn>`_, do the following:"
msgstr ""
"Xinference 也允许从其他模型托管平台下载模型。可以通过在拉起 Xinference 时"
"指定环境变量，比如，如果想要从 ModelScope 中下载模型，可以使用如下命令："

#: ../../source/getting_started/using_xinference.rst:193
msgid ""
"We can specify the model's UID using the ``--model-uid`` or ``-u`` flag. "
"If not specified, Xinference will generate a unique ID. The default "
"unique ID will be identical to the model name."
msgstr ""
"可以使用 ``--model-uid`` 或者 ``-u`` 参数指定模型的 UID，如果没有指定，"
"Xinference 会随机生成一个 ID。默认的 ID 和模型名保持一致。"
"``:"

#: ../../source/getting_started/using_xinference.rst:232
msgid ""
"For some engines, such as vllm, users need to specify the engine-related "
"parameters when running models. In this case, you can directly specify "
"the parameter name and value in the command line, for example:"
msgstr ""
"对于一些推理引擎，比如 vllm，用户需要在运行模型时指定引擎相关的参数，这种"
"情况下直接在命令行中指定对应的参数名和值即可，比如："

#: ../../source/getting_started/using_xinference.rst:240
msgid "`gpu_memory_utilization=0.9` will pass to vllm when launching model."
msgstr "在运行模型时，`gpu_memory_utilization=0.9` 会传到 vllm 后端。"

#: ../../source/getting_started/using_xinference.rst:242
msgid ""
"Congrats! You now have ``qwen2.5-instruct`` running by Xinference. Once "
"the model is running, we can try it out either via cURL, or via "
"Xinference's python client:"
msgstr ""
"到这一步，恭喜你已经成功通过 Xinference 将 ``qwen2.5-instruct`` 运行起来了。"
"一旦这个模型在运行中，我们可以通过命令行、cURL 或者是 Python 代码来预支"
"交互："

#: ../../source/getting_started/using_xinference.rst:302
msgid ""
"Xinference provides OpenAI-compatible APIs for its supported models, so "
"you can use Xinference as a local drop-in replacement for OpenAI APIs. "
"For example:"
msgstr ""
"Xinference 提供了与 OpenAI 兼容的 API，所以可以将 Xinference 运行的模型"
"当成 OpenAI的本地替代。比如："

#: ../../source/getting_started/using_xinference.rst:318
msgid "The following OpenAI APIs are supported:"
msgstr "以下是支持的 OpenAI 的 API："

#: ../../source/getting_started/using_xinference.rst:320
msgid ""
"Chat Completions: `https://platform.openai.com/docs/api-reference/chat "
"<https://platform.openai.com/docs/api-reference/chat>`_"
msgstr ""
"对话生成：`https://platform.openai.com/docs/api-reference/chat <https://"
"platform.openai.com/docs/api-reference/chat>`_"

#: ../../source/getting_started/using_xinference.rst:322
msgid ""
"Completions: `https://platform.openai.com/docs/api-reference/completions "
"<https://platform.openai.com/docs/api-reference/completions>`_"
msgstr ""
"生成: `https://platform.openai.com/docs/api-reference/completions <https:"
"//platform.openai.com/docs/api-reference/completions>`_"

#: ../../source/getting_started/using_xinference.rst:324
msgid ""
"Embeddings: `https://platform.openai.com/docs/api-reference/embeddings "
"<https://platform.openai.com/docs/api-reference/embeddings>`_"
msgstr ""
"向量生成：`https://platform.openai.com/docs/api-reference/embeddings <"
"https://platform.openai.com/docs/api-reference/embeddings>`_"

#: ../../source/getting_started/using_xinference.rst:327
msgid "Manage Models"
msgstr "管理模型"

#: ../../source/getting_started/using_xinference.rst:329
msgid ""
"In addition to launching models, Xinference offers various ways to manage"
" the entire lifecycle of models. You can manage models in Xinference "
"through the command line, cURL, or Xinference's python client."
msgstr ""
"除了启动模型，Xinference 提供了管理模型整个生命周期的能力。同样的，你可以"
"使用命令行、cURL 以及 Python 代码来管理："

#: ../../source/getting_started/using_xinference.rst:332
msgid ""
"You can list all models of a certain type that are available to launch in"
" Xinference:"
msgstr "可以列出所有 Xinference 支持的指定类型的模型："

#: ../../source/getting_started/using_xinference.rst:350
msgid ""
"The following command gives you the currently running models in "
"Xinference:"
msgstr "接下来的命令可以列出所有在运行的模型："

#: ../../source/getting_started/using_xinference.rst:368
msgid ""
"When you no longer need a model that is currently running, you can remove"
" it in the following way to free up the resources it occupies:"
msgstr "当你不需要某个正在运行的模型，可以通过以下的方式来停止它并释放资源："

#: ../../source/getting_started/using_xinference.rst:387
msgid "Deploy Xinference In a Cluster"
msgstr "集群中部署 Xinference"

#: ../../source/getting_started/using_xinference.rst:389
msgid ""
"To deploy Xinference in a cluster, you need to start a Xinference "
"supervisor on one server and Xinference workers on the other servers."
msgstr ""
"若要在集群环境中部署 Xinference，需要在一台机器中启动 supervisor 节点，并"
"在当前或者其他节点启动 worker 节点"

#: ../../source/getting_started/using_xinference.rst:392
msgid ""
"First, make sure you have already installed Xinference on each of the "
"servers according to the instructions provided :ref:`here "
"<installation>`. Then follow the steps below:"
msgstr ""
"首先，根据 :ref:`文档 <installation>` 确保所有的服务器上都安装了 "
"Xinference。接下来按照步骤："

#: ../../source/getting_started/using_xinference.rst:396
msgid "Start the Supervisor"
msgstr "启动 Supervisor"

#: ../../source/getting_started/using_xinference.rst:397
msgid ""
"On the server where you want to run the Xinference supervisor, run the "
"following command:"
msgstr "在服务器上执行以下命令来启动 Supervisor 节点："

#: ../../source/getting_started/using_xinference.rst:403
msgid ""
"Replace ``${supervisor_host}`` with the actual host of your supervisor "
"server."
msgstr "用当前节点的 IP 来替换 ``${supervisor_host}``。"

#: ../../source/getting_started/using_xinference.rst:406
msgid ""
"You can the supervisor's web UI at `http://${supervisor_host}:9997/ui "
"<http://${supervisor_host}:9997/ui>`_ and visit "
"`http://${supervisor_host}:9997/docs "
"<http://${supervisor_host}:9997/docs>`_ to inspect the API docs."
msgstr ""
"可以在 `http://${supervisor_host}:9997/ui <http://${supervisor_host}:9997"
"/ui>`_ 访问 web UI，在 `http://${supervisor_host}:9997/docs <http://${"
"supervisor_host}:9997/docs>`_ 访问 API 文档。"

#: ../../source/getting_started/using_xinference.rst:410
msgid "Start the Workers"
msgstr "启动 Worker"

#: ../../source/getting_started/using_xinference.rst:412
msgid ""
"On each of the other servers where you want to run Xinference workers, "
"run the following command:"
msgstr "在需要启动 Xinference worker 的机器上执行以下命令："

#: ../../source/getting_started/using_xinference.rst:419
msgid ""
"Note that you must replace ``${worker_host}``  with the actual host of "
"your worker server."
msgstr "需要注意的是，必须使用当前Worker节点的 IP 来替换 ``${worker_host}``。"

#: ../../source/getting_started/using_xinference.rst:422
msgid ""
"Note that if you need to interact with the Xinference in a cluster via "
"the command line, you should include the ``-e`` or ``--endpoint`` flag to"
" specify the supervisor server's endpoint. For example:"
msgstr ""
"需要注意的是，如果你需要通过命令行与集群交互，应该通过 ``-e`` 或者 ``--"
"endpoint`` 参数来指定 supervisor 的地址，比如："

#: ../../source/getting_started/using_xinference.rst:430
msgid "Using Xinference With Docker"
msgstr "使用 Docker 部署 Xinference"

#: ../../source/getting_started/using_xinference.rst:432
msgid "To start Xinference in a Docker container, run the following command:"
msgstr "用以下命令在容器中运行 Xinference："

#: ../../source/getting_started/using_xinference.rst:435
msgid "Run On Nvidia GPU Host"
msgstr "在拥有英伟达显卡的机器上运行"

#: ../../source/getting_started/using_xinference.rst:442
msgid "Run On CPU Only Host"
msgstr "在只有 CPU 的机器上运行"

#: ../../source/getting_started/using_xinference.rst:448
msgid ""
"Replace ``<your_version>`` with Xinference versions, e.g. ``v0.10.3``, "
"``latest`` can be used for the latest version."
msgstr ""
"将 ``<your_version>`` 替换为 Xinference 的版本，比如 ``v0.10.3``，可以用 "
"``latest`` 来用于最新版本。"

#: ../../source/getting_started/using_xinference.rst:450
msgid ""
"For more docker usage, refer to :ref:`Using Docker Image "
"<using_docker_image>`."
msgstr "更多 docker 使用，请参考 :ref:`使用 docker 镜像 <using_docker_image>`。"

#: ../../source/getting_started/using_xinference.rst:454
msgid "What's Next?"
msgstr "更多"

#: ../../source/getting_started/using_xinference.rst:456
msgid ""
"Congratulations on getting started with Xinference! To help you navigate "
"and make the most out of this powerful tool, here are some resources and "
"guides:"
msgstr ""
"恭喜你，已经初步掌握了 Xinference 的用法！为了帮助你更好地使用工具，下面"
"是其他的一些文档和指导资源："

#: ../../source/getting_started/using_xinference.rst:459
msgid ""
":ref:`How to Use Client APIs for Different Types of Models "
"<user_guide_client_api>`"
msgstr ":ref:`如何使用 Python 创建不同类型的模型 <user_guide_client_api>`"

#: ../../source/getting_started/using_xinference.rst:461
msgid ":ref:`Choosing the Right Backends for Your Needs <user_guide_backends>`"
msgstr ":ref:`选择正确的推理引擎 <user_guide_backends>` "

#~ msgid "Run Llama-2"
#~ msgstr "运行 Llama-2"
