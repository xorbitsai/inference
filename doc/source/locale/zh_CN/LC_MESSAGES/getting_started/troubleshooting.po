# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-10 12:40+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.14.0\n"

#: ../../source/getting_started/troubleshooting.rst:5
msgid "Troubleshooting"
msgstr "故障排除"

#: ../../source/getting_started/troubleshooting.rst:9
msgid "No huggingface repo access"
msgstr "没有 huggingface 仓库权限"

#: ../../source/getting_started/troubleshooting.rst:11
msgid ""
"Sometimes, you may face errors accessing huggingface models, such as the "
"following message when accessing `llama2`:"
msgstr ""
"获取模型时，有时候会遇到权限问题。比如在获取 ``llama2`` 模型时可能会有"
"以下提示："

#: ../../source/getting_started/troubleshooting.rst:18
msgid ""
"This typically indicates either a lack of access rights to the repository"
" or missing huggingface access tokens. The following sections provide "
"guidance on addressing these issues."
msgstr ""
"这种情况一般是缺少 huggingface 仓库的权限，或者是没有配置 huggingface "
"token。可以按照接下来的方式解决这个问题。"

#: ../../source/getting_started/troubleshooting.rst:22
msgid "Get access to the huggingface repo"
msgstr "申请 huggingface 仓库权限"

#: ../../source/getting_started/troubleshooting.rst:24
msgid ""
"To obtain access, navigate to the desired huggingface repository and "
"agree to its terms and conditions. As an illustration, for the `llama2` "
"model, you can use this link: `https://huggingface.co/meta-llama/Llama-2"
"-7b-hf <https://huggingface.co/meta-llama/Llama-2-7b-hf>`_."
msgstr ""
"想要获取访问权限，打开对应的 huggingface 仓库，同意其条款和注意事项。以 `"
"`llama2`` 为例，可以打开这个链接去申请：`https://huggingface.co/meta-"
"llama/Llama-2-7b-hf <https://huggingface.co/meta-llama/Llama-2-7b-hf>`_."

#: ../../source/getting_started/troubleshooting.rst:29
msgid "Set up credentials to access huggingface"
msgstr "设置访问 huggingface 凭证"

#: ../../source/getting_started/troubleshooting.rst:31
msgid ""
"Your credential to access huggingface can be found online at "
"`https://huggingface.co/settings/tokens "
"<https://huggingface.co/settings/tokens>`_."
msgstr ""
"可以在 huggingface 页面找到凭证，`https://huggingface.co/settings/tokens "
"<https://huggingface.co/settings/tokens>`_."

#: ../../source/getting_started/troubleshooting.rst:33
msgid ""
"You can set the token as an environmental variable, with ``export "
"HUGGING_FACE_HUB_TOKEN=your_token_here``."
msgstr ""
"可以通过设置环境变量设置访问凭证，``export HUGGING_FACE_HUB_TOKEN=your_"
"token_here``。"

#: ../../source/getting_started/troubleshooting.rst:37
msgid "Incompatibility Between NVIDIA Driver and PyTorch Version"
msgstr "英伟达驱动和 PyTorch 版本不匹配"

#: ../../source/getting_started/troubleshooting.rst:39
msgid "If you are using a NVIDIA GPU, you may face the following error:"
msgstr "如果你在使用英伟达显卡，你可能会遇到以下错误："

#: ../../source/getting_started/troubleshooting.rst:50
msgid ""
"This typically indicates that your CUDA driver version is not compatible "
"with the PyTorch version you are using."
msgstr "这种情况一般是 CUDA 的版本和 Pytorch 版本不兼容导致的。"

#: ../../source/getting_started/troubleshooting.rst:52
msgid ""
"Go to `https://pytorch.org <https://pytorch.org>`_ to install a PyTorch "
"version that has been compiled with your version of the CUDA driver. **Do"
" not install a cuda version smaller than 11.8, preferably between 11.8 "
"and 12.1.**"
msgstr ""
"可以到 `https://pytorch.org <https://pytorch.org>`_ 官网安装和 CUDA 对应"
"的预编译版本的 PyTorch。同时，**请检查安装的 CUDA 版本不要小于 11.8，最好"
"版本在 11.8 到 12.1之间。**"

#: ../../source/getting_started/troubleshooting.rst:55
msgid ""
"Say if your CUDA driver version is 11.8, then you can install PyTorch "
"with the following command:"
msgstr "比如你的 CUDA 版本是 11.8，可以使用以下命令安装对应的 PyTorch："

#: ../../source/getting_started/troubleshooting.rst:63
msgid ""
"Xinference service cannot be accessed from external systems through "
"``<IP>:9997``"
msgstr "外部系统无法通过 ``<IP>:9997`` 访问 Xinference 服务"

#: ../../source/getting_started/troubleshooting.rst:65
msgid "Use ``-H 0.0.0.0`` parameter in when starting Xinference:"
msgstr "在启动 Xinference 时记得要加上 ``-H 0.0.0.0`` 参数:"

#: ../../source/getting_started/troubleshooting.rst:71
msgid ""
"Then Xinference service will listen on all network interfaces (not "
"limited to ``127.0.0.1`` or ``localhost``)."
msgstr ""
"那么 Xinference 服务将监听所有网络接口（而不仅限于 ``127.0.0.1`` 或 ``"
"localhost``）。"

#: ../../source/getting_started/troubleshooting.rst:73
msgid ""
"If you are using the :ref:`using_docker_image`, please add ``-p "
"<PORT>:9997`` during the docker run command, then access is available "
"through ``<IP>:<PORT>`` of the local machine."
msgstr ""
"如果使用的是 :ref:`using_docker_image`，请在 Docker 运行命令中 加上 ``-p "
"<PORT>:9997`` ，，你就可以通过本地机器的 ``<IP>:<PORT>`` 进行访问。"

#: ../../source/getting_started/troubleshooting.rst:78
msgid ""
"Launching a built-in model takes a long time, and sometimes the model "
"fails to download"
msgstr "启动内置模型需要很长时间，模型有时下载失败"

#: ../../source/getting_started/troubleshooting.rst:80
msgid ""
"Xinference by default uses HuggingFace as the source for models. If your "
"machines are in Mainland China, there might be accessibility issues when "
"using built-in models."
msgstr ""
"Xinference 默认使用 HuggingFace作为模型源。如果你的机器在中国大陆，使用"
"内置模型可能会有访问问题。"

#: ../../source/getting_started/troubleshooting.rst:84
msgid ""
"To address this, add environment variable "
"``XINFERENCE_MODEL_SRC=modelscope`` when starting the Xinference to "
"change the model source to ModelScope, which is optimized for Mainland "
"China."
msgstr ""
"要解决这个问题，可以在启动 Xinference 时添加环境变量 ``XINFERENCE_MODEL_"
"SRC=modelscope``，将模型源更改为 ModelScope，在中国大陆速度下载更快。"

#: ../../source/getting_started/troubleshooting.rst:88
msgid ""
"If you’re starting Xinference with Docker, include ``-e XINFERENCE_MODEL"
"_SRC=modelscope`` during the docker run command."
msgstr ""
"如果你用 Docker 启动 Xinference，可以在 Docker 命令中包含 ``-e XINFERENCE"
"_MODEL_SRC=modelscope`` 选项。"

#: ../../source/getting_started/troubleshooting.rst:92
msgid ""
"When using the official Docker image, RayWorkerVllm died due to OOM, "
"causing the model to fail to load"
msgstr "使用官方 Docker 映像时，RayWorkerVllm 因 OOM 而死亡，导致模型无法加载"

#: ../../source/getting_started/troubleshooting.rst:94
msgid ""
"Docker's ``--shm-size`` parameter is used to set the size of shared "
"memory. The default size of shared memory (/dev/shm) is 64MB, which may "
"be too small for vLLM backend."
msgstr ""
"Docker 的 ``--shm-size`` 参数可以用来设置共享内存的大小。共享内存(/dev/"
"shm)的默认大小是 64MB，对于 vLLM 后端来说可能不够。"

#: ../../source/getting_started/troubleshooting.rst:98
msgid ""
"You can increase its size by setting the ``--shm-size`` parameter as "
"follows:"
msgstr "你可以通过设置参数 ``--shm-size`` 来增加它的大小："

#: ../../source/getting_started/troubleshooting.rst:106
msgid "Missing ``model_engine`` parameter when launching LLM models"
msgstr "加载 LLM 模型时提示缺失 ``model_engine`` 参数"

#: ../../source/getting_started/troubleshooting.rst:108
msgid ""
"Since version ``v0.11.0``, launching LLM models requires an additional "
"``model_engine`` parameter. For specific information, please refer to "
":ref:`here <about_model_engine>`."
msgstr ""
"自 ``v0.11.0`` 版本开始，加载 LLM 模型时需要传入额外参数 ``model_engine``"
" 。具体信息请参考 :ref:`这里 <about_model_engine>` 。"

#: ../../source/getting_started/troubleshooting.rst:112
msgid "Resolving MKL Threading Layer Conflicts"
msgstr "解决 MKL 线程层冲突"

#: ../../source/getting_started/troubleshooting.rst:114
msgid ""
"When starting the Xinference server, you may encounter the error: "
"``ValueError: Model architectures ['Qwen2ForCausalLM'] failed to be "
"inspected. Please check the logs for more details.``"
msgstr ""
"在启动 Xinference 服务器时，如果遇到错误：``ValueError: Model "
"architectures ['Qwen2ForCausalLM'] failed to be inspected. . Please check"
" the logs for more details.``"

#: ../../source/getting_started/troubleshooting.rst:116
msgid "The underlying cause shown in the logs is:"
msgstr "日志中显示的根本原因是："

#: ../../source/getting_started/troubleshooting.rst:123
msgid ""
"This typically occurs when NumPy was installed via conda. Conda's NumPy "
"is built with Intel MKL optimizations, which conflicts with the GNU "
"OpenMP library (libgomp) already loaded in your environment."
msgstr ""
"这通常是因为你的 NumPy 是通过 conda 安装的，而 conda 的 NumPy 是使用 "
"Intel MKL 优化构建的，这导致它与环境中已加载的 GNU OpenMP 库（libgomp）"
"产生冲突。"

#: ../../source/getting_started/troubleshooting.rst:126
msgid "Solution 1: Override the Threading Layer"
msgstr "解决方案 1：重写线程层"

#: ../../source/getting_started/troubleshooting.rst:128
msgid "Force Intel's Math Kernel Library to use GNU's OpenMP implementation:"
msgstr ""
"设置 MKL_THREADING_LAYER=GNU 可以强制 Intel 数学核心库（MKL）使用 GNU 的 "
"OpenMP 实现："

#: ../../source/getting_started/troubleshooting.rst:135
msgid "Solution 2: Reinstall NumPy with pip"
msgstr "解决方案 2：使用 pip 重新安装 NumPy"

#: ../../source/getting_started/troubleshooting.rst:137
msgid "Uninstall conda's NumPy and reinstall using pip:"
msgstr "卸载 conda 安装的 numpy，然后使用 pip 重新安装。"

#: ../../source/getting_started/troubleshooting.rst:146
msgid "Related Note: vLLM and PyTorch"
msgstr "相关说明：vLLM 与 PyTorch"

#: ../../source/getting_started/troubleshooting.rst:148
msgid ""
"If you're using vLLM, avoid installing PyTorch with conda. Refer to the "
"official vLLM installation guide for GPU-specific instructions: "
"https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html"
msgstr ""
"如果你在使用 vLLM，请避免通过 conda 安装 PyTorch。有关特定 GPU 的安装说明"
"，请参阅 vLLM 官方安装指南：https://docs.vllm.ai/en/latest/getting_"
"started/installation/gpu.html"

#: ../../source/getting_started/troubleshooting.rst:151
msgid "Configuring PyPI Mirrors to Speed Up Package Installation"
msgstr "配置 PyPI 镜像以加快软件包安装速度"

#: ../../source/getting_started/troubleshooting.rst:153
msgid ""
"If you're in Mainland China, using a PyPI mirror can significantly speed "
"up package installation. Here are some commonly used mirrors:"
msgstr ""
"如果你在中国大陆，使用 PyPI 镜像可以显著加快软件包的安装速度。以下是一些"
"常用的镜像源："

#: ../../source/getting_started/troubleshooting.rst:155
msgid "Tsinghua University: ``https://pypi.tuna.tsinghua.edu.cn/simple``"
msgstr "清华大学镜像：``https://pypi.tuna.tsinghua.edu.cn/simple``"

#: ../../source/getting_started/troubleshooting.rst:156
msgid "Alibaba Cloud: ``https://mirrors.aliyun.com/pypi/simple/``"
msgstr "阿里云镜像：``https://mirrors.aliyun.com/pypi/simple/``"

#: ../../source/getting_started/troubleshooting.rst:157
msgid "Tencent Cloud: ``https://mirrors.cloud.tencent.com/pypi/simple``"
msgstr "腾讯云镜像：``https://mirrors.cloud.tencent.com/pypi/simple``"

#: ../../source/getting_started/troubleshooting.rst:159
msgid ""
"However, be aware that some packages may not be available on certain "
"mirrors. For example, if you're installing ``xinference[audio]`` using "
"only the Aliyun mirror, the installation may fail."
msgstr ""
"但请注意，某些镜像源上可能缺少部分软件包。例如，如果你仅使用阿里云镜像"
"安装 ``xinference[audio]``，安装可能会失败。"

#: ../../source/getting_started/troubleshooting.rst:161
msgid ""
"This happens because ``num2words``, a dependency used by ``MeloTTS``, is "
"not available on the Aliyun mirror. As a result, ``pip install "
"xinference[audio]`` will resolve to older versions like "
"``xinference==1.2.0`` and ``xoscar==0.8.0`` (as of Oct 27, 2025)."
msgstr ""
"这是因为 ``MeloTTS`` 所依赖的 ``num2words`` 软件包在阿里云镜像上不可用。"
"因此，在执行 ``pip install xinference[audio]`` 时，可能会回退安装旧版本，"
"如 ``xinference==1.2.0`` 和 ``xoscar==0.8.0`` （截至 2025 年 10 月 27 日"
"）。"

#: ../../source/getting_started/troubleshooting.rst:163
msgid ""
"These older versions are incompatible and will produce the error: "
"``MainActorPool.append_sub_pool() got an unexpected keyword argument "
"'start_method'``"
msgstr ""
"这些旧版本不兼容，会导致以下错误：``MainActorPool.append_sub_pool() got "
"an unexpected keyword argument 'start_method'``"

#: ../../source/getting_started/troubleshooting.rst:174
msgid ""
"To avoid this issue when installing the xinference audio package, use "
"multiple mirrors:"
msgstr "为避免在安装 xinference 音频包时出现此问题，建议同时使用多个镜像源："

#: ../../source/getting_started/troubleshooting.rst:188
msgid "Installing Xinference 1.12.0 with uv Fails (As of November 2025)"
msgstr "使用 uv 安装 Xinference 1.12.0 失败（截至 2025 年 11 月）"

#: ../../source/getting_started/troubleshooting.rst:190
msgid ""
"**Note:** This is a temporary issue due to the current package ecosystem "
"and uv prioritizing **higher versions for direct dependencies** over "
"**indirect dependencies**."
msgstr ""
"**注意：** 这是一个临时性问题，原因在于当前的软件包生态系统以及 uv 的依赖解析策略——"
"它会优先选择 **直接依赖的高版本**，而不是 **间接依赖的版本**。"

#: ../../source/getting_started/troubleshooting.rst:193
msgid "Symptom"
msgstr "症状"

#: ../../source/getting_started/troubleshooting.rst:195
msgid ""
"When installing xinference 1.12.0 as of November 2025 using ``uv pip "
"install xinference``, you may encounter an issue where very old package "
"versions are installed, particularly:"
msgstr ""
"在 2025 年 11 月使用 ``uv pip install xinference`` 安装 xinference 1.12.0 时，"
"你可能会遇到安装到非常旧版本依赖包的问题，尤其是："

#: ../../source/getting_started/troubleshooting.rst:197
msgid "``transformers==4.12.2`` (from 2021)"
msgstr "``transformers==4.12.2`` （来自 2021 年的版本）"

#: ../../source/getting_started/troubleshooting.rst:198
msgid "``tokenizers==0.10.3`` (from 2021)"
msgstr "``tokenizers==0.10.3`` （来自 2021 年的版本）"

#: ../../source/getting_started/troubleshooting.rst:199
msgid "``huggingface-hub==1.0.1``"
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:201
msgid "Then uv fails with \"Failed to build `tokenizers==0.10.3`\""
msgstr "随后 uv 报错：\"Failed to build `tokenizers==0.10.3`\"（构建 `tokenizers==0.10.3` 失败）"

#: ../../source/getting_started/troubleshooting.rst:204
msgid "Root Cause"
msgstr "根本原因"

#: ../../source/getting_started/troubleshooting.rst:206
msgid ""
"This occurs because uv prioritizes **higher versions for direct "
"dependencies** over **indirect dependencies**:"
msgstr "出现该问题的原因是 uv 会优先选择 **直接依赖的高版本**，而忽略 **间接依赖** 中的版本要求："

#: ../../source/getting_started/troubleshooting.rst:208
msgid ""
"xinference 1.12.0 specifies ``huggingface-hub>=0.19.4`` as a **direct "
"dependency** (no upper bound)"
msgstr "xinference 1.12.0 将 ``huggingface-hub>=0.19.4`` 指定为 **直接依赖** （没有上限约束）"

#: ../../source/getting_started/troubleshooting.rst:209
msgid "uv selects the latest: ``huggingface-hub==1.0.1`` as of November 06 2025"
msgstr "截至 2025 年 11 月 6 日，uv 会选择最新版本：``huggingface-hub==1.0.1``"

#: ../../source/getting_started/troubleshooting.rst:210
msgid ""
"However, ``transformers`` (an **indirect dependency** via ``peft``) "
"requires ``huggingface-hub<1.0``"
msgstr "然而，``transformers``（通过 ``peft`` 引入的**间接依赖**）要求 ``huggingface-hub<1.0``"

#: ../../source/getting_started/troubleshooting.rst:211
msgid ""
"To resolve the conflict, uv keeps the direct dependency at 1.0.1 and "
"downgrades the indirect dependency ``transformers`` to ancient version "
"4.12.2"
msgstr ""
"为了解决依赖冲突，uv 保留了直接依赖 ``huggingface-hub==1.0.1``，并将间接依赖 "
"``transformers`` 降级到了非常旧的版本 4.12.2。"

#: ../../source/getting_started/troubleshooting.rst:213
msgid ""
"**This is by design in uv**: it prioritizes what you explicitly ask for "
"(direct dependencies) over transitive dependencies. Refer to "
"https://github.com/astral-sh/uv/issues/16601"
msgstr ""
"**这属于 uv 的设计特性**：它会优先满足你显式指定的依赖（直接依赖），而非传递依赖。"
"参考链接：https://github.com/astral-sh/uv/issues/16601"

#: ../../source/getting_started/troubleshooting.rst:216
msgid "Solutions"
msgstr "解决方案"

#: ../../source/getting_started/troubleshooting.rst:218
msgid "**Solution 1: Pre-constrain huggingface-hub (Recommended)**"
msgstr "**解决方案 1：预先限定 huggingface-hub 版本（推荐）**"

#: ../../source/getting_started/troubleshooting.rst:220
msgid "Explicitly constrain ``huggingface-hub`` to a compatible version range:"
msgstr "显式地将 ``huggingface-hub`` 限定在一个兼容的版本范围内："

#: ../../source/getting_started/troubleshooting.rst:226
msgid ""
"This forces uv to select a ``huggingface-hub`` version that's compatible "
"with modern ``transformers``."
msgstr "这样可以强制 uv 选择与现代版本 ``transformers`` 兼容的 ``huggingface-hub`` 版本。"

#: ../../source/getting_started/troubleshooting.rst:228
msgid "**Solution 2: Make transformers a direct dependency**"
msgstr "**解决方案 2：将 transformers 设为直接依赖**"

#: ../../source/getting_started/troubleshooting.rst:230
msgid ""
"By specifying ``transformers`` explicitly, it becomes a direct dependency"
" and uv will prefer higher versions:"
msgstr "通过显式指定 ``transformers``，它会成为直接依赖，uv 将优先选择更高版本："

#: ../../source/getting_started/troubleshooting.rst:236
msgid "**Solution 3: Use pip**"
msgstr "**解决方案 3：使用 pip**"

#: ../../source/getting_started/troubleshooting.rst:238
msgid ""
"Or just resort to using ``pip install xinference`` which will resolve to "
"the following versions"
msgstr "或者直接使用 ``pip install xinference``，它会自动解析到以下版本组合："

#: ../../source/getting_started/troubleshooting.rst:240
msgid "``transformers==4.57.1``"
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:241
msgid "``huggingface-hub==0.36.0``"
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:242
msgid "``tokenizers==0.22.1``"
msgstr ""

