# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-04-28 18:35+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.14.0\n"

#: ../../source/getting_started/troubleshooting.rst:5
msgid "Troubleshooting"
msgstr "故障排除"

#: ../../source/getting_started/troubleshooting.rst:9
msgid "No huggingface repo access"
msgstr "没有 huggingface 仓库权限"

#: ../../source/getting_started/troubleshooting.rst:11
msgid ""
"Sometimes, you may face errors accessing huggingface models, such as the "
"following message when accessing `llama2`:"
msgstr ""
"获取模型时，有时候会遇到权限问题。比如在获取 ``llama2`` 模型时可能会有"
"以下提示："

#: ../../source/getting_started/troubleshooting.rst:18
msgid ""
"This typically indicates either a lack of access rights to the repository"
" or missing huggingface access tokens. The following sections provide "
"guidance on addressing these issues."
msgstr ""
"这种情况一般是缺少 huggingface 仓库的权限，或者是没有配置 huggingface "
"token。可以按照接下来的方式解决这个问题。"

#: ../../source/getting_started/troubleshooting.rst:22
msgid "Get access to the huggingface repo"
msgstr "申请 huggingface 仓库权限"

#: ../../source/getting_started/troubleshooting.rst:24
msgid ""
"To obtain access, navigate to the desired huggingface repository and "
"agree to its terms and conditions. As an illustration, for the `llama2` "
"model, you can use this link: `https://huggingface.co/meta-llama/Llama-2"
"-7b-hf <https://huggingface.co/meta-llama/Llama-2-7b-hf>`_."
msgstr ""
"想要获取访问权限，打开对应的 huggingface 仓库，同意其条款和注意事项。以 `"
"`llama2`` 为例，可以打开这个链接去申请：`https://huggingface.co/meta-"
"llama/Llama-2-7b-hf <https://huggingface.co/meta-llama/Llama-2-7b-hf>`_."

#: ../../source/getting_started/troubleshooting.rst:29
msgid "Set up credentials to access huggingface"
msgstr "设置访问 huggingface 凭证"

#: ../../source/getting_started/troubleshooting.rst:31
msgid ""
"Your credential to access huggingface can be found online at "
"`https://huggingface.co/settings/tokens "
"<https://huggingface.co/settings/tokens>`_."
msgstr ""
"可以在 huggingface 页面找到凭证，`https://huggingface.co/settings/tokens "
"<https://huggingface.co/settings/tokens>`_."

#: ../../source/getting_started/troubleshooting.rst:33
msgid ""
"You can set the token as an environmental variable, with ``export "
"HUGGING_FACE_HUB_TOKEN=your_token_here``."
msgstr ""
"可以通过设置环境变量设置访问凭证，``export HUGGING_FACE_HUB_TOKEN=your_"
"token_here``。"

#: ../../source/getting_started/troubleshooting.rst:37
msgid "Incompatibility Between NVIDIA Driver and PyTorch Version"
msgstr "英伟达驱动和 PyTorch 版本不匹配"

#: ../../source/getting_started/troubleshooting.rst:39
msgid "If you are using a NVIDIA GPU, you may face the following error:"
msgstr "如果你在使用英伟达显卡，你可能会遇到以下错误："

#: ../../source/getting_started/troubleshooting.rst:50
msgid ""
"This typically indicates that your CUDA driver version is not compatible "
"with the PyTorch version you are using."
msgstr "这种情况一般是 CUDA 的版本和 Pytorch 版本不兼容导致的。"

#: ../../source/getting_started/troubleshooting.rst:52
msgid ""
"Go to `https://pytorch.org <https://pytorch.org>`_ to install a PyTorch "
"version that has been compiled with your version of the CUDA driver. **Do"
" not install a cuda version smaller than 11.8, preferably between 11.8 "
"and 12.1.**"
msgstr ""
"可以到 `https://pytorch.org <https://pytorch.org>`_ 官网安装和 CUDA 对应"
"的预编译版本的 PyTorch。同时，**请检查安装的 CUDA 版本不要小于 11.8，最好"
"版本在 11.8 到 12.1之间。**"

#: ../../source/getting_started/troubleshooting.rst:55
msgid ""
"Say if your CUDA driver version is 11.8, then you can install PyTorch "
"with the following command:"
msgstr "比如你的 CUDA 版本是 11.8，可以使用以下命令安装对应的 PyTorch："

#: ../../source/getting_started/troubleshooting.rst:63
msgid ""
"Xinference service cannot be accessed from external systems through "
"``<IP>:9997``"
msgstr "外部系统无法通过 ``<IP>:9997`` 访问 Xinference 服务"

#: ../../source/getting_started/troubleshooting.rst:65
msgid "Use ``-H 0.0.0.0`` parameter in when starting Xinference:"
msgstr "在启动 Xinference 时记得要加上 ``-H 0.0.0.0`` 参数:"

#: ../../source/getting_started/troubleshooting.rst:71
msgid ""
"Then Xinference service will listen on all network interfaces (not "
"limited to ``127.0.0.1`` or ``localhost``)."
msgstr ""
"那么 Xinference 服务将监听所有网络接口（而不仅限于 ``127.0.0.1`` 或 ``"
"localhost``）。"

#: ../../source/getting_started/troubleshooting.rst:73
msgid ""
"If you are using the :ref:`using_docker_image`, please add ``-p "
"<PORT>:9997`` during the docker run command, then access is available "
"through ``<IP>:<PORT>`` of the local machine."
msgstr ""
"如果使用的是 :ref:`using_docker_image`，请在 Docker 运行命令中 加上 ``-p "
"<PORT>:9997`` ，，你就可以通过本地机器的 ``<IP>:<PORT>`` 进行访问。"

#: ../../source/getting_started/troubleshooting.rst:78
msgid ""
"Launching a built-in model takes a long time, and sometimes the model "
"fails to download"
msgstr "启动内置模型需要很长时间，模型有时下载失败"

#: ../../source/getting_started/troubleshooting.rst:80
msgid ""
"Xinference by default uses HuggingFace as the source for models. If your "
"machines are in Mainland China, there might be accessibility issues when "
"using built-in models."
msgstr ""
"Xinference 默认使用 HuggingFace作为模型源。如果你的机器在中国大陆，使用"
"内置模型可能会有访问问题。"

#: ../../source/getting_started/troubleshooting.rst:84
msgid ""
"To address this, add environment variable "
"``XINFERENCE_MODEL_SRC=modelscope`` when starting the Xinference to "
"change the model source to ModelScope, which is optimized for Mainland "
"China."
msgstr ""
"要解决这个问题，可以在启动 Xinference 时添加环境变量 ``XINFERENCE_MODEL_"
"SRC=modelscope``，将模型源更改为 ModelScope，在中国大陆速度下载更快。"

#: ../../source/getting_started/troubleshooting.rst:88
msgid ""
"If you’re starting Xinference with Docker, include ``-e XINFERENCE_MODEL"
"_SRC=modelscope`` during the docker run command."
msgstr ""
"如果你用 Docker 启动 Xinference，可以在 Docker 命令中包含 ``-e XINFERENCE"
"_MODEL_SRC=modelscope`` 选项。"

#: ../../source/getting_started/troubleshooting.rst:92
msgid ""
"When using the official Docker image, RayWorkerVllm died due to OOM, "
"causing the model to fail to load"
msgstr "使用官方 Docker 映像时，RayWorkerVllm 因 OOM 而死亡，导致模型无法加载"

#: ../../source/getting_started/troubleshooting.rst:94
msgid ""
"Docker's ``--shm-size`` parameter is used to set the size of shared "
"memory. The default size of shared memory (/dev/shm) is 64MB, which may "
"be too small for vLLM backend."
msgstr ""
"Docker 的 ``--shm-size`` 参数可以用来设置共享内存的大小。共享内存(/dev/"
"shm)的默认大小是 64MB，对于 vLLM 后端来说可能不够。"

#: ../../source/getting_started/troubleshooting.rst:98
msgid ""
"You can increase its size by setting the ``--shm-size`` parameter as "
"follows:"
msgstr "你可以通过设置参数 ``--shm-size`` 来增加它的大小："

#: ../../source/getting_started/troubleshooting.rst:106
msgid "Missing ``model_engine`` parameter when launching LLM models"
msgstr "加载 LLM 模型时提示缺失 ``model_engine`` 参数"

#: ../../source/getting_started/troubleshooting.rst:108
msgid ""
"Since version ``v0.11.0``, launching LLM models requires an additional "
"``model_engine`` parameter. For specific information, please refer to "
":ref:`here <about_model_engine>`."
msgstr ""
"自 ``v0.11.0`` 版本开始，加载 LLM 模型时需要传入额外参数 ``model_engine``"
" 。具体信息请参考 :ref:`这里 <about_model_engine>` 。"

#: ../../source/getting_started/troubleshooting.rst:112
msgid ""
"Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is "
"incompatible with libgomp-a34b3233.so.1 library."
msgstr ""
"错误：mkl-service + Intel(R) MKL：MKL_THREADING_LAYER=INTEL 与 libgomp-a34b3233.so.1 库不兼容。"

#: ../../source/getting_started/troubleshooting.rst:114
msgid ""
"When start Xinference server and you hit the error \"ValueError: Model "
"architectures ['Qwen2ForCausalLM'] failed to be inspected. Please check "
"the logs for more details. \""
msgstr ""
"在启动 Xinference 服务器时，如果遇到错误：“ValueError: Model architectures "
"['Qwen2ForCausalLM'] failed to be inspected. Please check the logs for more details.”"

#: ../../source/getting_started/troubleshooting.rst:116
msgid ""
"The logs shows the error, ``\"Error: mkl-service + Intel(R) MKL: "
"MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 "
"library. Try to import numpy first or set the threading layer "
"accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\"``"
msgstr ""
"日志中显示错误：Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL "
"is incompatible with libgomp-a34b3233.so.1 library. Try to import numpy first "
"or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it."


#: ../../source/getting_started/troubleshooting.rst:118
msgid ""
"This is mostly because your NumPy is installed by conda and conda's Numpy"
" is built with Intel MKL optimizations, which is causing a conflict with "
"the GNU OpenMP library (libgomp) that's already loaded in the "
"environment."
msgstr ""
"这通常是因为你的 NumPy 是通过 conda 安装的，而 conda 的 NumPy 是使用 Intel MKL 优化构建的，"
"这导致它与环境中已加载的 GNU OpenMP 库（libgomp）产生冲突。"

#: ../../source/getting_started/troubleshooting.rst:124
msgid ""
"Setting ``MKL_THREADING_LAYER=GNU`` forces Intel's Math Kernel Library to"
" use GNU's OpenMP implementation instead of Intel's own implementation."
msgstr ""
"设置 MKL_THREADING_LAYER=GNU 可以强制 Intel 数学核心库（MKL）使用 GNU 的 OpenMP 实现，而不是使用 Intel 自己的实现。"

#: ../../source/getting_started/troubleshooting.rst:126
msgid "Or you can uninstall conda's numpy and reinstall with pip."
msgstr ""
"或者你也可以卸载 conda 安装的 numpy，然后使用 pip 重新安装。"

#: ../../source/getting_started/troubleshooting.rst:128
msgid ""
"On a related subject, if you use vllm, do not install pytorch with conda,"
" check "
"https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html for "
"detailed information."
msgstr ""
"相关地，如果你使用 vllm，不要通过 conda 安装 pytorch，详细信息请参考：https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html 。"

