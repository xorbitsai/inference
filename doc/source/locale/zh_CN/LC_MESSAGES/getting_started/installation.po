# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-04-19 00:40+0200\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.16.0\n"

#: ../../source/getting_started/installation.rst:5
msgid "Installation"
msgstr "安装"

#: ../../source/getting_started/installation.rst:6
msgid ""
"Xinference can be installed with ``pip`` on Linux, Windows, and macOS. To"
" run models using Xinference, you will need to install the backend "
"corresponding to the type of model you intend to serve."
msgstr ""
"Xinference 在 Linux, Windows, MacOS 上都可以通过 ``pip`` 来安装。如果需要"
"使用 Xinference 进行模型推理，可以根据不同的模型指定不同的引擎。"

#: ../../source/getting_started/installation.rst:8
msgid ""
"If you aim to serve all supported models, you can install all the "
"necessary dependencies with a single command::"
msgstr "如果你希望能够推理所有支持的模型，可以用以下命令安装所有需要的依赖："

#: ../../source/getting_started/installation.rst:13
msgid ""
"If you want to serve models in GGUF format, it's advised to install the "
"``llama-cpp-python`` dependency manually based on your hardware "
"specifications to enable acceleration. For more details, see the "
":ref:`installation_gguf` section."
msgstr ""
"如果你想使用 GGUF 格式的模型，建议根据当前使用的硬件手动安装所需要的依赖"
"，以充分利用硬件的加速能力。更多细节可以参考 :ref:`installation_gguf` "
"这一章节。"

#: ../../source/getting_started/installation.rst:16
msgid ""
"If you want to install only the necessary backends, here's a breakdown of"
" how to do it."
msgstr "如果你只想安装必要的依赖，接下来是如何操作的详细步骤。"

#: ../../source/getting_started/installation.rst:21
msgid "Transformers Backend"
msgstr "Transformers 引擎"

#: ../../source/getting_started/installation.rst:22
msgid ""
"PyTorch (transformers) supports the inference of most state-of-art "
"models. It is the default backend for models in PyTorch format::"
msgstr ""
"PyTorch(transformers) 引擎支持几乎有所的最新模型，这是 Pytorch 模型默认"
"使用的引擎："

#: ../../source/getting_started/installation.rst:28
msgid "vLLM Backend"
msgstr "vLLM 引擎"

#: ../../source/getting_started/installation.rst:29
msgid ""
"vLLM is a fast and easy-to-use library for LLM inference and serving. "
"Xinference will choose vLLM as the backend to achieve better throughput "
"when the following conditions are met:"
msgstr ""
"vLLM 是一个支持高并发的高性能大模型推理引擎。当满足以下条件时，Xinference"
" 会自动选择 vllm 作为引擎来达到更高的吞吐量："

#: ../../source/getting_started/installation.rst:31
msgid "The model format is ``pytorch``, ``gptq`` or ``awq``."
msgstr "模型格式为 ``pytorch`` ， ``gptq`` 或者 ``awq`` 。"

#: ../../source/getting_started/installation.rst:32
msgid "When the model format is ``pytorch``, the quantization is ``none``."
msgstr "当模型格式为 ``pytorch`` 时，量化选项需为 ``none`` 。"

#: ../../source/getting_started/installation.rst:33
msgid "When the model format is ``awq``, the quantization is ``Int4``."
msgstr "当模型格式为 ``awq`` 时，量化选项需为 ``Int4`` 。"

#: ../../source/getting_started/installation.rst:34
msgid ""
"When the model format is ``gptq``, the quantization is ``Int3``, ``Int4``"
" or ``Int8``."
msgstr ""
"当模型格式为 ``gptq`` 时，量化选项需为 ``Int3`` 、 ``Int4`` 或者 ``Int8``"
" 。"

#: ../../source/getting_started/installation.rst:35
msgid "The system is Linux and has at least one CUDA device"
msgstr "操作系统为 Linux 并且至少有一个支持 CUDA 的设备"

#: ../../source/getting_started/installation.rst:36
msgid ""
"The model family (for custom models) / model name (for builtin models) is"
" within the list of models supported by vLLM"
msgstr ""
"自定义模型的 ``model_family`` 字段和内置模型的 ``model_name`` 字段在 vLLM"
" 的支持列表中。"

#: ../../source/getting_started/installation.rst:38
msgid "Currently, supported models include:"
msgstr "目前，支持的模型包括："

#: ../../source/getting_started/installation.rst:42
msgid ""
"``llama-2``, ``llama-3``, ``llama-3.1``, ``llama-3.2-vision``, "
"``llama-2-chat``, ``llama-3-instruct``, ``llama-3.1-instruct``, "
"``llama-3.3-instruct``"
msgstr ""

#: ../../source/getting_started/installation.rst:43
msgid ""
"``mistral-v0.1``, ``mistral-instruct-v0.1``, ``mistral-instruct-v0.2``, "
"``mistral-instruct-v0.3``, ``mistral-nemo-instruct``, ``mistral-large-"
"instruct``"
msgstr ""

#: ../../source/getting_started/installation.rst:44
msgid "``codestral-v0.1``"
msgstr ""

#: ../../source/getting_started/installation.rst:45
msgid "``Yi``, ``Yi-1.5``, ``Yi-chat``, ``Yi-1.5-chat``, ``Yi-1.5-chat-16k``"
msgstr ""

#: ../../source/getting_started/installation.rst:46
msgid "``code-llama``, ``code-llama-python``, ``code-llama-instruct``"
msgstr ""

#: ../../source/getting_started/installation.rst:47
msgid ""
"``deepseek``, ``deepseek-coder``, ``deepseek-chat``, ``deepseek-coder-"
"instruct``, ``deepseek-r1-distill-qwen``, ``deepseek-v2-chat``, "
"``deepseek-v2-chat-0628``, ``deepseek-v2.5``, ``deepseek-v3``, "
"``deepseek-r1``, ``deepseek-r1-distill-llama``"
msgstr ""

#: ../../source/getting_started/installation.rst:48
msgid "``yi-coder``, ``yi-coder-chat``"
msgstr ""

#: ../../source/getting_started/installation.rst:49
msgid "``codeqwen1.5``, ``codeqwen1.5-chat``"
msgstr ""

#: ../../source/getting_started/installation.rst:50
msgid ""
"``qwen2.5``, ``qwen2.5-coder``, ``qwen2.5-instruct``, ``qwen2.5-coder-"
"instruct``, ``qwen2.5-instruct-1m``"
msgstr ""

#: ../../source/getting_started/installation.rst:51
msgid "``baichuan-2-chat``"
msgstr ""

#: ../../source/getting_started/installation.rst:52
msgid "``internlm2-chat``"
msgstr ""

#: ../../source/getting_started/installation.rst:53
msgid "``internlm2.5-chat``, ``internlm2.5-chat-1m``"
msgstr ""

#: ../../source/getting_started/installation.rst:54
msgid "``qwen-chat``"
msgstr ""

#: ../../source/getting_started/installation.rst:55
msgid "``mixtral-instruct-v0.1``, ``mixtral-8x22B-instruct-v0.1``"
msgstr ""

#: ../../source/getting_started/installation.rst:56
msgid "``chatglm3``, ``chatglm3-32k``, ``chatglm3-128k``"
msgstr ""

#: ../../source/getting_started/installation.rst:57
msgid "``glm4-chat``, ``glm4-chat-1m``"
msgstr ""

#: ../../source/getting_started/installation.rst:58
msgid "``codegeex4``"
msgstr ""

#: ../../source/getting_started/installation.rst:59
msgid "``qwen1.5-chat``, ``qwen1.5-moe-chat``"
msgstr ""

#: ../../source/getting_started/installation.rst:60
msgid "``qwen2-instruct``, ``qwen2-moe-instruct``"
msgstr ""

#: ../../source/getting_started/installation.rst:61
msgid "``QwQ-32B-Preview``, ``QwQ-32B``"
msgstr ""

#: ../../source/getting_started/installation.rst:62
msgid "``marco-o1``"
msgstr ""

#: ../../source/getting_started/installation.rst:63
msgid "``fin-r1``"
msgstr ""

#: ../../source/getting_started/installation.rst:64
msgid "``gemma-it``, ``gemma-2-it``, ``gemma-3-1b-it``"
msgstr ""

#: ../../source/getting_started/installation.rst:65
msgid "``orion-chat``, ``orion-chat-rag``"
msgstr ""

#: ../../source/getting_started/installation.rst:66
msgid "``c4ai-command-r-v01``"
msgstr ""

#: ../../source/getting_started/installation.rst:67
msgid "``minicpm3-4b``"
msgstr ""

#: ../../source/getting_started/installation.rst:68
msgid "``internlm3-instruct``"
msgstr ""

#: ../../source/getting_started/installation.rst:69
msgid "``moonlight-16b-a3b-instruct``"
msgstr ""

#: ../../source/getting_started/installation.rst:72
msgid "To install Xinference and vLLM::"
msgstr "安装 xinference 和 vLLM："

#: ../../source/getting_started/installation.rst:85
msgid "Llama.cpp Backend"
msgstr "Llama.cpp 引擎"

#: ../../source/getting_started/installation.rst:86
msgid ""
"Xinference supports models in ``gguf`` format via ``xllamacpp`` or "
"``llama-cpp-python``. `xllamacpp "
"<https://github.com/xorbitsai/xllamacpp>`_ is developed by Xinference "
"team, and will be the sole backend for llama.cpp in the future."
msgstr ""
"Xinference 通过 xllamacpp 或 llama-cpp-python 支持 gguf 格式的模型。`"
"xllamacpp <https://github.com/xorbitsai/xllamacpp>`_ 由 Xinference 团队"
"开发，并将在未来成为 llama.cpp 的唯一后端。"

#: ../../source/getting_started/installation.rst:92
msgid ""
"``xllamacpp`` is the default option for llama.cpp backend since v1.5.0. "
"To enable ``llama-cpp-python``, add environment variable "
"``USE_XLLAMACPP=0``."
msgstr ""
"自 v1.5.0 起，``xllamacpp`` 成为 llama.cpp 后端的默认选项。如需启用 ``"
"llama-cpp-python``，请设置环境变量 ``USE_XLLAMACPP=0``。"

#: ../../source/getting_started/installation.rst:97
msgid ""
"Since Xinference v1.5.0, ``llama-cpp-python`` will be deprecated. For "
"Xinference v1.6.0, ``llama-cpp-python`` will be removed."
msgstr ""
"自 Xinference v1.5.0 起，``llama-cpp-python`` 将被弃用；在 Xinference "
"v1.6.0 中，该后端将被移除。"

#: ../../source/getting_started/installation.rst:100
#: ../../source/getting_started/installation.rst:137
#: ../../source/getting_started/installation.rst:150
msgid "Initial setup::"
msgstr "初始步骤："

#: ../../source/getting_started/installation.rst:104
msgid "Installation instructions for ``xllamacpp``:"
msgstr "``xllamacpp`` 的安装说明："

#: ../../source/getting_started/installation.rst:106
msgid "CPU or Mac Metal::"
msgstr "CPU 或 Mac Metal："

#: ../../source/getting_started/installation.rst:110
msgid "CUDA::"
msgstr ""

#: ../../source/getting_started/installation.rst:114
msgid "HIP::"
msgstr ""

#: ../../source/getting_started/installation.rst:118
msgid "Hardware-Specific installations for ``llama-cpp-python``:"
msgstr "``llama-cpp-python`` 不同硬件的安装方式："

#: ../../source/getting_started/installation.rst:120
msgid "Apple Silicon::"
msgstr "Apple M系列"

#: ../../source/getting_started/installation.rst:124
msgid "Nvidia cards::"
msgstr "英伟达显卡："

#: ../../source/getting_started/installation.rst:128
msgid "AMD cards::"
msgstr "AMD 显卡："

#: ../../source/getting_started/installation.rst:134
msgid "SGLang Backend"
msgstr "SGLang 引擎"

#: ../../source/getting_started/installation.rst:135
msgid ""
"SGLang has a high-performance inference runtime with RadixAttention. It "
"significantly accelerates the execution of complex LLM programs by "
"automatic KV cache reuse across multiple calls. And it also supports "
"other common techniques like continuous batching and tensor parallelism."
msgstr ""
"SGLang 具有基于 RadixAttention 的高性能推理运行时。它通过在多个调用之间"
"自动重用KV缓存，显著加速了复杂 LLM 程序的执行。它还支持其他常见推理技术，"
"如连续批处理和张量并行处理。"

#: ../../source/getting_started/installation.rst:147
msgid "MLX Backend"
msgstr "MLX 引擎"

#: ../../source/getting_started/installation.rst:148
msgid "MLX-lm is designed for Apple silicon users to run LLM efficiently."
msgstr "MLX-lm 用来在苹果 silicon 芯片上提供高效的 LLM 推理。"

#: ../../source/getting_started/installation.rst:155
msgid "Other Platforms"
msgstr "其他平台"

#: ../../source/getting_started/installation.rst:157
msgid ":ref:`Ascend NPU <installation_npu>`"
msgstr ""

#~ msgid ""
#~ "``deepseek``, ``deepseek-coder``, ``deepseek-"
#~ "chat``, ``deepseek-coder-instruct``, "
#~ "``deepseek-r1-distill-qwen``, ``deepseek-v2-chat``, "
#~ "``deepseek-v2-chat-0628``, ``deepseek-v2.5``"
#~ msgstr ""

#~ msgid "``QwQ-32B-Preview``"
#~ msgstr ""

#~ msgid ""
#~ "Xinference supports models in ``gguf`` "
#~ "format via ``llama-cpp-python``. It's"
#~ " advised to install the llama.cpp-"
#~ "related dependencies manually based on "
#~ "your hardware specifications to enable "
#~ "acceleration."
#~ msgstr ""
#~ "Xinference 通过 ``llama-cpp-python`"
#~ "` 支持 ``gguf`` 格式的模型。"
#~ "建议根据当前使用的硬件手动安装依赖，从而"
#~ "获得最佳的加速效果。"

#~ msgid ""
#~ "``qwen2.5``, ``qwen2.5-coder``, ``qwen2.5-instruct``, "
#~ "``qwen2.5-coder-instruct``"
#~ msgstr ""

#~ msgid "``gemma-it``, ``gemma-2-it``"
#~ msgstr ""

#~ msgid "e.g. Starting local Xinference via"
#~ msgstr "例如，通过以下方式启动本地 Xinference"

#~ msgid "``USE_XLLAMACPP=1 xinference-local``"
#~ msgstr ""

#~ msgid "Cuda::"
#~ msgstr ""

