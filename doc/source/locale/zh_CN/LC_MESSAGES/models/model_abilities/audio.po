# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-04-07 10:20+0200\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.16.0\n"

#: ../../source/models/model_abilities/audio.rst:5
msgid "Audio (Experimental)"
msgstr "音频（实验性质）"

#: ../../source/models/model_abilities/audio.rst:7
msgid "Learn how to turn audio into text or text into audio with Xinference."
msgstr "学习如何使用 Xinference 将音频转换为文本或将文本转换为音频。"

#: ../../source/models/model_abilities/audio.rst:11
msgid "Introduction"
msgstr "介绍"

#: ../../source/models/model_abilities/audio.rst:14
msgid "The Audio API provides three methods for interacting with audio:"
msgstr "Audio API提供了三种与音频交互的方法："

#: ../../source/models/model_abilities/audio.rst:17
msgid "The transcriptions endpoint transcribes audio into the input language."
msgstr "转录终端将音频转录为输入语言。"

#: ../../source/models/model_abilities/audio.rst:18
msgid "The translations endpoint translates audio into English."
msgstr "翻译端点将音频转换为英文。"

#: ../../source/models/model_abilities/audio.rst:19
msgid "The speech endpoint generates audio from the input text."
msgstr "转录终端将音频转录为输入语言。"

#: ../../source/models/model_abilities/audio.rst:26
msgid "API ENDPOINT"
msgstr "API 端点"

#: ../../source/models/model_abilities/audio.rst:27
msgid "OpenAI-compatible ENDPOINT"
msgstr "OpenAI 兼容端点"

#: ../../source/models/model_abilities/audio.rst:29
msgid "Transcription API"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:30
msgid "/v1/audio/transcriptions"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:32
msgid "Translation API"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:33
msgid "/v1/audio/translations"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:35
msgid "Speech API"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:36
msgid "/v1/audio/speech"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:40
msgid "Supported models"
msgstr "支持的模型列表"

#: ../../source/models/model_abilities/audio.rst:42
msgid "The audio API is supported with the following models in Xinference:"
msgstr "在Xinference中，以下模型支持音频API："

#: ../../source/models/model_abilities/audio.rst:45
msgid "Audio to text"
msgstr "语音转文本"

#: ../../source/models/model_abilities/audio.rst:47
msgid "whisper-tiny"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:48
msgid "whisper-tiny.en"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:49
msgid "whisper-base"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:50
msgid "whisper-base.en"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:51
msgid "whisper-medium"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:52
msgid "whisper-medium.en"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:53
msgid "whisper-large-v3"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:54
msgid "whisper-large-v3-turbo"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:55
msgid "Belle-distilwhisper-large-v2-zh"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:56
msgid "Belle-whisper-large-v2-zh"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:57
msgid "Belle-whisper-large-v3-zh"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:58
msgid "SenseVoiceSmall"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:62
msgid "Text to audio"
msgstr "文本转语音"

#: ../../source/models/model_abilities/audio.rst:64
msgid "ChatTTS"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:65
msgid "CosyVoice"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:66
msgid "FishSpeech-1.5"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:67
msgid "F5-TTS"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:70
msgid "Quickstart"
msgstr "快速入门"

#: ../../source/models/model_abilities/audio.rst:73
msgid "Transcription"
msgstr "转录"

#: ../../source/models/model_abilities/audio.rst:75
msgid ""
"The Transcription API mimics OpenAI's `create transcriptions API "
"<https://platform.openai.com/docs/api-"
"reference/audio/createTranscription>`_. We can try Transcription API out "
"either via cURL, OpenAI Client, or Xinference's python client:"
msgstr ""
"Transcription API 模仿了 OpenAI 的 `create transcriptions API <https://"
"platform.openai.com/docs/api-reference/audio/createTranscription>`_。你"
"可以通过 cURL、OpenAI Client 或者 Xinference 的 Python 客户端来尝试 "
"Transcription API："

#: ../../source/models/model_abilities/audio.rst:126
msgid "Translation"
msgstr "翻译"

#: ../../source/models/model_abilities/audio.rst:128
msgid ""
"The Translation API mimics OpenAI's `create translations API "
"<https://platform.openai.com/docs/api-"
"reference/audio/createTranslation>`_. We can try Translation API out "
"either via cURL, OpenAI Client, or Xinference's python client:"
msgstr ""
"Translation API 模仿了 OpenAI 的 `create translations API <https://"
"platform.openai.com/docs/api-reference/audio/createTranslation>`_。你可以"
"通过 cURL、OpenAI Client 或 Xinference 的 Python 客户端来尝试使用 "
"Translation API："

#: ../../source/models/model_abilities/audio.rst:178
msgid "Speech"
msgstr "语音"

#: ../../source/models/model_abilities/audio.rst:182
msgid ""
"The Speech API mimics OpenAI's `create speech API "
"<https://platform.openai.com/docs/api-reference/audio/createSpeech>`_. We"
" can try Speech API out either via cURL, OpenAI Client, or Xinference's "
"python client:"
msgstr ""
"Transcription API 模仿了 OpenAI 的 `create speech API <https://platform."
"openai.com/docs/api-reference/audio/createSpeech>`_。你可以通过 cURL、"
"OpenAI Client 或者 Xinference 的 Python 客户端来尝试 Speech API："

#: ../../source/models/model_abilities/audio.rst:185
msgid "Speech API use non-stream by default as"
msgstr "Speech API 默认使用非流式"

#: ../../source/models/model_abilities/audio.rst:187
msgid ""
"The stream output of ChatTTS is not as good as the non-stream output, "
"please refer to: https://github.com/2noise/ChatTTS/pull/564"
msgstr ""
"ChatTTS 的流式输出不如非流式的效果好，参考：https://github.com/2noise/"
"ChatTTS/pull/564"

#: ../../source/models/model_abilities/audio.rst:188
msgid ""
"The stream requires ffmpeg<7: "
"https://pytorch.org/audio/stable/installation.html#optional-dependencies"
msgstr ""
"流式要求 ffmpeg<7：https://pytorch.org/audio/stable/installation.html#"
"optional-dependencies"

#: ../../source/models/model_abilities/audio.rst:240
msgid "ChatTTS Usage"
msgstr "ChatTTS 使用"

#: ../../source/models/model_abilities/audio.rst:242
#: ../../source/models/model_abilities/audio.rst:389
msgid "Basic usage, refer to :ref:`audio speech usage <audio_speech>`."
msgstr "基本使用，参考 :ref:`语音使用章节 <audio_speech>`。"

#: ../../source/models/model_abilities/audio.rst:244
msgid ""
"Fixed tone color. We can use fixed tone color provided by "
"https://github.com/6drf21e/ChatTTS_Speaker, Download the "
"`evaluation_result.csv "
"<https://github.com/6drf21e/ChatTTS_Speaker/blob/main/evaluation_results.csv>`_"
" , take ``seed_2155`` as example, we get the ``emb_data`` of it."
msgstr ""
"固定音色。我们可以使用由 https://github.com/6drf21e/ChatTTS_Speaker 提供"
"的固定音色，下载 `evaluation_result.csv <https://github.com/6drf21e/"
"ChatTTS_Speaker/blob/main/evaluation_results.csv>`_ ，以 ``seed_2155`` "
"音色作为例子，我们使用 ``emb_data`` 列的数据。"

#: ../../source/models/model_abilities/audio.rst:257
msgid "Use the fixed tone color of ``seed_2155`` to generate speech."
msgstr "使用 ``seed_2155`` 固定音色来创建语音。"

#: ../../source/models/model_abilities/audio.rst:273
msgid "CosyVoice Usage"
msgstr "CosyVoice 模型使用"

#: ../../source/models/model_abilities/audio.rst:275
msgid "Basic usage, launch model ``CosyVoice-300M-SFT``."
msgstr "基本使用，加载模型 ``CosyVoice-300M-SFT``。"

#: ../../source/models/model_abilities/audio.rst:324
msgid "Clone voice, launch model ``CosyVoice-300M``."
msgstr "克隆声音，加载模型 ``CosyVoice-300M``。"

#: ../../source/models/model_abilities/audio.rst:347
msgid "Cross lingual usage, launch model ``CosyVoice-300M``."
msgstr "跨语言使用，加载模型 ``CosyVoice-300M``。"

#: ../../source/models/model_abilities/audio.rst:366
msgid "Instruction based, launch model ``CosyVoice-300M-Instruct``."
msgstr "基于指令的声音合成，加载模型 ``CosyVoice-300M-Instruct``。"

#: ../../source/models/model_abilities/audio.rst:383
msgid ""
"More instructions and examples, could be found at https://fun-audio-"
"llm.github.io/ ."
msgstr "更多指令和例子，可以参考 https://fun-audio-llm.github.io/ 。"

#: ../../source/models/model_abilities/audio.rst:387
msgid "FishSpeech Usage"
msgstr "FishSpeech 模型使用"

#: ../../source/models/model_abilities/audio.rst:391
msgid ""
"Clone voice, launch model ``FishSpeech-1.5``. Please use `prompt_speech` "
"instead of `reference_audio` and `prompt_text` instead of "
"`reference_text` to clone voice from the reference audio for the "
"FishSpeech model. This arguments is aligned to voice cloning of "
"CosyVoice."
msgstr ""
"克隆语音，启动模型 ``FishSpeech-1.5``。请使用 `prompt_speech`而不是 `"
"reference_audio` 以及 `prompt_text` 而不是 `reference_text` 来为 "
"FishSpeech 模型提供参考音频。这个参数和 CosyVoice 的语音克隆保持一致。"

#: ../../source/models/model_abilities/audio.rst:417
msgid "SenseVoiceSmall Offline Usage"
msgstr "SenseVoiceSmall 离线使用"

#: ../../source/models/model_abilities/audio.rst:419
msgid ""
"Now SenseVoiceSmall use a small vad model ``fsmn-vad``, it will be "
"downloaded thus network required."
msgstr ""
"现在 SenseVoiceSmall 使用一个小的 VAD 模型 ``fsmn-vad``，因此它需要网络来"
"下载。"

#: ../../source/models/model_abilities/audio.rst:421
msgid "For offline environment, you can download the vad model in advance."
msgstr "对于离线环境，你可以提前下载这个 VAD 模型。"

#: ../../source/models/model_abilities/audio.rst:423
msgid ""
"Download from `huggingface <https://huggingface.co/funasr/fsmn-vad>`_ or "
"`modelscope <https://modelscope.cn/models/iic/speech_fsmn_vad_zh-cn-16k-"
"common-pytorch/files>`_. Assume downloaded to ``/path/to/fsmn-vad``."
msgstr ""
"从 `huggingface <https://huggingface.co/funasr/fsmn-vad>`_ 或者 `"
"modelscope <https://modelscope.cn/models/iic/speech_fsmn_vad_zh-cn-16k-"
"common-pytorch/files>`_ 下载。假设下载到 ``/path/to/fsmn-vad``。"

#: ../../source/models/model_abilities/audio.rst:426
msgid ""
"Then when launching SenseVoiceSmall with Web UI, you can add an "
"additional parameter with key ``vad_model`` and value ``/path/to/fsmn-"
"vad`` which is the downloaded path. When launching with command line, you"
" can add an option ``--vad_model /path/to/fsmn-vad``."
msgstr ""
"然后当用 Web UI 加载 SenseVoiceSmall 时，添加额外选项，key 是 ``vad_model"
"``，值是之前的下载路径 ``/path/to/fsmn-vad``。用命令行加载时，增加选项 ``"
"--vad_model /path/to/fsmn-vad``。"

#: ../../source/models/model_abilities/audio.rst:431
msgid "Kokoro Usage"
msgstr "Kokoro 模型使用"

#: ../../source/models/model_abilities/audio.rst:433
msgid ""
"The Kokoro model supports multiple languages, but the default language is"
" English. If you want to use other languages, such as Chinese, you need "
"to install additional dependency packages and add an additional parameter"
" when starting the model."
msgstr ""
"Kokoro模型支持多语言，默认是英文。如果你想使用非默认语言，例如中文，则"
"需要安装额外依赖包并且在模型启动时增加对应参数。"

#: ../../source/models/model_abilities/audio.rst:437
msgid "pip install misaki[zh]"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:439
msgid ""
"Initialize the model with the parameter lang_code='z', For all available "
"``lang_code`` options, please refer to `kokoro source code "
"<https://github.com/hexgrad/kokoro/blob/main/kokoro/pipeline.py#L22>`_. "
"If the model is started through the web UI, an additional parameter needs"
" to be added, with the key as ``lang_code`` and the value as ``z``. If "
"the model is started through the xinference client, the parameters are "
"passed via the launch_model interface:"
msgstr ""
"使用 lang_code='z' 参数初始化模型，可以参考 `kokoro source code <https://"
"github.com/hexgrad/kokoro/blob/main/kokoro/pipeline.py#L22>`_ 查看所有"
"支持的 lang_code。如果你是通过 Web UI启动的模型，则需要添加额外参数，key"
"是 ``lang_code``，value是 ``z``。如果你是通过 xinference client启动的模型"
"，则可以参考如下代码传递参数："

#: ../../source/models/model_abilities/audio.rst:456
msgid ""
"When inferring, the voice must start with 'z', for example: "
"``zf_xiaoyi``. The currently supported voices are: "
"https://huggingface.co/hexgrad/Kokoro-82M/tree/main/voices. For example:"
msgstr ""
"当推理时，需要使用 'z' 开头的 voice，例如：``zf_xiaoyi``。目前支持的 "
"voices 可以参考 https://huggingface.co/hexgrad/Kokoro-82M/tree/main/"
"voices。使用方法如下："

