# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-09-22 11:25+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/models/model_abilities/audio.rst:5
msgid "Audio"
msgstr "音频"

#: ../../source/models/model_abilities/audio.rst:7
msgid "Learn how to turn audio into text or text into audio with Xinference."
msgstr "学习如何使用 Xinference 将音频转换为文本或将文本转换为音频。"

#: ../../source/models/model_abilities/audio.rst:11
msgid "Introduction"
msgstr "介绍"

#: ../../source/models/model_abilities/audio.rst:14
msgid "The Audio API provides three methods for interacting with audio:"
msgstr "Audio API提供了三种与音频交互的方法："

#: ../../source/models/model_abilities/audio.rst:17
msgid "The transcriptions endpoint transcribes audio into the input language."
msgstr "转录终端将音频转录为输入语言。"

#: ../../source/models/model_abilities/audio.rst:18
msgid "The translations endpoint translates audio into English."
msgstr "翻译端点将音频转换为英文。"

#: ../../source/models/model_abilities/audio.rst:19
msgid "The speech endpoint generates audio from the input text."
msgstr "转录终端将音频转录为输入语言。"

#: ../../source/models/model_abilities/audio.rst:26
msgid "API ENDPOINT"
msgstr "API 端点"

#: ../../source/models/model_abilities/audio.rst:27
msgid "OpenAI-compatible ENDPOINT"
msgstr "OpenAI 兼容端点"

#: ../../source/models/model_abilities/audio.rst:29
msgid "Transcription API"
msgstr "转录 API"

#: ../../source/models/model_abilities/audio.rst:30
msgid "/v1/audio/transcriptions"
msgstr "/v1/audio/transcriptions"

#: ../../source/models/model_abilities/audio.rst:32
msgid "Translation API"
msgstr "翻译 API"

#: ../../source/models/model_abilities/audio.rst:33
msgid "/v1/audio/translations"
msgstr "/v1/audio/translations"

#: ../../source/models/model_abilities/audio.rst:35
msgid "Speech API"
msgstr "语音 API"

#: ../../source/models/model_abilities/audio.rst:36
msgid "/v1/audio/speech"
msgstr "/v1/audio/speech"

#: ../../source/models/model_abilities/audio.rst:40
msgid "Supported models"
msgstr "支持的模型列表"

#: ../../source/models/model_abilities/audio.rst:42
msgid "The audio API is supported with the following models in Xinference:"
msgstr "在Xinference中，以下模型支持音频API："

#: ../../source/models/model_abilities/audio.rst:45
msgid "Audio to text"
msgstr "语音转文本"

#: ../../source/models/model_abilities/audio.rst:47
msgid ":ref:`whisper-tiny <models_builtin_whisper-tiny>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:48
msgid ":ref:`whisper-tiny.en <models_builtin_whisper-tiny.en>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:49
msgid ":ref:`whisper-base <models_builtin_whisper-base>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:50
msgid ":ref:`whisper-base.en <models_builtin_whisper-base.en>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:51
msgid ":ref:`whisper-medium <models_builtin_whisper-medium>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:52
msgid ":ref:`whisper-medium.en <models_builtin_whisper-medium.en>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:53
msgid ":ref:`whisper-large-v3 <models_builtin_whisper-large-v3>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:54
msgid ":ref:`whisper-large-v3-turbo <models_builtin_whisper-large-v3-turbo>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:55
msgid ""
":ref:`Belle-distilwhisper-large-v2-zh <models_builtin_belle-"
"distilwhisper-large-v2-zh>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:56
msgid ""
":ref:`Belle-whisper-large-v2-zh <models_builtin_belle-whisper-"
"large-v2-zh>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:57
msgid ""
":ref:`Belle-whisper-large-v3-zh <models_builtin_belle-whisper-"
"large-v3-zh>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:58
msgid ":ref:`SenseVoiceSmall <models_builtin_sensevoicesmall>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:59
msgid ":ref:`Paraformer-zh <models_builtin_paraformer-zh>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:61
#: ../../source/models/model_abilities/audio.rst:99
msgid "For Mac M-series chips only:"
msgstr "仅适用于 Mac M 系列芯片："

#: ../../source/models/model_abilities/audio.rst:63
msgid ":ref:`whisper-tiny-mlx <models_builtin_whisper-tiny-mlx>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:64
msgid ":ref:`whisper-tiny.en-mlx <models_builtin_whisper-tiny.en-mlx>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:65
msgid ":ref:`whisper-base-mlx <models_builtin_whisper-base-mlx>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:66
msgid ":ref:`whisper-base.en-mlx <models_builtin_whisper-base.en-mlx>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:67
msgid ":ref:`whisper-medium-mlx <models_builtin_whisper-medium-mlx>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:68
msgid ":ref:`whisper-medium.en-mlx <models_builtin_whisper-medium.en-mlx>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:69
msgid ":ref:`whisper-large-v3-mlx <models_builtin_whisper-large-v3-mlx>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:70
msgid ""
":ref:`whisper-large-v3-turbo-mlx <models_builtin_whisper-large-v3-turbo-"
"mlx>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:74
msgid "Text to audio (TTS)"
msgstr "文本转语音（TTS）"

#: ../../source/models/model_abilities/audio.rst:76
msgid ""
"**Models supporting zero-shot** (direct synthesis without reference "
"audio):"
msgstr "**支持zero-shot的模型** （无需参考音频）"

#: ../../source/models/model_abilities/audio.rst:78
msgid ":ref:`ChatTTS <models_builtin_chattts>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:79
msgid ":ref:`CosyVoice-300M-SFT <models_builtin_cosyvoice-300m-sft>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:80
msgid ":ref:`CosyVoice-300M-Instruct <models_builtin_cosyvoice-300m-instruct>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:81
msgid "MeloTTS series"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:82
msgid ":ref:`Kokoro-82M <models_builtin_kokoro-82m>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:83
#: ../../source/models/model_abilities/audio.rst:102
msgid ":ref:`Kokoro-82M-MLX <models_builtin_kokoro-82m-mlx>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:84
msgid ":ref:`MegaTTS3 <models_builtin_megatts3>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:86
msgid "**Models supporting voice cloning** (requires reference audio):"
msgstr "**支持语音克隆的模型** （需要参考音频）"

#: ../../source/models/model_abilities/audio.rst:88
msgid ":ref:`CosyVoice-300M <models_builtin_cosyvoice-300m>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:89
msgid ":ref:`CosyVoice 2.0 <models_builtin_cosyvoice2-0.5b>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:90
msgid ":ref:`FishSpeech-1.5 <models_builtin_fishspeech-1.5>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:91
msgid ":ref:`F5-TTS <models_builtin_f5-tts>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:92
#: ../../source/models/model_abilities/audio.rst:101
msgid ":ref:`F5-TTS-MLX <models_builtin_f5-tts-mlx>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:93
#: ../../source/models/model_abilities/audio.rst:97
msgid ":ref:`IndexTTS2 <models_builtin_indextts2>`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:95
msgid "**Models supporting emotion control**:"
msgstr "**支持情感控制的模型**"

#: ../../source/models/model_abilities/audio.rst:105
msgid "Quickstart"
msgstr "快速入门"

#: ../../source/models/model_abilities/audio.rst:108
msgid "Transcription"
msgstr "转录"

#: ../../source/models/model_abilities/audio.rst:110
msgid ""
"The Transcription API mimics OpenAI's `create transcriptions API "
"<https://platform.openai.com/docs/api-"
"reference/audio/createTranscription>`_. We can try Transcription API out "
"either via cURL, OpenAI Client, or Xinference's python client:"
msgstr ""
"Transcription API 模仿了 OpenAI 的 `create transcriptions API <https://"
"platform.openai.com/docs/api-reference/audio/createTranscription>`_。你"
"可以通过 cURL、OpenAI Client 或者 Xinference 的 Python 客户端来尝试 "
"Transcription API："

#: ../../source/models/model_abilities/audio.rst:161
msgid "Translation"
msgstr "翻译"

#: ../../source/models/model_abilities/audio.rst:163
msgid ""
"The Translation API mimics OpenAI's `create translations API "
"<https://platform.openai.com/docs/api-"
"reference/audio/createTranslation>`_. We can try Translation API out "
"either via cURL, OpenAI Client, or Xinference's python client:"
msgstr ""
"Translation API 模仿了 OpenAI 的 `create translations API <https://"
"platform.openai.com/docs/api-reference/audio/createTranslation>`_。你可以"
"通过 cURL、OpenAI Client 或 Xinference 的 Python 客户端来尝试使用 "
"Translation API："

#: ../../source/models/model_abilities/audio.rst:213
msgid "Speech"
msgstr "语音"

#: ../../source/models/model_abilities/audio.rst:217
msgid ""
"The Speech API mimics OpenAI's `create speech API "
"<https://platform.openai.com/docs/api-reference/audio/createSpeech>`_. We"
" can try Speech API out either via cURL, OpenAI Client, or Xinference's "
"python client:"
msgstr ""
"Transcription API 模仿了 OpenAI 的 `create speech API <https://platform."
"openai.com/docs/api-reference/audio/createSpeech>`_。你可以通过 cURL、"
"OpenAI Client 或者 Xinference 的 Python 客户端来尝试 Speech API："

#: ../../source/models/model_abilities/audio.rst:220
msgid "Speech API use non-stream by default as"
msgstr "Speech API 默认使用非流式"

#: ../../source/models/model_abilities/audio.rst:222
msgid ""
"The stream output of ChatTTS is not as good as the non-stream output, "
"please refer to: https://github.com/2noise/ChatTTS/pull/564"
msgstr ""
"ChatTTS 的流式输出不如非流式的效果好，参考：https://github.com/2noise/"
"ChatTTS/pull/564"

#: ../../source/models/model_abilities/audio.rst:223
msgid ""
"The stream requires ffmpeg<7: "
"https://pytorch.org/audio/stable/installation.html#optional-dependencies"
msgstr ""
"流式要求 ffmpeg<7：https://pytorch.org/audio/stable/installation.html#"
"optional-dependencies"

#: ../../source/models/model_abilities/audio.rst:275
msgid "ChatTTS Usage"
msgstr "ChatTTS 使用"

#: ../../source/models/model_abilities/audio.rst:277
#: ../../source/models/model_abilities/audio.rst:480
msgid "Basic usage, refer to :ref:`audio speech usage <audio_speech>`."
msgstr "基本使用，参考 :ref:`语音使用章节 <audio_speech>`。"

#: ../../source/models/model_abilities/audio.rst:279
msgid ""
"Fixed tone color. We can use fixed tone color provided by "
"https://github.com/6drf21e/ChatTTS_Speaker, Download the "
"`evaluation_result.csv "
"<https://github.com/6drf21e/ChatTTS_Speaker/blob/main/evaluation_results.csv>`_"
" , take ``seed_2155`` as example, we get the ``emb_data`` of it."
msgstr ""
"固定音色。我们可以使用由 https://github.com/6drf21e/ChatTTS_Speaker 提供"
"的固定音色，下载 `evaluation_result.csv <https://github.com/6drf21e/"
"ChatTTS_Speaker/blob/main/evaluation_results.csv>`_ ，以 ``seed_2155`` "
"音色作为例子，我们使用 ``emb_data`` 列的数据。"

#: ../../source/models/model_abilities/audio.rst:292
msgid "Use the fixed tone color of ``seed_2155`` to generate speech."
msgstr "使用 ``seed_2155`` 固定音色来创建语音。"

#: ../../source/models/model_abilities/audio.rst:308
msgid "CosyVoice Usage"
msgstr "CosyVoice 模型使用"

#: ../../source/models/model_abilities/audio.rst:310
msgid ""
"CosyVoice has two versions: CosyVoice 1.0 and CosyVoice 2.0. CosyVoice "
"1.0 has three different models:"
msgstr ""
"CosyVoice 有两个版本：CosyVoice 1.0 和 CosyVoice 2.0。CosyVoice 1.0 有 3 "
"个不同模型："

#: ../../source/models/model_abilities/audio.rst:312
msgid ""
"**CosyVoice-300M-SFT**: Choose this model if you just want to convert "
"text to audio. There are pretrained voices available: ['中文女', '中文男'"
", '日语男', '粤语女', '英文女', '英文男', '韩语女']"
msgstr ""
"**CosyVoice-300M-SFT**: 如果你只想把文本转换为语音，选择这个模型。它提供"
"了一些预训练的音色: ['中文女', '中文男', '日语男', '粤语女', '英文女', '"
"英文男', '韩语女']"

#: ../../source/models/model_abilities/audio.rst:313
msgid ""
"**CosyVoice-300M**: Choose this model if you want to clone voice or "
"convert text to audio in different languages. The ``prompt_speech`` is "
"always required and should be a WAV file. For optimal performance, use a "
"sample rate of 16,000 Hz."
msgstr ""
"**CosyVoice-300M**: 如果你想克隆声音或者把文本转换成另一种语言的语音，"
"选择这个模型。使用这个模型，你必须提供 ``prompt_speech`` WAV格式音频文件"
"，请使用 16,000 Hz 采样率以获得更好的性能。"

#: ../../source/models/model_abilities/audio.rst:314
msgid ""
"**CosyVoice-300M-Instruct**: Choose this model If you need precise "
"control over the tone and pitch."
msgstr "**CosyVoice-300M-Instruct**: 如果你想精确控制音调和音色，选择这个模型。"

#: ../../source/models/model_abilities/audio.rst:316
msgid "Basic usage, launch model ``CosyVoice-300M-SFT``."
msgstr "基本使用，加载模型 ``CosyVoice-300M-SFT``。"

#: ../../source/models/model_abilities/audio.rst:365
msgid "Clone voice, launch model ``CosyVoice-300M``."
msgstr "克隆声音，加载模型 ``CosyVoice-300M``。"

#: ../../source/models/model_abilities/audio.rst:395
msgid "Cross lingual usage, launch model ``CosyVoice-300M``."
msgstr "跨语言使用，加载模型 ``CosyVoice-300M``。"

#: ../../source/models/model_abilities/audio.rst:418
msgid "Instruction based, launch model ``CosyVoice-300M-Instruct``."
msgstr "基于指令的声音合成，加载模型 ``CosyVoice-300M-Instruct``。"

#: ../../source/models/model_abilities/audio.rst:435
msgid ""
"CosyVoice 2.0 only has one model, it provides all the capabilities of the"
" three CosyVoice models. The usage is the same as CosyVoice."
msgstr ""
"CosyVoice 2.0 只有一个模型，但它包含了 CosyVoice 三个模型的所有能力。使用"
"方法与 CosyVoice 一样。"

#: ../../source/models/model_abilities/audio.rst:437
msgid "CosyVoice 2.0 stream usage, launch model ``CosyVoice2-0.5B``."
msgstr "CosyVoice 2.0 流式使用，加载模型 ``CosyVoice2-0.5B``。"

#: ../../source/models/model_abilities/audio.rst:474
msgid ""
"More instructions and examples, could be found at https://fun-audio-"
"llm.github.io/ ."
msgstr "更多指令和例子，可以参考 https://fun-audio-llm.github.io/ 。"

#: ../../source/models/model_abilities/audio.rst:478
msgid "FishSpeech Usage"
msgstr "FishSpeech 模型使用"

#: ../../source/models/model_abilities/audio.rst:482
msgid ""
"Clone voice, launch model ``FishSpeech-1.5``. Please use `prompt_speech` "
"instead of `reference_audio` and `prompt_text` instead of "
"`reference_text` to clone voice from the reference audio for the "
"FishSpeech model. This arguments is aligned to voice cloning of "
"CosyVoice."
msgstr ""
"克隆语音，启动模型 ``FishSpeech-1.5``。请使用 `prompt_speech`而不是 `"
"reference_audio` 以及 `prompt_text` 而不是 `reference_text` 来为 "
"FishSpeech 模型提供参考音频。这个参数和 CosyVoice 的语音克隆保持一致。"

#: ../../source/models/model_abilities/audio.rst:509
msgid "Paraformer Usage"
msgstr "Paraformer 使用说明"

#: ../../source/models/model_abilities/audio.rst:512
msgid "model"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:512
msgid "vad"
msgstr "语音活动检测（vad）"

#: ../../source/models/model_abilities/audio.rst:512
msgid "punc"
msgstr "标点恢复（punc）"

#: ../../source/models/model_abilities/audio.rst:512
msgid "timestamp"
msgstr "时间戳"

#: ../../source/models/model_abilities/audio.rst:512
msgid "speaker"
msgstr "说话人"

#: ../../source/models/model_abilities/audio.rst:512
msgid "hotword"
msgstr "热词"

#: ../../source/models/model_abilities/audio.rst:514
msgid ":ref:`models_builtin_paraformer-zh`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:514
#: ../../source/models/model_abilities/audio.rst:516
#: ../../source/models/model_abilities/audio.rst:518
#: ../../source/models/model_abilities/audio.rst:520
#: ../../source/models/model_abilities/audio.rst:522
msgid "yes"
msgstr "是"

#: ../../source/models/model_abilities/audio.rst:514
#: ../../source/models/model_abilities/audio.rst:516
#: ../../source/models/model_abilities/audio.rst:518
#: ../../source/models/model_abilities/audio.rst:520
msgid "no"
msgstr "否"

#: ../../source/models/model_abilities/audio.rst:516
msgid ":ref:`models_builtin_paraformer-zh-hotword`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:518
msgid ":ref:`models_builtin_paraformer-zh-spk`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:520
msgid ":ref:`models_builtin_paraformer-zh-long`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:522
msgid ":ref:`models_builtin_seaco-paraformer-zh` (recommend)"
msgstr ":ref:`models_builtin_seaco-paraformer-zh` （推荐）"

#: ../../source/models/model_abilities/audio.rst:525
msgid "**VAD & Punctuation Usage**"
msgstr "**VAD 与标点符号的使用**"

#: ../../source/models/model_abilities/audio.rst:527
msgid "All Paraformer models support VAD and punctuation."
msgstr "所有 Paraformer 模型均支持 VAD 和标点功能。"

#: ../../source/models/model_abilities/audio.rst:529
msgid "**Timestamp & Speaker Usage**"
msgstr "**时间戳和说话人识别使用说明**"

#: ../../source/models/model_abilities/audio.rst:531
msgid "Only the following models support `timestamp` and `speaker`:"
msgstr "仅以下模型支持 `时间戳` 和 `说话人` 识别："

#: ../../source/models/model_abilities/audio.rst:533
msgid "`paraformer-zh-spk`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:534
msgid "`paraformer-zh-long`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:535
#: ../../source/models/model_abilities/audio.rst:560
msgid "`seaco-paraformer-zh`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:537
msgid "Among them, only `paraformer-zh-spk` enables **speaker info by default**."
msgstr "其中，仅 `paraformer-zh-spk` 默认启用说话人识别功能。"

#: ../../source/models/model_abilities/audio.rst:539
msgid ""
"If you need speaker info when using `paraformer-zh-long` or `seaco-"
"paraformer-zh`:"
msgstr ""
"如果你使用的是 `paraformer-zh-long` 或 `seaco-paraformer-zh`，且需要启用"
"说话人识别功能："

#: ../../source/models/model_abilities/audio.rst:541
msgid ""
"In Web UI: add an extra parameter with key ``spk_model`` and value "
"``cam++``"
msgstr "在 Web UI 中：添加名为 ``spk_model``、值为 ``cam++`` 的参数"

#: ../../source/models/model_abilities/audio.rst:542
msgid "In command line: add the option ``--spk_model cam++``"
msgstr "在命令行中：添加参数 ``--spk_model cam++``"

#: ../../source/models/model_abilities/audio.rst:544
#: ../../source/models/model_abilities/audio.rst:562
msgid "Example:"
msgstr "示例："

#: ../../source/models/model_abilities/audio.rst:555
msgid "**Hotword Usage**"
msgstr "**热词功能使用说明**"

#: ../../source/models/model_abilities/audio.rst:557
msgid "Only the following models support `hotword`:"
msgstr "仅以下模型支持 `hotword` （热词功能）："

#: ../../source/models/model_abilities/audio.rst:559
msgid "`paraformer-zh-hotword`"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:576
msgid "SenseVoiceSmall Offline Usage"
msgstr "SenseVoiceSmall 离线使用"

#: ../../source/models/model_abilities/audio.rst:578
msgid ""
"Now SenseVoiceSmall use a small vad model ``fsmn-vad``, it will be "
"downloaded thus network required."
msgstr ""
"现在 SenseVoiceSmall 使用一个小的 VAD 模型 ``fsmn-vad``，因此它需要网络来"
"下载。"

#: ../../source/models/model_abilities/audio.rst:580
msgid "For offline environment, you can download the vad model in advance."
msgstr "对于离线环境，你可以提前下载这个 VAD 模型。"

#: ../../source/models/model_abilities/audio.rst:582
msgid ""
"Download from `huggingface <https://huggingface.co/funasr/fsmn-vad>`_ or "
"`modelscope <https://modelscope.cn/models/iic/speech_fsmn_vad_zh-cn-16k-"
"common-pytorch/files>`_. Assume downloaded to ``/path/to/fsmn-vad``."
msgstr ""
"从 `huggingface <https://huggingface.co/funasr/fsmn-vad>`_ 或者 `"
"modelscope <https://modelscope.cn/models/iic/speech_fsmn_vad_zh-cn-16k-"
"common-pytorch/files>`_ 下载。假设下载到 ``/path/to/fsmn-vad``。"

#: ../../source/models/model_abilities/audio.rst:585
msgid ""
"Then when launching SenseVoiceSmall with Web UI, you can add an "
"additional parameter with key ``vad_model`` and value ``/path/to/fsmn-"
"vad`` which is the downloaded path. When launching with command line, you"
" can add an option ``--vad_model /path/to/fsmn-vad``."
msgstr ""
"然后当用 Web UI 加载 SenseVoiceSmall 时，添加额外选项，key 是 ``vad_model"
"``，值是之前的下载路径 ``/path/to/fsmn-vad``。用命令行加载时，增加选项 ``"
"--vad_model /path/to/fsmn-vad``。"

#: ../../source/models/model_abilities/audio.rst:590
msgid "Kokoro Usage"
msgstr "Kokoro 模型使用"

#: ../../source/models/model_abilities/audio.rst:592
msgid ""
"The Kokoro model supports multiple languages, but the default language is"
" English. If you want to use other languages, such as Chinese, you need "
"to install additional dependency packages and add an additional parameter"
" when starting the model."
msgstr ""
"Kokoro模型支持多语言，默认是英文。如果你想使用非默认语言，例如中文，则"
"需要安装额外依赖包并且在模型启动时增加对应参数。"

#: ../../source/models/model_abilities/audio.rst:596
msgid "pip install misaki[zh]"
msgstr ""

#: ../../source/models/model_abilities/audio.rst:598
msgid ""
"Initialize the model with the parameter lang_code='z', For all available "
"``lang_code`` options, please refer to `kokoro source code "
"<https://github.com/hexgrad/kokoro/blob/main/kokoro/pipeline.py#L22>`_. "
"If the model is started through the web UI, an additional parameter needs"
" to be added, with the key as ``lang_code`` and the value as ``z``. If "
"the model is started through the xinference client, the parameters are "
"passed via the launch_model interface:"
msgstr ""
"使用 lang_code='z' 参数初始化模型，可以参考 `kokoro source code <https://"
"github.com/hexgrad/kokoro/blob/main/kokoro/pipeline.py#L22>`_ 查看所有"
"支持的 lang_code。如果你是通过 Web UI启动的模型，则需要添加额外参数，key"
"是 ``lang_code``，value是 ``z``。如果你是通过 xinference client启动的模型"
"，则可以参考如下代码传递参数："

#: ../../source/models/model_abilities/audio.rst:615
msgid ""
"When inferring, the voice must start with 'z', for example: "
"``zf_xiaoyi``. The currently supported voices are: "
"https://huggingface.co/hexgrad/Kokoro-82M/tree/main/voices. For example:"
msgstr ""
"当推理时，需要使用 'z' 开头的 voice，例如：``zf_xiaoyi``。目前支持的 "
"voices 可以参考 https://huggingface.co/hexgrad/Kokoro-82M/tree/main/"
"voices。使用方法如下："

#: ../../source/models/model_abilities/audio.rst:625
msgid "IndexTTS2 Usage"
msgstr "IndexTTS2 使用"

#: ../../source/models/model_abilities/audio.rst:627
msgid ""
"The IndexTTS2 model supports emotion control, you can use this feature by"
" specifying some additional parameters. Here are several examples of how "
"to use IndexTTS2:"
msgstr ""
"IndexTTS2模型支持情感控制，你可以通过使用一些额外的参数来时用这个功能。"
"以下为IndexTTS2的使用方式："

#: ../../source/models/model_abilities/audio.rst:630
msgid "Synthesize new speech with a single reference audio file (voice cloning):"
msgstr "单一参考音频（音色克隆）："

#: ../../source/models/model_abilities/audio.rst:646
msgid ""
"Using a separate, emotional reference audio file to condition the speech "
"synthesis:"
msgstr "指定情感参考音频："

#: ../../source/models/model_abilities/audio.rst:666
msgid ""
"When an emotional reference audio file is specified, you can optionally "
"set the ``emo_alpha`` to adjust how much it affects the output. Valid "
"range is ``0.0 - 1.0`` , and the default value is ``1.0`` (100%):"
msgstr ""
"当指定情感参考音频时，可以选择设置 ``emo_alpha`` 参数以调整其对输出的影响"
"程度。有效范围为 ``0.0 - 1.0`` ，默认值为 ``1.0`` (100%)。"

#: ../../source/models/model_abilities/audio.rst:689
msgid ""
"It's also possible to omit the emotional reference audio and instead "
"provide an 8-float list specifying the intensity of each emotion, in the "
"following order: ``[happy, angry, sad, afraid, disgusted, melancholic, "
"surprised, calm]`` . You can additionally use the ``use_random`` "
"parameter to introduce stochasticity during inference; the default is "
"``False`` , and setting it to ``True`` enables randomness:"
msgstr ""
"可以省略情绪参考音频，转而提供一个包含8个浮点数的列表，按以下顺序指定每种"
"情绪的强度: ``[快乐, 愤怒, 悲伤, 恐惧, 厌恶, 忧郁, 惊讶, 平静]`` 。您还可以"
"使用 ``use_random`` 参数在推理过程中引入随机性情绪；默认值为 ``False`` ，设置为 ``"
"True`` 即可启用随机性情绪。"

#: ../../source/models/model_abilities/audio.rst:712
msgid ""
"Alternatively, you can enable ``use_emo_text`` to guide the emotions "
"based on your provided ``text`` script. Your text script will then "
"automatically be converted into emotion vectors. It's recommended to use "
"``emo_alpha`` around 0.6 (or lower) when using the text emotion modes, "
"for more natural sounding speech. You can introduce randomness with "
"``use_random`` (default: ``False``; ``True`` enables randomness):"
msgstr ""
"或者，您可以启用 ``use_emo_text`` 功能，根据您提供的 ``text`` 脚本引导情感"
"表达。您的文本脚本将自动转换为情感向量。使用文本情感模式时，建议将 ``emo_"
"alpha`` 设置为 0.6 左右（或更低），以获得更自然的语音效果。您可通过 ``use_"
"random`` 引入随机性（默认值：``False`` ；``True`` 启用随机性）："

#: ../../source/models/model_abilities/audio.rst:737
msgid ""
"It's also possible to directly provide a specific text emotion "
"description via the ``emo_text`` parameter. Your emotion text will then "
"automatically be converted into emotion vectors. This gives you separate "
"control of the text script and the text emotion description:"
msgstr ""
"您也可以通过 ``emo_text`` 参数直接提供特定的文本情绪描述。您的情绪文本将"
"自动转换为情绪向量。这使您能够分别控制文本脚本和文本情绪描述："

#~ msgid "**random sampling**"
#~ msgstr ""

#~ msgid ""
#~ "Enabling random sampling will reduce the"
#~ " voice cloning fidelity of the speech"
#~ " synthesis."
#~ msgstr ""

#~ msgid ""
#~ "5.Alternatively, you can enable `use_emo_text`"
#~ " to guide the emotions based on"
#~ msgstr ""

#~ msgid ""
#~ "your provided `text` script. Your text"
#~ " script will then automatically be "
#~ "converted into emotion vectors. It's "
#~ "recommended to use `emo_alpha` around "
#~ "0.6 (or lower) when using the text"
#~ " emotion modes, for more natural "
#~ "sounding speech. You can introduce "
#~ "randomness with `use_random` (default: "
#~ "`False`; `True` enables randomness):"
#~ msgstr ""

