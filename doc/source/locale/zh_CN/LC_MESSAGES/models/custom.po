# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-28 14:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/models/custom.rst:5
msgid "Custom Models"
msgstr "自定义模型"

#: ../../source/models/custom.rst:6
msgid ""
"Xinference provides a flexible and comprehensive way to integrate, "
"manage, and utilize custom models."
msgstr "Xinference 提供了一种灵活而全面的方式来集成、管理和应用自定义模型。"

#: ../../source/models/custom.rst:10
msgid "Directly launch an existing model"
msgstr "无需注册而直接启动自定义模型"

#: ../../source/models/custom.rst:11
msgid ""
"Since ``v0.14.0``, you can directly launch an existing model by passing "
"``model_path`` to the launch interface without downloading it. This way "
"requires that the model's ``model_family`` is among the built-in "
"supported models, and eliminates the hassle of registering the model."
msgstr ""
"从 ``v0.14.0`` 版本开始，如果你需要注册的模型的家族是 Xinference 内置支持的模型，你可以直接通过 launch 接口中的 "
"``model_path`` 参数来启动它，从而免去注册步骤的麻烦。现在非常推荐使用这种方式。"

#: ../../source/models/custom.rst:15
msgid "For example:"
msgstr "例如："

#: ../../source/models/custom.rst:47
msgid ""
"The above example demonstrates how to directly launch a qwen1.5-chat "
"model file without registering it."
msgstr "上面的例子展示了当我已有 qwen1.5-chat 模型文件时，如何直接 launch 它。"

#: ../../source/models/custom.rst:49
msgid ""
"For distributed scenarios, if your model file is on a specific worker, "
"you can directly launch it using the ``worker_ip`` and ``model_path`` "
"parameters with the launch interface."
msgstr ""
"对于分布式场景，将你的模型文件置于某个 worker ，然后通过 launch 接口的 ``worker_ip`` 和 "
"``model_path`` 参数来达到直接 launch 的效果。"

#: ../../source/models/custom.rst:53
msgid ""
"For CLI usage, prefer ``--model-path`` (kebab-case). ``--model_path`` is "
"legacy-compatible but not recommended."
msgstr ""
"对于命令行界面（CLI）的使用，请优先使用 ``--model-path``（分号分隔的大小写混合形式）。``--model_path`` "
"兼容旧版规范，但不建议使用。"

#: ../../source/models/custom.rst:56
msgid "Define a custom model"
msgstr "定义一个自定义模型"

#: ../../source/models/custom.rst:59
msgid "Web UI: Automatic LLM Config Parsing"
msgstr "Web UI：自动解析大型语言模型配置"

#: ../../source/models/custom.rst:61
msgid ""
"When registering a custom LLM via the Web UI, Xinference can "
"automatically parse the model configuration and pre-fill key fields for "
"you."
msgstr "通过Web UI注册自定义LLM时，Xinference可自动解析模型配置并为您预填关键字段。"

#: ../../source/models/custom.rst:64
msgid "You only need to provide:"
msgstr "您仅需要提供："

#: ../../source/models/custom.rst:66
msgid "**Model path / Model ID** (where the model lives, local path or hub ID)"
msgstr " **模型路径/模型ID** （模型所在位置，本地路径或中心ID）"

#: ../../source/models/custom.rst:67
msgid "**Model Family**"
msgstr " **模型家族** "

#: ../../source/models/custom.rst:69
msgid "After parsing, the UI can auto-populate fields such as:"
msgstr "解析后，用户界面可自动填充以下字段："

#: ../../source/models/custom.rst:71
msgid "``Context Length``"
msgstr " ``上下文长度`` "

#: ../../source/models/custom.rst:72
msgid "``Model_Languages``"
msgstr " ``模型语言`` "

#: ../../source/models/custom.rst:73
msgid "``Model_Abilities``"
msgstr " ``模型能力`` "

#: ../../source/models/custom.rst:74
msgid "``Model_Specs``"
msgstr " ``模型规格`` "

#: ../../source/models/custom.rst:76
msgid "You can review and edit these fields before saving the custom model."
msgstr "在保存自定义模型之前，您可以查看并编辑这些字段。"

#: ../../source/models/custom.rst:78
msgid "Define a custom model based on the following templates:"
msgstr "基于以下模板定义一个自定义模型："

#: ../../source/models/custom.rst:82
msgid "LLM"
msgstr "语言模型"

#: ../../source/models/custom.rst:129
msgid "embedding"
msgstr "嵌入模型"

#: ../../source/models/custom.rst:164
msgid "Rerank"
msgstr "重排序模型"

#: ../../source/models/custom.rst:199
msgid "image"
msgstr "图像模型"

#: ../../source/models/custom.rst:234
msgid "audio"
msgstr "音频模型"

#: ../../source/models/custom.rst:267
msgid "flexible"
msgstr "灵活模型"

#: ../../source/models/custom.rst:294
msgid ""
"model_name: A string defining the name of the model. The name must start "
"with a letter or a digit and can only contain letters, digits, "
"underscores, or dashes."
msgstr "model_name: 模型名称。名称必须以字母或数字开头，且只能包含字母、数字、下划线或短划线。"

#: ../../source/models/custom.rst:295
msgid ""
"context_length: An optional integer that specifies the maximum context "
"size the model was trained to accommodate, encompassing both the input "
"and output lengths. If not defined, the default value is 2048 tokens "
"(~1,500 words)."
msgstr ""
"context_length: "
"一个可选的整数，模型支持的最大上下文长度，包括输入和输出长度。如果未定义，默认值为2048个token（约1,500个词）。"

#: ../../source/models/custom.rst:296
msgid ""
"dimensions: An interger defining the size of the vector output by the "
"embedding model."
msgstr "dimensions: 一个整数，用于定义嵌入模型输出的向量大小。"

#: ../../source/models/custom.rst:297
msgid ""
"max_tokens: An interger defining the maximum number of input tokens the "
"embedding model can process in a single request."
msgstr "max_tokens: 一个整数，定义嵌入模型在单次请求中可处理的最大输入token数量。"

#: ../../source/models/custom.rst:298
msgid ""
"model_lang: A list of strings representing the supported languages for "
"the model. Example: [\"en\"], which means that the model supports "
"English."
msgstr "model_lang: 一个字符串列表，表示模型支持的语言。例如：['en']，表示该模型支持英语。"

#: ../../source/models/custom.rst:299
msgid ""
"model_ability: A list of strings defining the abilities of the model. It "
"could include options like \"embed\", \"generate\", and \"chat\". In this"
" case, the model has the ability to \"generate\"."
msgstr ""
"model_ability: 一个字符串列表，定义模型的能力。它可以包括像 'embed'、'generate' 和 'chat' "
"这样的选项。示例表示模型具有 'generate' 的能力。"

#: ../../source/models/custom.rst:300
msgid ""
"model_family: A required string representing the family of the model you "
"want to register. This parameter must not conflict with any builtin model"
" names."
msgstr "model_family: 一个必要的字符串，表示要注册的模型族。该参数名称不得与任何内置模型名称冲突。"

#: ../../source/models/custom.rst:301
msgid ""
"model_specs: An array of objects defining the specifications of the "
"model. These include:"
msgstr "model_specs: 一个包含定义模型规格的对象数组。这些规格包括："

#: ../../source/models/custom.rst:302
msgid ""
"model_format: A string that defines the model format, like \"pytorch\" or"
" \"ggufv2\"."
msgstr "model_format: 一个定义模型格式的字符串，可以是 'pytorch' 或 'ggufv2'。"

#: ../../source/models/custom.rst:303
msgid ""
"model_size_in_billions: An integer defining the size of the model in "
"billions of parameters."
msgstr "model_size_in_billions: 一个整数，定义模型的参数量，以十亿为单位。"

#: ../../source/models/custom.rst:304
msgid ""
"quantizations: A list of strings defining the available quantizations for"
" the model. For PyTorch models, it could be \"4-bit\", \"8-bit\", or "
"\"none\". For ggufv2 models, the quantizations should correspond to "
"values that work with the ``model_file_name_template``. Some engines also"
" support ``fp4`` / ``fp8`` / ``bnb`` formats (see :ref:`installation` for"
" backend support details)."
msgstr ""
"quantizations: 一个字符串列表，定义模型的量化方式。对于 PyTorch 模型，它可以是 \"4-bit\"、\"8-bit\" 或"
" \"none\"。对于 ggufv2 模型，量化方式应与 ``model_file_name_template`` 中的值对应。"
"某些引擎还支持 ``fp4`` / ``fp8`` / ``bnb`` 格式（后端支持详情请参见 :ref:`installation` ）。"

#: ../../source/models/custom.rst:306
msgid ""
"model_id: A string representing the model ID, possibly referring to an "
"identifier used by Hugging Face. **If model_uri is missing, Xinference "
"will try to download the model from the huggingface repository specified "
"here.**."
msgstr ""
"model_id：代表模型 id 的字符串，可以是该模型对应的 HuggingFace 仓库 id。如果 model_uri "
"字段缺失，Xinference 将尝试从此id指示的HuggingFace仓库下载该模型。"

#: ../../source/models/custom.rst:307
msgid ""
"model_hub: A string representing where to download the model from, like "
"\"Huggingface\" or \"modelscope\""
msgstr "model_hub: 一个可选字符串，表示从何处下载模型，例如 HuggingFace 或 modelscope。"

#: ../../source/models/custom.rst:308
msgid ""
"model_uri: A string representing the URI where the model can be loaded "
"from, such as \"file:///path/to/llama-2-7b\". **When the model format is "
"ggufv2, model_uri must be the specific file path. When the model format "
"is pytorch, model_uri must be the path to the directory containing the "
"model files.** If model URI is absent, Xinference will try to download "
"the model from Hugging Face with the model ID."
msgstr ""
"model_uri：表示模型文件位置的字符串，例如本地目录：\"file:///path/to/llama-2-7b\"。当 "
"model_format 是 ggufv2 ，此字段必须是具体的模型文件路径。而当 model_format 是 pytorch "
"时，此字段必须是一个包含所有模型文件的目录。"

#: ../../source/models/custom.rst:309
msgid ""
"model_revision: A string representing the specific version or commit hash"
" of the model files to use from the repository."
msgstr "model_revision: 一个字符串，表示从存储库中使用的模型文件的具体版本或提交哈希值。"

#: ../../source/models/custom.rst:310
msgid ""
"chat_template: If ``model_ability`` includes ``chat`` , you must "
"configure this option to generate the correct full prompt during chat. "
"This is a Jinja template string. Usually, you can find it in the "
"``tokenizer_config.json`` file within the model directory."
msgstr ""
"chat_template：如果 ``model_ability`` 中包含 ``chat`` "
"，那么此选项必须配置以生成合适的完整提示词。这是一个 Jinja 模版字符串。通常，你可以在模型目录的 "
"``tokenizer_config.json`` 文件中找到。"

#: ../../source/models/custom.rst:311
msgid ""
"stop_token_ids: If ``model_ability`` includes ``chat`` , you can "
"configure this option to control when the model stops during chat. This "
"is a list of integers, and you can typically extract the corresponding "
"values from the ``generation_config.json`` or ``tokenizer_config.json`` "
"file in the model directory."
msgstr ""
"stop_token_ids：如果 ``model_ability`` 中包含 ``chat`` "
"，那么推荐配置此选项以合理控制对话的停止。这是一个包含整数的列表，你可以在模型目录的 ``generation_config.json`` 和 "
"``tokenizer_config.json`` 文件中提取相应的值。"

#: ../../source/models/custom.rst:312
msgid ""
"stop: If ``model_ability`` includes ``chat`` , you can configure this "
"option to control when the model stops during chat. This is a list of "
"strings, and you can typically extract the corresponding values from the "
"``generation_config.json`` or ``tokenizer_config.json`` file in the model"
" directory."
msgstr ""
"stop：如果 ``model_ability`` 中包含 ``chat`` "
"，那么推荐配置此选项以合理控制对话的停止。这是一个包含字符串的列表，你可以在模型目录的 ``tokenizer_config.json`` "
"文件中找到 token 值对应的字符串。"

#: ../../source/models/custom.rst:313
msgid ""
"reasoning_start_tag: A special token or prompt used to explicitly "
"instruct the LLM to begin its chain-of-thought or reasoning process in "
"its output."
msgstr "reasoning_start_tag: 一个特殊的 token 或 prompt，用于明确指示大语言模型在其输出中思维链或推理过程的起点。"

#: ../../source/models/custom.rst:314
msgid ""
"reasoning_end_tag: A special token or prompt used to explicitly mark the "
"end of the model's chain-of-thought or reasoning process in its output."
msgstr "reasoning_end_tag: 一个特殊的 token 或 prompt，用于明确指示大语言模型在其输出中思维链或推理过程的终点。"

#: ../../source/models/custom.rst:315
msgid ""
"cache_config: A string representing the parameters and rules for how the "
"system stores and manages temporary data (cache)."
msgstr "cache_config: 一个字符串，表示系统存储和管理临时数据（缓存）的参数。"

#: ../../source/models/custom.rst:316
msgid ""
"virtualenv: A settings object for model dependency isolation. Please "
"refer to :ref:`this document <virtualenv>` for details."
msgstr ""

#: ../../source/models/custom.rst:319
msgid "Register a Custom Model"
msgstr "注册一个自定义模型"

#: ../../source/models/custom.rst:321
msgid "Register a custom model programmatically:"
msgstr "以代码的方式注册自定义模型"

#: ../../source/models/custom.rst:336 ../../source/models/custom.rst:354
#: ../../source/models/custom.rst:369 ../../source/models/custom.rst:424
msgid "Or via CLI:"
msgstr "以命令行的方式"

#: ../../source/models/custom.rst:342
msgid ""
"Note that replace the ``<model_type>`` above with ``LLM``, ``embedding`` "
"or ``rerank``. The same as below."
msgstr "注意将以下部分的 ``<model_type>`` 替换为 ``LLM``、``embedding`` 或 ``rerank`` 。"

#: ../../source/models/custom.rst:346
msgid "List the Built-in and Custom Models"
msgstr "列举内置和自定义模型"

#: ../../source/models/custom.rst:348
msgid "List built-in and custom models programmatically:"
msgstr "以代码的方式列举内置和自定义模型"

#: ../../source/models/custom.rst:361
msgid "Launch the Custom Model"
msgstr "启动自定义模型"

#: ../../source/models/custom.rst:363
msgid "Launch the custom model programmatically:"
msgstr "以代码的方式启动自定义模型"

#: ../../source/models/custom.rst:376
msgid "Interact with the Custom Model"
msgstr "使用自定义模型"

#: ../../source/models/custom.rst:378
msgid "Invoke the model programmatically:"
msgstr "以代码的方式调用模型"

#: ../../source/models/custom.rst:385
msgid "Result:"
msgstr "结果为："

#: ../../source/models/custom.rst:409
#, python-brace-format
msgid "Or via CLI, replace ``${UID}`` with real model UID:"
msgstr "或者以命令行的方式，用实际的模型 UID 替换 ``${UID}``："

#: ../../source/models/custom.rst:416
msgid "Unregister the Custom Model"
msgstr "注销自定义模型"

#: ../../source/models/custom.rst:418
msgid "Unregister the custom model programmatically:"
msgstr "以代码的方式注销自定义模型"

#~ msgid ""
#~ "model_file_name_template: Required by gguf "
#~ "models. An f-string template used for"
#~ " defining the model file name based"
#~ " on the quantization. **Note that "
#~ "this field is just a template for"
#~ " the format of the ggufv2 model "
#~ "file, do not fill in the specific"
#~ " path of the model file.**"
#~ msgstr ""
#~ "model_file_name_template: gguf 模型所需。一个 f-string "
#~ "模板，用于根据量化定义模型文件名。注意，这里不要填入文件的路径。"

#~ msgid "Define a custom embedding model"
#~ msgstr "定义自定义 embedding 模型"

#~ msgid "Define a custom embedding model based on the following template:"
#~ msgstr "基于以下模板定义一个自定义 embedding 模型："

#~ msgid "dimensions: A integer that specifies the embedding dimensions."
#~ msgstr "dimensions: 表示 embedding 维度的整型值。"

#~ msgid ""
#~ "max_tokens: A integer that represents "
#~ "the max sequence length that the "
#~ "embedding model supports."
#~ msgstr "max_tokens: 表示 embedding 模型支持的最大输入序列长度的整型值。"

#~ msgid ""
#~ "language: A list of strings representing"
#~ " the supported languages for the "
#~ "model. Example: [\"en\"], which means "
#~ "that the model supports English."
#~ msgstr "model_lang: 一个字符串列表，表示模型支持的语言。例如：['en']，表示该模型支持英语。"

#~ msgid ""
#~ "model_id: A string representing the "
#~ "model ID, possibly referring to an "
#~ "identifier used by Hugging Face."
#~ msgstr "model_id: 一个表示模型标识的字符串，类似 HuggingFace 或 ModelScope 使用的标识符。"

#~ msgid ""
#~ "model_uri: A string representing the URI"
#~ " where the model can be loaded "
#~ "from, such as \"file:///path/to/your_model\". "
#~ "If model URI is absent, Xinference "
#~ "will try to download the model "
#~ "from Hugging Face with the model "
#~ "ID."
#~ msgstr ""
#~ "model_uri: 表示模型的 URI 的字符串，例如 "
#~ "\"file:///path/to/llama-2-7b\"。如果模型 URI 不存在，Xinference "
#~ "将尝试使用 model_id 从 HuggingFace 或 "
#~ "ModelScope 下载模型。"

#~ msgid "Define a custom Rerank model"
#~ msgstr "定义自定义 rerank 模型"

#~ msgid "Define a custom rerank model based on the following template:"
#~ msgstr "基于以下模板定义一个自定义大语言模型："

#~ msgid ""
#~ "type: A string defining the type "
#~ "of the model, including ``normal``, "
#~ "``LLM-based`` and ``LLM-based layerwise``."
#~ msgstr "type: 表示模型的类型，可选值包括 ``normal``、``LLM-based`` 和 ``LLM-based layerwise``。"

#~ msgid ""
#~ "virtualenv: An array refers to the "
#~ "name or path of a self-contained"
#~ " Python environment used to isolate "
#~ "dependencies required to run a specific"
#~ " model or project. Please refer to"
#~ " :ref:`this document <virtualenv>`."
#~ msgstr ""
#~ "virtualenv: 一个数组，指代用于隔离特定模型或项目运行所依赖的独立环境名称或路径。详情请阅读 "
#~ ":ref:`这个文档 <virtualenv>`。"

#~ msgid "**Model engine** (e.g., transformers / vllm / sglang)"
#~ msgstr ""

#~ msgid "``model_family`` / ``model_name``"
#~ msgstr ""

#~ msgid "``model_format``"
#~ msgstr ""

#~ msgid "``model_size_in_billions``"
#~ msgstr ""

#~ msgid "``quantization`` (if detectable)"
#~ msgstr ""

#~ msgid "``architectures`` and other model metadata (when available)"
#~ msgstr ""

