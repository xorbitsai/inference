# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-12-25 17:11+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"

#: ../../source/examples/ai_podcast.rst:5
msgid "Example: AI Podcast ğŸ™"
msgstr "ç¤ºä¾‹ï¼šæ™ºèƒ½æ’­å®¢ ğŸ™"

#: ../../source/examples/ai_podcast.rst:7
msgid "**Description**:"
msgstr "æè¿°"

#: ../../source/examples/ai_podcast.rst:9
msgid "ğŸ™ï¸AI Podcast - Voice Conversations with Multiple Agents on M2 Max ğŸ’»"
msgstr "ğŸ™ï¸AIæ’­å®¢ - åœ¨M2 MaxèŠ¯ç‰‡ä¸Šè¿›è¡Œå¤šæ™ºèƒ½ä½“è¯­éŸ³å¯¹è¯"

#: ../../source/examples/ai_podcast.rst:11
msgid "**Support Language** :"
msgstr "**æ”¯æŒè¯­è¨€**ï¼š"

#: ../../source/examples/ai_podcast.rst:13
msgid "English (AI_Podcast.py)"
msgstr "è‹±æ–‡å¯¹åº”ä»£ç æ–‡ä»¶ï¼šAI_Podcast.py"

#: ../../source/examples/ai_podcast.rst:15
msgid "Chinese (AI_Podcast_ZH.py)"
msgstr "ä¸­æ–‡å¯¹åº”ä»£ç æ–‡ä»¶ï¼šAI_Podcast_ZH.py"

#: ../../source/examples/ai_podcast.rst:17
msgid "**Used Technology (EN version)** :"
msgstr "è‹±æ–‡ç‰ˆæœ¬æ¶‰åŠæŠ€æœ¯ï¼š"

#: ../../source/examples/ai_podcast.rst:19
msgid ""
"@ `OpenAI <https://twitter.com/OpenAI>`_ 's `whisper "
"<https://pypi.org/project/openai-whisper/>`_"
msgstr ""
"@ `OpenAI <https://twitter.com/OpenAI>`_ `whisper <https://pypi.org/project/openai-whisper/>`_"

#: ../../source/examples/ai_podcast.rst:21
msgid ""
"@ `ggerganov <https://twitter.com/ggerganov>`_ 's `ggml "
"<https://github.com/ggerganov/ggml>`_"
msgstr ""
"@ `ggerganov <https://twitter.com/ggerganov>`_ `ggml <https://github.com/ggerganov/ggml>`_"

#: ../../source/examples/ai_podcast.rst:23
msgid ""
"@ `WizardLM_AI <https://twitter.com/WizardLM_AI>`_ 's `wizardlm v1.0 "
"<https://huggingface.co/WizardLM>`_"
msgstr ""
"@ `WizardLM_AI <https://twitter.com/WizardLM_AI>`_ `wizardlm v1.0 <https://huggingface.co/WizardLM>`_"

#: ../../source/examples/ai_podcast.rst:25/
msgid ""
"@ `lmsysorg <https://twitter.com/lmsysorg>`_ 's `vicuna v1.3 "
"<https://huggingface.co/lmsys/vicuna-7b-v1.3>`_"
msgstr ""
"@ `lmsysorg <https://twitter.com/lmsysorg>`_ `vicuna v1.3 <https://huggingface.co/lmsys/vicuna-7b-v1.3>`_"

#: ../../source/examples/ai_podcast.rst:27
msgid "@ `Xinference <https://github.com/xorbitsai/inference>`_ as a launcher"
msgstr "`Xinference <https://github.com/xorbitsai/inference>`_ ä½œä¸ºå¹³å°"

#: ../../source/examples/ai_podcast.rst:29
msgid "**Detailed Explanation on the Demo Functionality** :"
msgstr "**å…³äºæ¼”ç¤ºåŠŸèƒ½çš„è¯¦ç»†è¯´æ˜**ï¼š"

#: ../../source/examples/ai_podcast.rst:31
msgid ""
"Generate the Wizardlm Model and Vicuna Model when the program is "
"launching with Xorbits Inference. Initiate the Chatroom by giving the two"
" chatbot their names and telling them that there is a human user called "
"\"username\", where \"username\" is given by user's input. Initialize a "
"empty chat history for the chatroom."
msgstr ""
"å¯åŠ¨ XInference, éƒ¨ç½² Wizardlm æ¨¡å‹å’Œ Vicuna æ¨¡å‹ã€‚"
"é€šè¿‡ä¸ºä¸¤ä¸ªæ¨¡å‹æŒ‡å®šåç§°å¹¶å‘Šè¯‰å®ƒä»¬æœ‰ä¸€ä¸ªåä¸ºâ€œusernameâ€çš„äººç±»ç”¨æˆ·æ¥å¯åŠ¨èŠå¤©å®¤ï¼Œå…¶ä¸­â€œusernameâ€æ˜¯ç”±ç”¨æˆ·è¾“å…¥æä¾›çš„ã€‚ç„¶åä¸ºèŠå¤©å®¤åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„èŠå¤©å†å²ã€‚"

#: ../../source/examples/ai_podcast.rst:35
msgid ""
"Use Audio device to store recording into file, and transcribe the file "
"using OpenAI's Whisper to receive a human readable text as string."
msgstr ""
"ä½¿ç”¨éŸ³é¢‘è®¾å¤‡å°†å½•éŸ³å­˜å‚¨åˆ°æ–‡ä»¶ä¸­ï¼Œç„¶åä½¿ç”¨OpenAIçš„Whisperå°†æ–‡ä»¶è½¬å½•ä¸ºäººç±»å¯è¯»çš„æ–‡æœ¬å­—ç¬¦ä¸²ã€‚"

#: ../../source/examples/ai_podcast.rst:37
msgid ""
"Based on the input message string, determine which agents the user want "
"to talk to. Call the target agents and parse in the input string and chat"
" history for the model to generate."
msgstr ""
"åŸºäºè¾“å…¥çš„æ¶ˆæ¯å­—ç¬¦ä¸²ï¼Œç¡®å®šç”¨æˆ·æƒ³è¦ä¸å“ªäº›ä»£ç†ï¼ˆæ¨¡å‹ï¼‰è¿›è¡Œå¯¹è¯ã€‚è°ƒç”¨è¿™äº›ç›®æ ‡ä»£ç†å¹¶å°†ç”¨æˆ·è¾“å…¥å­—ç¬¦ä¸²å’ŒèŠå¤©å†å²ä½œä¸ºè¾“å…¥è®©æ¨¡å‹å»ç”Ÿæˆå¯¹åº”çš„å†…å®¹ã€‚"

#: ../../source/examples/ai_podcast.rst:40
msgid ""
"When the responses are ready, use Macos's \"Say\" Command to produce "
"audio through speaker. Each agents have their own voice while speaking."
msgstr ""
"å½“æ¨¡å‹çš„è¾“å‡ºå‡†å¤‡å¥½æ—¶ï¼Œä½¿ç”¨MacOSçš„â€œSayâ€å‘½ä»¤é€šè¿‡æ‰¬å£°å™¨ç”ŸæˆéŸ³é¢‘ã€‚æ¯ä¸ªä»£ç†åœ¨è¯´è¯æ—¶éƒ½æœ‰è‡ªå·±çš„å£°éŸ³ã€‚"

#: ../../source/examples/ai_podcast.rst:43
msgid ""
"Store the user input and the agent response into chat history, and "
"recursively looping the program until user explicitly says words like "
"\"see you\" in their responses."
msgstr ""
"å°†ç”¨æˆ·è¾“å…¥å’Œä»£ç†å“åº”å­˜å‚¨åˆ°èŠå¤©å†å²ä¸­ï¼Œå¹¶å¾ªç¯é€’å½’ç¨‹åºï¼Œç›´åˆ°ç”¨æˆ·æ˜ç¡®åœ¨å…¶å“åº”ä¸­è¯´å‡ºâ€œå†è§â€ä¹‹ç±»çš„è¯è¯­ã€‚"

#: ../../source/examples/ai_podcast.rst:46
msgid "**Highlight Features with Xinference** :"
msgstr "**Xinferenceçš„çªå‡ºç‰¹æ€§**ï¼š"

#: ../../source/examples/ai_podcast.rst:48
msgid ""
"With Xinference's distributed system, we can easily deploy two different "
"models in the same session and in the same \"chatroom\". With enough "
"resources, the framework can deploy any amount of models you like at the "
"same time."
msgstr ""
"å€ŸåŠ© Xinference çš„åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ¨åŒä¸€ä¼šè¯å’ŒåŒä¸€â€œèŠå¤©å®¤â€ä¸­éƒ¨ç½²ä¸¤ä¸ªä¸åŒçš„æ¨¡å‹ã€‚"
"åœ¨è¶³å¤Ÿçš„èµ„æºæƒ…å†µä¸‹ï¼Œè¯¥æ¡†æ¶å¯ä»¥åŒæ—¶éƒ¨ç½²ä»»æ„æ•°é‡çš„æ¨¡å‹ã€‚"

#: ../../source/examples/ai_podcast.rst:51
msgid ""
"With Xinference, you can deploy the model easily by just adding a few "
"lines of code. For examples, for launching the vicuna model in the demo, "
"just by::"
msgstr ""
"ä½¿ç”¨ Xinferenceï¼Œåªéœ€æ·»åŠ å‡ è¡Œä»£ç å°±å¯ä»¥è½»æ¾éƒ¨ç½²æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨æ¼”ç¤ºä¸­å¯åŠ¨vicunaæ¨¡å‹ï¼Œåªéœ€ï¼š"

#: ../../source/examples/ai_podcast.rst:68
msgid ""
"Then, the Xinference client will handle \"target model downloading and "
"caching\", \"set up environment and process for the model\", and \"run "
"the service at selected endpoint. \" You are now ready to play with your "
"llm model."
msgstr ""
"ç„¶åï¼ŒXinference å®¢æˆ·ç«¯å°†å¤„ç†â€œç›®æ ‡æ¨¡å‹çš„ä¸‹è½½å’Œç¼“å­˜â€ã€â€œä¸ºæ¨¡å‹è®¾ç½®ç¯å¢ƒå’Œè¿›ç¨‹â€ä»¥åŠâ€œåœ¨é€‰æ‹©çš„ç«¯ç‚¹è¿è¡ŒæœåŠ¡â€ã€‚ä½ ç°åœ¨å·²ç»å‡†å¤‡å¥½ä¸ä½ çš„ llm æ¨¡å‹äº¤äº’ã€‚"

#: ../../source/examples/ai_podcast.rst:71
msgid "**Original Demo Video** :"
msgstr "**åŸå§‹æ¼”ç¤ºè§†é¢‘**"

#: ../../source/examples/ai_podcast.rst:73
msgid ""
"`ğŸ™ï¸AI Podcast - Voice Conversations with Multiple Agents on M2 MaxğŸ’»ğŸ”¥ğŸ¤– <"
"https://twitter.com/yichaocheng/status/1679129417778442240>`_"
msgstr ""
"`ğŸ™ï¸AIæ’­å®¢ - åœ¨M2 MaxèŠ¯ç‰‡ä¸Šè¿›è¡Œå¤šæ™ºèƒ½ä½“è¯­éŸ³å¯¹è¯ğŸ’»ğŸ”¥ğŸ¤– <https://twitter.com/yichaocheng/status/1679129417778442240>`_"

#: ../../source/examples/ai_podcast.rst:75
msgid "**Source Code** :"
msgstr "**æºä»£ç **ï¼š"

#: ../../source/examples/ai_podcast.rst:77
msgid ""
"`AI_Podcast "
"<https://github.com/xorbitsai/inference/blob/main/examples/AI_podcast.py>`_"
" (English Version)"
msgstr ""
"`AIæ’­å®¢ <https://github.com/xorbitsai/inference/blob/main/examples/AI_podcast.py>`_ï¼ˆè‹±æ–‡ç‰ˆï¼‰"

#: ../../source/examples/ai_podcast.rst:79
msgid ""
"`AI_Podcast_ZH "
"<https://github.com/xorbitsai/inference/blob/main/examples/AI_podcast_ZH.py>`_"
" (Chinese Version)"
msgstr ""
"`AIæ’­å®¢ <https://github.com/xorbitsai/inference/blob/main/examples/AI_podcast_ZH.py>`_ï¼ˆä¸­æ–‡ç‰ˆï¼‰"
