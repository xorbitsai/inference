# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-12 18:34+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/user_guide/launch.rst:5
msgid "Model Launching Instructions"
msgstr "模型加载指南"

#: ../../source/user_guide/launch.rst:7
msgid "This document aims to provide a functional overview of model launching."
msgstr "本文档旨在提供模型加载的功能说明。"

#: ../../source/user_guide/launch.rst:10
msgid "Replica"
msgstr "副本"

#: ../../source/user_guide/launch.rst:12
msgid ""
"Replicas specify the number of model instances to load. For example, if "
"you have two GPUs and each can host one replica of the model, you can set"
" the replica count to 2. This way, two identical instances of the model "
"will be distributed across the two GPUs. Xinference automatically load-"
"balances requests to ensure even distribution across multiple GPUs. "
"Meanwhile, users see it as a single model, which greatly improves overall"
" resource utilization."
msgstr ""
"副本用来指定模型加载的实例份数。比如，你有两张 GPU，每张卡可以放下模型的"
"一个副本，你可以设置副本数为 2。这样，两个完全相同的模型实例将分布在这"
"两张 GPU 上。Xinference 会自动进行负载均衡，确保请求均匀分配到多张卡上。"
"用户看到的仍是一个模型，这大大提升了整体资源利用率。"

#: ../../source/user_guide/launch.rst:17
msgid "Traditional Multi-Instance Deployment："
msgstr "旧版本多实例部署："

#: ../../source/user_guide/launch.rst:19
msgid ""
"Before v1.15.0：When you have multiple GPU cards, each capable of hosting"
" one model instance, you can set the number of instances equal to the "
"number of GPUs. For example:"
msgstr ""
"在v1.15.0版本前：当您拥有多张GPU显卡时，每张显卡可承载一个模型实例，此时"
"可将实例数量设置为等于GPU数量。例如:"

#: ../../source/user_guide/launch.rst:21
msgid "2 GPUs, 2 instances: Each GPU runs one model instance"
msgstr "2张GPU，2个实例：每张GPU运行一个模型实例"

#: ../../source/user_guide/launch.rst:22
msgid "4 GPUs, 4 instances: Each GPU runs one model instance"
msgstr "4张GPU，4个实例：每张GPU运行一个模型实例"

#: ../../source/user_guide/launch.rst:26
msgid "Introduce a new environment variable:"
msgstr "引入一个新的环境变量:"

#: ../../source/user_guide/launch.rst:32
msgid ""
"Control whether to enable the single GPU multi-copy feature Default "
"value: 1"
msgstr "控制是否启用单GPU多副本功能，默认值：1"

#: ../../source/user_guide/launch.rst:35
msgid "New Feature: Smart Replica Deployment"
msgstr "新功能：智能副本部署"

#: ../../source/user_guide/launch.rst:37
msgid "Single GPU Multi-Replica"
msgstr "单GPU多副本"

#: ../../source/user_guide/launch.rst:39
msgid "New Support: Run multiple model replicas even with just one GPU."
msgstr "新增支持：即使仅有一块GPU，也能运行多个模型副本。"

#: ../../source/user_guide/launch.rst:41
msgid "Scenario: You have 1 GPU with sufficient VRAM"
msgstr "场景：您拥有1个GPU且显存充足"

#: ../../source/user_guide/launch.rst:42
msgid "Configuration: Replica Count = 3, GPU Count = 1"
msgstr "配置：副本数量=3，GPU数量=1"

#: ../../source/user_guide/launch.rst:43
msgid "Result: 3 model instances running on the same GPU, sharing GPU resources"
msgstr "结果：3个模型实例，在同一GPU上运行，共享GPU资源"

#: ../../source/user_guide/launch.rst:45
msgid "Hybrid GPU Allocation"
msgstr "混合GPU分配"

#: ../../source/user_guide/launch.rst:47
msgid ""
"Smart Allocation: Number of replicas may differ from GPU count; system "
"intelligently distributes"
msgstr "智能分配: 副本数可以不等于GPU数量，系统会智能分配"

#: ../../source/user_guide/launch.rst:49
msgid "Scenario: You have 2 GPUs and need 3 replicas"
msgstr "场景: 你有2张GPU，需要3个副本"

#: ../../source/user_guide/launch.rst:50
msgid "Configuration: Replicas=3, GPUs=2"
msgstr "配置: 副本数=3，GPU数量=2"

#: ../../source/user_guide/launch.rst:51
msgid "Result: GPU0 runs 2 instances, GPU1 runs 1 instance"
msgstr "结果: GPU0运行2个实例，GPU1运行1个实例"

#: ../../source/user_guide/launch.rst:54
msgid "GPU Allocation Strategy"
msgstr "GPU分配策略"

#: ../../source/user_guide/launch.rst:56
msgid ""
"The current policy is *Idle Priority*: The scheduler always attempts to "
"assign replicas to the least utilized GPU. Use the "
"``XINFERENCE_LAUNCH_ALLOWED_GPUS`` parameter to restrict the range of "
"available GPUs."
msgstr ""
"当前策略为 *空闲优先* ：调度器始终尝试将副本分配至最空闲的GPU。"
"使用 ``XINFERENCE_LAUNCH_ALLOWED_GPUS`` 参数限制可选GPU范围。"

#: ../../source/user_guide/launch.rst:59
msgid "Set Environment Variables"
msgstr "设置环境变量"

#: ../../source/user_guide/launch.rst:63
msgid ""
"Sometimes, we want to specify environment variables for a particular "
"model at runtime. Since v1.8.1, Xinference provides the capability to "
"configure these individually without needing to set them before starting "
"Xinference."
msgstr ""
"有时我们希望在运行时为特定模型指定环境变量。从 v1.8.1 开始，Xinference "
"提供了单独配置环境变量的功能，无需在启动 Xinference 前设置。"

#: ../../source/user_guide/launch.rst:66
msgid "For Web UI."
msgstr "针对 Web UI。"

#: ../../source/user_guide/launch.rst:72
msgid ""
"When using the command line, use ``--env`` to specify an environment "
"variable."
msgstr "命令行使用时，使用 ``--env`` 指定环境变量。"

#: ../../source/user_guide/launch.rst:74
msgid "Example usage:"
msgstr "示例用法："

#: ../../source/user_guide/launch.rst:80
msgid ""
"Take vLLM as an example: it has versions V1 and V0, and by default, it "
"automatically determines which version to use. If you want to force the "
"use of V0 by setting ``VLLM_USE_V1=0`` when launching a model, you can "
"specify this during model launching."
msgstr ""
"以 vLLM 为例，它有 V1 和 V0 两个版本，默认会自动判定使用哪个版本。如果想"
"在加载模型时强制通过设置 ``VLLM_USE_V1=0`` 来使用 V0，可以指定该环境变量"
"。"

#: ../../source/user_guide/launch.rst:84
msgid "Configuring Model Virtual Environment"
msgstr "配置模型虚拟空间"

#: ../../source/user_guide/launch.rst:88
msgid ""
"For this part, please refer to :ref:`toggling virtual environments and "
"customizing dependencies <model_launching_virtualenv>`."
msgstr ""
"对于这部分，请参考 :ref:`开关虚拟空间和定制依赖 <model_launching_"
"virtualenv>`。"

#~ msgid ""
#~ "The current strategy is *idle-first "
#~ "with a first round spread*: the "
#~ "scheduler first tries to place one "
#~ "replica on each available GPU (always"
#~ " picking the emptiest unused GPU). "
#~ "Once every GPU has at least one"
#~ " replica, remaining replicas keep stacking"
#~ " onto the GPU that is currently "
#~ "the emptiest (single-GPU multi-replica"
#~ " is allowed). Use "
#~ "``XINFERENCE_LAUNCH_ALLOWED_GPUS`` to limit which"
#~ " GPUs can be chosen."
#~ msgstr ""
#~ "当前策略为 *空闲优先且首轮分散* ："
#~ "调度器首先尝试将每个副本分配至可用GPU"
#~ "（始终选择最空闲的未用GPU）。当每块"
#~ "GPU至少承载一个副本后，剩余副本将持续"
#~ "堆叠至当前最空闲的GPU（允许单GPU承载"
#~ "多个副本）。使用 ``XINFERENCE_LAUNCH_"
#~ "ALLOWED_GPUS`` 参数限制可选GPU范围。"

