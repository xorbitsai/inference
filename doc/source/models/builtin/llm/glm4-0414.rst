.. _models_llm_glm4-0414:

========================================
glm4-0414
========================================

- **Context Length:** 32768
- **Model Name:** glm4-0414
- **Languages:** en, zh
- **Abilities:** chat, tools
- **Description:** The GLM family welcomes new members, the GLM-4-32B-0414 series models, featuring 32 billion parameters. Its performance is comparable to OpenAI’s GPT series and DeepSeek’s V3/R1 series

Specifications
^^^^^^^^^^^^^^


Model Spec 1 (pytorch, 9 Billion)
++++++++++++++++++++++++++++++++++++++++

- **Model Format:** pytorch
- **Model Size (in billions):** 9
- **Quantizations:** none
- **Engines**: vLLM, Transformers
- **Model ID:** THUDM/GLM-4-9B-0414
- **Model Hubs**:  `Hugging Face <https://huggingface.co/THUDM/GLM-4-9B-0414>`__, `ModelScope <https://modelscope.cn/models/ZhipuAI/GLM-4-9B-0414>`__

Execute the following command to launch the model, remember to replace ``${quantization}`` with your
chosen quantization method from the options listed above::

   xinference launch --model-engine ${engine} --model-name glm4-0414 --size-in-billions 9 --model-format pytorch --quantization ${quantization}


Model Spec 2 (pytorch, 32 Billion)
++++++++++++++++++++++++++++++++++++++++

- **Model Format:** pytorch
- **Model Size (in billions):** 32
- **Quantizations:** none
- **Engines**: vLLM, Transformers
- **Model ID:** THUDM/GLM-4-32B-0414
- **Model Hubs**:  `Hugging Face <https://huggingface.co/THUDM/GLM-4-32B-0414>`__, `ModelScope <https://modelscope.cn/models/ZhipuAI/GLM-4-32B-0414>`__

Execute the following command to launch the model, remember to replace ``${quantization}`` with your
chosen quantization method from the options listed above::

   xinference launch --model-engine ${engine} --model-name glm4-0414 --size-in-billions 32 --model-format pytorch --quantization ${quantization}


Model Spec 3 (mlx, 9 Billion)
++++++++++++++++++++++++++++++++++++++++

- **Model Format:** mlx
- **Model Size (in billions):** 9
- **Quantizations:** 4bit, 6bit, 8bit, bf16
- **Engines**: 
- **Model ID:** mlx-community/GLM-4-9B-0414-{quantization}
- **Model Hubs**:  `Hugging Face <https://huggingface.co/mlx-community/GLM-4-9B-0414-{quantization}>`__, `ModelScope <https://modelscope.cn/models/mlx-community/GLM-4-9B-0414-{quantization}>`__

Execute the following command to launch the model, remember to replace ``${quantization}`` with your
chosen quantization method from the options listed above::

   xinference launch --model-engine ${engine} --model-name glm4-0414 --size-in-billions 9 --model-format mlx --quantization ${quantization}


Model Spec 4 (mlx, 32 Billion)
++++++++++++++++++++++++++++++++++++++++

- **Model Format:** mlx
- **Model Size (in billions):** 32
- **Quantizations:** 4bit, 8bit
- **Engines**: 
- **Model ID:** mlx-community/GLM-4-32B-0414-{quantization}
- **Model Hubs**:  `Hugging Face <https://huggingface.co/mlx-community/GLM-4-32B-0414-{quantization}>`__, `ModelScope <https://modelscope.cn/models/mlx-community/GLM-4-32B-0414-{quantization}>`__

Execute the following command to launch the model, remember to replace ``${quantization}`` with your
chosen quantization method from the options listed above::

   xinference launch --model-engine ${engine} --model-name glm4-0414 --size-in-billions 32 --model-format mlx --quantization ${quantization}


Model Spec 5 (ggufv2, 9 Billion)
++++++++++++++++++++++++++++++++++++++++

- **Model Format:** ggufv2
- **Model Size (in billions):** 9
- **Quantizations:** IQ2_M, IQ3_M, IQ3_XS, IQ3_XXS, IQ4_NL, IQ4_XS, Q2_K, Q2_K_L, Q3_K_L, Q3_K_M, Q3_K_S, Q3_K_XL, Q4_0, Q4_1, Q4_K_L, Q4_K_M, Q4_K_S, Q5_K_L, Q5_K_M, Q5_K_S, Q6_K, Q6_K_L, Q8_0, bf16
- **Engines**: vLLM, llama.cpp
- **Model ID:** bartowski/THUDM_GLM-4-9B-0414-GGUF
- **Model Hubs**:  `Hugging Face <https://huggingface.co/bartowski/THUDM_GLM-4-9B-0414-GGUF>`__, `ModelScope <https://modelscope.cn/models/bartowski/THUDM_GLM-4-9B-0414-GGUF>`__

Execute the following command to launch the model, remember to replace ``${quantization}`` with your
chosen quantization method from the options listed above::

   xinference launch --model-engine ${engine} --model-name glm4-0414 --size-in-billions 9 --model-format ggufv2 --quantization ${quantization}


Model Spec 6 (ggufv2, 32 Billion)
++++++++++++++++++++++++++++++++++++++++

- **Model Format:** ggufv2
- **Model Size (in billions):** 32
- **Quantizations:** IQ2_M, IQ2_S, IQ2_XS, IQ3_M, IQ3_XS, IQ3_XXS, IQ4_NL, IQ4_XS, Q2_K, Q2_K_L, Q3_K_L, Q3_K_M, Q3_K_S, Q3_K_XL, Q4_0, Q4_1, Q4_K_L, Q4_K_M, Q4_K_S, Q5_K_L, Q5_K_M, Q5_K_S, Q6_K, Q6_K_L, Q8_0
- **Engines**: vLLM, llama.cpp
- **Model ID:** bartowski/THUDM_GLM-4-9B-0414-GGUF
- **Model Hubs**:  `Hugging Face <https://huggingface.co/bartowski/THUDM_GLM-4-9B-0414-GGUF>`__, `ModelScope <https://modelscope.cn/models/bartowski/THUDM_GLM-4-9B-0414-GGUF>`__

Execute the following command to launch the model, remember to replace ``${quantization}`` with your
chosen quantization method from the options listed above::

   xinference launch --model-engine ${engine} --model-name glm4-0414 --size-in-billions 32 --model-format ggufv2 --quantization ${quantization}

