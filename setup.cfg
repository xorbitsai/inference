[metadata]
name = xinference
description = Model Serving Made Easy
author = Qin Xuye
author_email = qinxuye@xprobe.io
maintainer = Qin Xuye
maintainer_email = qinxuye@xprobe.io
license = Apache License 2.0
url = https://github.com/xorbitsai/inference
python_requires = >=3.9
classifier =
    Operating System :: OS Independent
    Programming Language :: Python
    Programming Language :: Python :: 3
    Programming Language :: Python :: 3.9
    Programming Language :: Python :: 3.10
    Programming Language :: Python :: 3.11
    Programming Language :: Python :: 3.12
    Programming Language :: Python :: Implementation :: CPython
    Topic :: Software Development :: Libraries

[options]
zip_safe = False
include_package_data = True
packages = find:
install_requires =
    xoscar>=0.3.0
    torch
    gradio
    pillow
    click
    tqdm>=4.27
    tabulate
    requests
    pydantic
    fastapi>=0.110.3
    uvicorn
    huggingface-hub>=0.19.4
    typing_extensions
    modelscope>=1.10.0
    sse_starlette>=1.6.5  # ensure_bytes API break change: https://github.com/sysid/sse-starlette/issues/65
    openai>1  # For typing
    python-jose[cryptography]
    passlib[bcrypt]
    aioprometheus[starlette]>=23.12.0
    nvidia-ml-py
    async-timeout
    peft
    timm
    setproctitle

[options.packages.find]
exclude =
    *.conftest*
    *.tests.*
    *.tests

[options.extras_require]
dev =
    cython>=0.29
    pytest>=3.5.0
    pytest-cov>=2.5.0
    pytest-timeout>=1.2.0
    pytest-forked>=1.0
    pytest-asyncio>=0.14.0
    pytest-mock>=3.11.1
    ipython>=6.5.0
    sphinx>=3.0.0
    pydata-sphinx-theme>=0.3.0
    sphinx-intl>=0.9.9
    jieba>=0.42.0
    flake8>=3.8.0
    black
    openai>1
    langchain
    langchain-community
    orjson
    sphinx-tabs
    sphinx-design
all =
    llama-cpp-python>=0.2.25,!=0.2.58
    transformers>=4.43.2
    torch>=2.0.0  # >=2.0 For CosyVoice
    accelerate>=0.28.0
    sentencepiece
    transformers_stream_generator
    bitsandbytes
    protobuf
    einops
    tiktoken>=0.6.0
    sentence-transformers>=3.1.0
    vllm>=0.2.6 ; sys_platform=='linux'
    diffusers>=0.30.0
    imageio-ffmpeg  # For video
    controlnet_aux
    orjson
    auto-gptq ; sys_platform!='darwin'
    autoawq<0.2.6 ; sys_platform!='darwin'  # autoawq 0.2.6 pinned torch to 2.3
    optimum
    outlines>=0.0.34
    sglang>=0.2.7 ; sys_platform=='linux'
    mlx-lm ; sys_platform=='darwin' and platform_machine=='arm64'
    attrdict  # For deepseek VL
    timm>=0.9.16  # For deepseek VL
    torchvision  # For deepseek VL
    FlagEmbedding  # For rerank
    funasr
    omegaconf~=2.3.0  # For ChatTTS
    nemo_text_processing<1.1.0  # 1.1.0 requires pynini==2.1.6.post1
    WeTextProcessing<1.0.4  # 1.0.4 requires pynini==2.1.6
    librosa  # For ChatTTS
    xxhash  # For ChatTTS
    torchaudio  # For ChatTTS
    ChatTTS>=0.2.1
    lightning>=2.0.0  # For CosyVoice, matcha
    hydra-core>=1.3.2  # For CosyVoice, matcha
    inflect  # For CosyVoice, matcha
    conformer  # For CosyVoice, matcha
    diffusers>=0.30.0  # For CosyVoice, matcha
    gdown  # For CosyVoice, matcha
    pyarrow  # For CosyVoice, matcha
    HyperPyYAML  # For CosyVoice
    onnxruntime==1.16.0  # For CosyVoice, use onnxruntime-gpu==1.16.0 if possible
    boto3>=1.28.55,<1.28.65 # For tensorizer
    tensorizer~=2.9.0
    eva-decord  # For video in VL
    jj-pytorchvideo # For CogVLM2-video
    loguru  # For Fish Speech
    natsort  # For Fish Speech
    loralib  # For Fish Speech
    ormsgpack  # For Fish Speech
    cachetools  # For Fish Speech
    silero-vad  # For Fish Speech
    qwen-vl-utils # For qwen2-vl
    datamodel_code_generator # for minicpm-4B
    jsonschema # for minicpm-4B
    verovio>=4.3.1  # For got_ocr2
    accelerate>=0.28.0  # For got_ocr2
intel =
    torch==2.1.0a0
    intel_extension_for_pytorch==2.1.10+xpu
llama_cpp =
    llama-cpp-python>=0.2.25,!=0.2.58
transformers =
    transformers>=4.43.2
    torch
    accelerate>=0.28.0
    sentencepiece
    transformers_stream_generator
    bitsandbytes
    protobuf
    einops
    tiktoken
    auto-gptq ; sys_platform!='darwin'
    autoawq<0.2.6 ; sys_platform!='darwin'  # autoawq 0.2.6 pinned torch to 2.3
    optimum
    attrdict  # For deepseek VL
    timm>=0.9.16  # For deepseek VL
    torchvision  # For deepseek VL
    peft
    eva-decord  # For video in VL
    jj-pytorchvideo # For CogVLM2-video
    qwen-vl-utils # For qwen2-vl
    datamodel_code_generator # for minicpm-4B
    jsonschema # for minicpm-4B
vllm =
    vllm>=0.2.6
sglang =
    sglang>=0.2.7 ; sys_platform=='linux'
    vllm>=0.5.2 ; sys_platform=='linux'
    outlines>=0.0.34
mlx =
    mlx-lm
embedding =
    sentence-transformers>=3.1.0
rerank =
    FlagEmbedding
image =
    diffusers>=0.30.0  # fix conflict with matcha-tts
    controlnet_aux
    deepcache
    verovio>=4.3.1  # For got_ocr2
    transformers>=4.37.2  # For got_ocr2
    tiktoken>=0.6.0  # For got_ocr2
    accelerate>=0.28.0  # For got_ocr2
    torch  # For got_ocr2
    torchvision  # For got_ocr2
video =
    diffusers>=0.30.0
    imageio-ffmpeg
audio =
    funasr
    omegaconf~=2.3.0
    nemo_text_processing<1.1.0  # 1.1.0 requires pynini==2.1.6.post1
    WeTextProcessing<1.0.4  # 1.0.4 requires pynini==2.1.6
    librosa
    xxhash
    torchaudio
    ChatTTS>=0.2.1
    tiktoken  # For CosyVoice, openai-whisper
    torch>=2.0.0  # For CosyVoice, matcha
    lightning>=2.0.0  # For CosyVoice, matcha
    hydra-core>=1.3.2  # For CosyVoice, matcha
    inflect  # For CosyVoice, matcha
    conformer  # For CosyVoice, matcha
    diffusers>=0.30.0  # For CosyVoice, matcha
    gdown  # For CosyVoice, matcha
    pyarrow  # For CosyVoice, matcha
    HyperPyYAML  # For CosyVoice
    onnxruntime==1.16.0  # For CosyVoice, use onnxruntime-gpu==1.16.0 if possible
    loguru  # For Fish Speech
    natsort  # For Fish Speech
    loralib  # For Fish Speech
    ormsgpack  # For Fish Speech
    cachetools  # For Fish Speech
    silero-vad  # For Fish Speech
doc =
    ipython>=6.5.0
    sphinx>=3.0.0
    pydata-sphinx-theme>=0.3.0
    sphinx-intl>=0.9.9
    sphinx-tabs
    sphinx-design
    prometheus_client
    timm
benchmark =
    psutil

[options.entry_points]
console_scripts =
    xinference = xinference.deploy.cmdline:cli
    xinference-local = xinference.deploy.cmdline:local
    xinference-supervisor = xinference.deploy.cmdline:supervisor
    xinference-worker = xinference.deploy.cmdline:worker

[coverage:run]
branch = True
relative_files = True
cover_pylib = False
plugins = Cython.Coverage
include =
    xinference/*
omit =
    xinference/_version.py
    *.pxd
    */tests/*

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    return NotImplemented

[versioneer]
VCS = git
style = pep440
versionfile_source = xinference/_version.py
versionfile_build = xinference/_version.py
tag_prefix = v
parentdir_prefix = xinference-

[flake8]
max-line-length = 100
select =
    E9,
    E101,
    E111,
    E117,
    E127,
    E201,
    E202,
    E223,
    E224,
    E225,
    E231,
    E242,
    E251,
    E273,
    E274,
    E275,
    E301,
    E302,
    E303,
    E304,
    E305,
    E401,
    E703,
    E901,
    E999,
    F7,
    F63,
    F82,
    F401,
    F811,
    F821,
    F822,
    F823,
    F841,
    W191,
    W291,
    W292,
    W293,
    W391,
    W601,
    W602,
    W603,
    W604,
    W605
exclude =
    __init__.py
    __pycache__
    .git/
    .github/
    build/
    ci/
    dist/
    docs/
    thirdparty

[codespell]
ignore-words-list = hist,rcall,fpr,ser,nd,inout,ot,Ba,ba,asend,hart,coo,splitted,datas,fro
skip = .idea,.git,./build,./docs/build,node_modules,static,generated,*.po,*.ts,*.json,*.c,*.cpp,*.cfg,thirdparty

[isort]
profile = black
skip = thirdparty

[mypy]
ignore_missing_imports=True
follow_imports=skip
exclude = thirdparty
