[metadata]
name = xinference
description = Model Serving Made Easy
author = Qin Xuye
author_email = qinxuye@xprobe.io
maintainer = Qin Xuye
maintainer_email = qinxuye@xprobe.io
license = Apache License 2.0
url = https://github.com/xorbitsai/inference
python_requires = >=3.10
classifier =
    Operating System :: OS Independent
    Programming Language :: Python
    Programming Language :: Python :: 3
    Programming Language :: Python :: 3.10
    Programming Language :: Python :: 3.11
    Programming Language :: Python :: 3.12
    Programming Language :: Python :: 3.13
    Programming Language :: Python :: Implementation :: CPython
    Topic :: Software Development :: Libraries

[options]
zip_safe = False
include_package_data = True
packages = find:
install_requires =
    xoscar>=0.7.13
    torch
    gradio<6.0.0
    pillow
    click<8.2.0
    tqdm>=4.27
    tabulate
    requests
    aiohttp
    pydantic
    fastapi>=0.110.3
    uvicorn
    huggingface-hub>=0.19.4
    typing_extensions
    modelscope>=1.19.0
    sse_starlette>=1.6.5  # ensure_bytes API break change: https://github.com/sysid/sse-starlette/issues/65
    openai>=1.40.0  # For typing
    python-jose[cryptography]
    bcrypt>=4.0.0
    aioprometheus[starlette]>=23.12.0
    nvidia-ml-py
    pynvml>=12
    async-timeout
    peft<=0.17.1
    timm
    setproctitle

[options.packages.find]
exclude =
    *.conftest*
    *.tests.*
    *.tests

[options.extras_require]
dev =
    cython>=0.29
    pytest>=3.5.0
    pytest-cov>=2.5.0
    pytest-timeout>=1.2.0
    pytest-forked>=1.0
    pytest-asyncio>=0.14.0
    pytest-mock>=3.11.1
    ipython>=6.5.0
    sphinx>=3.0.0
    pydata-sphinx-theme>=0.3.0
    sphinx-intl>=0.9.9
    jieba>=0.42.0
    flake8>=3.8.0
    black
    openai>=1.40.0
    anthropic
    langchain
    langchain-community
    langchain-openai
    orjson
    sphinx-tabs
    sphinx-design
all =
    %(anthropic)s
    %(virtualenv)s
    %(llama_cpp)s
    %(transformers)s
    %(vllm)s
    %(mlx)s
    %(embedding)s
    %(rerank)s
    %(image)s
    %(video)s
    %(audio)s
intel =
    torch==2.1.0a0
    intel_extension_for_pytorch==2.1.10+xpu
llama_cpp =
    xllamacpp>=0.2.0
transformers =
    transformers>=4.46.0
    torch
    accelerate>=0.28.0
    sentencepiece
    transformers_stream_generator
    bitsandbytes ; sys_platform=='linux'
    protobuf
    einops
    tiktoken
    optimum
    attrdict  # For deepseek VL
    timm>=0.9.16  # For deepseek VL
    torchvision  # For deepseek VL
    peft
    eva-decord  # For video in VL
    jj-pytorchvideo # For CogVLM2-video
    qwen-vl-utils!=0.0.9 # For qwen2-vl
    qwen_omni_utils  # For qwen2.5-omni
    datamodel_code_generator  # for minicpm-4B
    jsonschema  # for minicpm-4B
    blobfile  # for moonlight-16b-a3b
transformers_quantization =
    bitsandbytes ; sys_platform=='linux'
    gptqmodel
    autoawq!=0.2.6 ; sys_platform!='darwin'  # autoawq 0.2.6 pinned torch to 2.3
    datasets>=3.4.0  # see GH#3934
vllm =
    vllm>=0.2.6 ; sys_platform=='linux'
    xxhash
sglang =
    sglang[srt]>=0.4.2.post4 ; sys_platform=='linux'
mlx =
    mlx-lm>=0.21.5 ; sys_platform=='darwin' and platform_machine=='arm64'
    mlx-vlm>=0.3.4 ; sys_platform=='darwin' and platform_machine=='arm64'
    mlx-whisper ; sys_platform=='darwin' and platform_machine=='arm64'
    f5-tts-mlx ; sys_platform=='darwin' and platform_machine=='arm64'
    mlx-audio ; sys_platform=='darwin' and platform_machine=='arm64'
    qwen_vl_utils!=0.0.9
    tomli
embedding =
    sentence-transformers>=3.1.0
    FlagEmbedding
    datasets>=3.4.0  # see GH#3934
rerank =
    FlagEmbedding
    datasets>=3.4.0  # see GH#3934
image =
    diffusers>=0.32.0  # fix conflict with matcha-tts
    controlnet_aux
    deepcache
    verovio>=4.3.1  # For got_ocr2
    transformers>=4.37.2  # For got_ocr2
    tiktoken>=0.6.0  # For got_ocr2
    accelerate>=0.28.0  # For got_ocr2
    torch  # For got_ocr2
    torchvision  # For got_ocr2
    gguf  # For flux and sd3.5 series
video =
    diffusers>=0.32.0
    imageio-ffmpeg
audio =
    funasr==1.2.7
    omegaconf~=2.3.0
    nemo_text_processing<=1.1.0; sys_platform == 'linux'  # 1.1.0 requires pynini==2.1.6.post1
    WeText
    librosa
    xxhash
    torch>=2.0.0  # for CosyVoice, matcha
    torchaudio>=2.0.0
    ChatTTS>=0.2.1
    tiktoken # For CosyVoice, openai-whisper
    lightning>=2.0.0  # For CosyVoice, matcha
    hydra-core>=1.3.2  # For CosyVoice, matcha
    inflect  # For CosyVoice, matcha
    conformer  # For CosyVoice, matcha
    diffusers>=0.32.0  # For CosyVoice, matcha
    gdown  # For CosyVoice, matcha
    pyarrow  # For CosyVoice, matcha
    HyperPyYAML  # For CosyVoice
    onnxruntime>=1.16.0  # For CosyVoice, use onnxruntime-gpu==1.16.0 if possible
    pyworld>=0.3.4  # For CosyVoice
    loguru  # For Fish Speech
    natsort  # For Fish Speech
    loralib  # For Fish Speech
    ormsgpack  # For Fish Speech
    cachetools  # For Fish Speech
    silero-vad  # For Fish Speech
    vector-quantize-pytorch<=1.17.3,>=1.14.24 # For Fish Speech
    torchdiffeq  # For F5-TTS
    x_transformers>=1.31.14  # For F5-TTS
    pypinyin  # For F5-TTS
    tomli  # For F5-TTS
    vocos  # For F5-TTS
    librosa  # For F5-TTS
    jieba  # For F5-TTS
    soundfile  # For F5-TTS & MeloTTS & Kokoro
    cached_path  # For MeloTTS
    unidic-lite  # For MeloTTS, unidic requires manually download
    cn2an  # For MeloTTS
    mecab-python3  # For MeloTTS
    num2words  # For MeloTTS
    pykakasi  # For MeloTTS
    fugashi  # For MeloTTS
    g2p_en  # For MeloTTS
    anyascii  # For MeloTTS
    gruut[de,es,fr]  # For MeloTTS
    kokoro>=0.7.15  # Kokoro, already included misaki[en]
    misaki[en,zh]>=0.7.15  # Kokoro
    langdetect  # MegaTTS3
    pyloudnorm  # MegaTTS3
    json5 # IndexTTS2
    munch # IndexTTS2
    matplotlib # IndexTTS2
    flatten_dict # IndexTTS2
    julius # IndexTTS2
    tensorboard # IndexTTS2
    randomname # IndexTTS2
    argbind # IndexTTS2
doc =
    ipython>=6.5.0
    sphinx>=3.0.0
    pydata-sphinx-theme>=0.3.0
    sphinx-intl>=0.9.9
    sphinx-tabs
    sphinx-design
    prometheus_client
    timm
musa =
    mthreads-ml-py>=2.2.8
    torchada>=0.1.11
benchmark =
    psutil
virtualenv =
    uv
anthropic =
    anthropic

[options.entry_points]
console_scripts =
    xinference = xinference.deploy.cmdline:cli
    xinference-local = xinference.deploy.cmdline:local
    xinference-supervisor = xinference.deploy.cmdline:supervisor
    xinference-worker = xinference.deploy.cmdline:worker

[coverage:run]
branch = True
relative_files = True
cover_pylib = False
plugins = Cython.Coverage
include =
    xinference/*
omit =
    xinference/_version.py
    *.pxd
    */tests/*

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    return NotImplemented

[versioneer]
VCS = git
style = pep440
versionfile_source = xinference/_version.py
versionfile_build = xinference/_version.py
tag_prefix = v
parentdir_prefix = xinference-

[flake8]
max-line-length = 100
select =
    E9,
    E101,
    E111,
    E117,
    E127,
    E201,
    E202,
    E223,
    E224,
    E225,
    E231,
    E242,
    E251,
    E273,
    E274,
    E275,
    E301,
    E302,
    E303,
    E304,
    E305,
    E401,
    E703,
    E901,
    E999,
    F7,
    F63,
    F82,
    F401,
    F811,
    F821,
    F822,
    F823,
    F841,
    W191,
    W291,
    W292,
    W293,
    W391,
    W601,
    W602,
    W603,
    W604,
    W605
exclude =
    __init__.py
    __pycache__
    .git/
    .github/
    build/
    ci/
    dist/
    docs/
    thirdparty

[codespell]
ignore-words-list = hist,rcall,fpr,ser,nd,inout,ot,Ba,ba,asend,hart,coo,splitted,datas,fro
skip = .idea,.git,./build,./docs/build,node_modules,static,generated,*.po,*.ts,*.json,*.c,*.cpp,*.cfg,thirdparty

[isort]
profile = black
skip = thirdparty

[mypy]
ignore_missing_imports=True
follow_imports=skip
exclude = thirdparty
