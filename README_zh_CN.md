<div align="center">
<img src="./assets/xorbits-logo.png" width="180px" alt="xorbits" />

# Xorbits Inferenceï¼šæ¨¡å‹æ¨ç†ï¼Œ è½»è€Œæ˜“ä¸¾ ğŸ¤–

[![PyPI Latest Release](https://img.shields.io/pypi/v/xinference.svg?style=for-the-badge)](https://pypi.org/project/xinference/)
[![License](https://img.shields.io/pypi/l/xinference.svg?style=for-the-badge)](https://github.com/xorbitsai/inference/blob/main/LICENSE)
[![Build Status](https://img.shields.io/github/actions/workflow/status/xorbitsai/inference/python.yaml?branch=main&style=for-the-badge&label=GITHUB%20ACTIONS&logo=github)](https://actions-badge.atrox.dev/xorbitsai/inference/goto?ref=main)
[![Slack](https://img.shields.io/badge/join_Slack-781FF5.svg?logo=slack&style=for-the-badge)](https://join.slack.com/t/xorbitsio/shared_invite/zt-1o3z9ucdh-RbfhbPVpx7prOVdM1CAuxg)
[![Twitter](https://img.shields.io/twitter/follow/xorbitsio?logo=twitter&style=for-the-badge)](https://twitter.com/xorbitsio)

[English](README.md) | ä¸­æ–‡ä»‹ç» | [æ—¥æœ¬èª](README_ja_JP.md)
</div>
<br />


Xorbits Inferenceï¼ˆXinferenceï¼‰æ˜¯ä¸€ä¸ªæ€§èƒ½å¼ºå¤§ä¸”åŠŸèƒ½å…¨é¢çš„åˆ†å¸ƒå¼æ¨ç†æ¡†æ¶ã€‚å¯ç”¨äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œå¤šæ¨¡æ€æ¨¡å‹ç­‰å„ç§æ¨¡å‹çš„æ¨ç†ã€‚é€šè¿‡ Xorbits Inferenceï¼Œä½ å¯ä»¥è½»æ¾åœ°ä¸€é”®éƒ¨ç½²ä½ è‡ªå·±çš„æ¨¡å‹æˆ–å†…ç½®çš„å‰æ²¿å¼€æºæ¨¡å‹ã€‚æ— è®ºä½ æ˜¯ç ”ç©¶è€…ï¼Œå¼€å‘è€…ï¼Œæˆ–æ˜¯æ•°æ®ç§‘å­¦å®¶ï¼Œéƒ½å¯ä»¥é€šè¿‡ Xorbits Inference ä¸æœ€å‰æ²¿çš„ AI æ¨¡å‹ï¼Œå‘æ˜æ›´å¤šå¯èƒ½ã€‚


<div align="center">
<i><a href="https://join.slack.com/t/xorbitsio/shared_invite/zt-1z3zsm9ep-87yI9YZ_B79HLB2ccTq4WA">ğŸ‘‰ ç«‹åˆ»åŠ å…¥æˆ‘ä»¬çš„ Slack ç¤¾åŒº!</a></i>
</div>

## ğŸ”¥ è¿‘æœŸçƒ­ç‚¹
### æ¡†æ¶å¢å¼º
- æ”¯æŒæŒ‡å®š grammar è¾“å‡º: [#525](https://github.com/xorbitsai/inference/pull/525)
- æŠ•æœºé‡‡æ ·: [#509](https://github.com/xorbitsai/inference/pull/509)
- å¼•å…¥ vLLM: [#445](https://github.com/xorbitsai/inference/pull/445)
### æ–°æ¨¡å‹
- å†…ç½® [zephyr-7b-alpha](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha) ä¸ [zephyr-7b-beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta): [#597](https://github.com/xorbitsai/inference/pull/597)
- å†…ç½® [chatglm3](https://huggingface.co/THUDM/chatglm3-6b): [#587](https://github.com/xorbitsai/inference/pull/587)
- å†…ç½® [mistral-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) ä¸ [mistral-instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1): [#510](https://github.com/xorbitsai/inference/pull/510)
### é›†æˆ
- [Dify](https://docs.dify.ai/advanced/model-configuration/xinference): ä¸€ä¸ªæ¶µç›–äº†å¤§å‹è¯­è¨€æ¨¡å‹å¼€å‘ã€éƒ¨ç½²ã€ç»´æŠ¤å’Œä¼˜åŒ–çš„ LLMOps å¹³å°ã€‚
- [Chatbox](https://chatboxai.app/): ä¸€ä¸ªæ”¯æŒå‰æ²¿å¤§è¯­è¨€æ¨¡å‹çš„æ¡Œé¢å®¢æˆ·ç«¯ï¼Œæ”¯æŒ Windowsï¼ŒMacï¼Œä»¥åŠ Linuxã€‚

## ä¸»è¦åŠŸèƒ½
ğŸŒŸ **æ¨¡å‹æ¨ç†ï¼Œè½»è€Œæ˜“ä¸¾**ï¼šå¤§è¯­è¨€æ¨¡å‹ï¼Œè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œå¤šæ¨¡æ€æ¨¡å‹çš„éƒ¨ç½²æµç¨‹è¢«å¤§å¤§ç®€åŒ–ã€‚ä¸€ä¸ªå‘½ä»¤å³å¯å®Œæˆæ¨¡å‹çš„éƒ¨ç½²å·¥ä½œã€‚ 

âš¡ï¸ **å‰æ²¿æ¨¡å‹ï¼Œåº”æœ‰å°½æœ‰**ï¼šæ¡†æ¶å†…ç½®ä¼—å¤šä¸­è‹±æ–‡çš„å‰æ²¿å¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ baichuanï¼Œchatglm2 ç­‰ï¼Œä¸€é”®å³å¯ä½“éªŒï¼å†…ç½®æ¨¡å‹åˆ—è¡¨è¿˜åœ¨å¿«é€Ÿæ›´æ–°ä¸­ï¼

ğŸ–¥ **å¼‚æ„ç¡¬ä»¶ï¼Œå¿«å¦‚é—ªç”µ**ï¼šé€šè¿‡ [ggml](https://github.com/ggerganov/ggml)ï¼ŒåŒæ—¶ä½¿ç”¨ä½ çš„ GPU ä¸ CPU è¿›è¡Œæ¨ç†ï¼Œé™ä½å»¶è¿Ÿï¼Œæé«˜ååï¼

âš™ï¸ **æ¥å£è°ƒç”¨ï¼Œçµæ´»å¤šæ ·**ï¼šæä¾›å¤šç§ä½¿ç”¨æ¨¡å‹çš„æ¥å£ï¼ŒåŒ…æ‹¬ RPCï¼ŒRESTful APIï¼Œå‘½ä»¤è¡Œï¼Œweb UI ç­‰ç­‰ã€‚æ–¹ä¾¿æ¨¡å‹çš„ç®¡ç†ä¸ç›‘æ§ã€‚

ğŸŒ **é›†ç¾¤è®¡ç®—ï¼Œåˆ†å¸ƒååŒ**: æ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²ï¼Œé€šè¿‡å†…ç½®çš„èµ„æºè°ƒåº¦å™¨ï¼Œè®©ä¸åŒå¤§å°çš„æ¨¡å‹æŒ‰éœ€è°ƒåº¦åˆ°ä¸åŒæœºå™¨ï¼Œå……åˆ†ä½¿ç”¨é›†ç¾¤èµ„æºã€‚

ğŸ”Œ **å¼€æ”¾ç”Ÿæ€ï¼Œæ— ç¼å¯¹æ¥**: ä¸æµè¡Œçš„ä¸‰æ–¹åº“æ— ç¼å¯¹æ¥ï¼ŒåŒ…æ‹¬ [LangChain](https://python.langchain.com/docs/integrations/providers/xinference)ï¼Œ[LlamaIndex](https://gpt-index.readthedocs.io/en/stable/examples/llm/XinferenceLocalDeployment.html#i-run-pip-install-xinference-all-in-a-terminal-window)ï¼Œ[Dify](https://docs.dify.ai/advanced/model-configuration/xinference)ï¼Œä»¥åŠ [Chatbox](https://chatboxai.app/)ã€‚

## å¿«é€Ÿå…¥é—¨
### å®‰è£…
Xinference å¯ä»¥é€šè¿‡ `pip` ä» PyPI å®‰è£…ã€‚æˆ‘ä»¬éå¸¸æ¨èåœ¨å®‰è£…å‰åˆ›å»ºä¸€ä¸ªæ–°çš„è™šæ‹Ÿç¯å¢ƒä»¥é¿å…ä¾èµ–å†²çªã€‚

ä½¿ç”¨ Xinference å‰ï¼Œæ‚¨éœ€è¦å®‰è£…ä¸æ¨¡å‹ç±»å‹ç›¸å¯¹åº”çš„åç«¯ã€‚å¦‚æœæƒ³è¦æ¨ç†æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹ï¼Œå¯ä»¥å®‰è£…æ‰€æœ‰åç«¯ï¼š
```bash
pip install "xinference[all]"
```

**æ³¨æ„**ï¼šæ¨ç† GGML æ ¼å¼çš„æ¨¡å‹å‰ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®**æ‰‹åŠ¨å®‰è£… GGML ä¾èµ–**ï¼Œä»¥åœ¨ä¸åŒç¡¬ä»¶ä¸Šè¾¾åˆ°åŠ é€Ÿæ•ˆæœï¼Œè¯·å‚è€ƒ [GGML åç«¯](#ggml-backend)ã€‚

#### Transformers åç«¯
Transformers åç«¯æ”¯æŒç»å¤§å¤šæ•°å‰æ²¿æ¨¡å‹ï¼Œå®ƒæ˜¯ PyTorch æ ¼å¼æ¨¡å‹çš„é»˜è®¤åç«¯ï¼š
```bash
pip install "xinference[transformers]"
```

#### vLLM åç«¯
vLLM åç«¯èƒ½å¤Ÿæä¾›é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›ã€‚å½“æ»¡è¶³ä»¥ä¸‹æ¡ä»¶æ—¶ï¼ŒXinference ä¼šé€‰æ‹© vLLM ä½œä¸ºåç«¯ä»¥è¾¾åˆ°æ›´å¥½çš„ååé‡ï¼š

- æ¨¡å‹æ ¼å¼ä¸º PyTorch
- æ¨¡å‹åœ¨ä¸‹é¢çš„æ”¯æŒåˆ—è¡¨ä¸­
- é‡åŒ–é€‰æ‹© `none`ï¼ˆAWQ é‡åŒ–å°†ä¼šåœ¨è¿‘æœŸæ”¯æŒï¼‰
- Linux ç³»ç»Ÿï¼Œå¹¶æœ‰ CUDA è®¾å¤‡

ç›®å‰, æ”¯æŒçš„æ¨¡å‹åŒ…æ‹¬ï¼š

- ``llama-2``, ``llama-2-chat``
- ``baichuan``, ``baichuan-chat``
- ``internlm``, ``internlm-20b``, ``internlm-chat``, ``internlm-chat-20b``
- ``vicuna-v1.3``, ``vicuna-v1.5``

```bash
pip install "xinference[vllm]"
```

#### GGML åç«¯
æˆ‘ä»¬å¼ºçƒˆå»ºè®®**æ‰‹åŠ¨å®‰è£… GGML ä¾èµ–**ï¼Œä»¥åœ¨ä¸åŒç¡¬ä»¶ä¸Šè¾¾åˆ°åŠ é€Ÿæ•ˆæœã€‚

åˆå§‹å®‰è£…ï¼š
```bash
pip install xinference
pip install ctransformers
```

æ ¹æ®ç¡¬ä»¶ï¼Œé€‰æ‹©æ€§å®‰è£…ï¼š
- è‹¹æœèŠ¯ç‰‡ï¼ˆM1ï¼ŒM2ï¼‰:
```bash
    CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python
```

- è‹±ä¼Ÿè¾¾ GPU:
```bash
    CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python
```

- AMD GPU:
```bash
    CMAKE_ARGS="-DLLAMA_HIPBLAS=on" pip install llama-cpp-python
```

### éƒ¨ç½²
ä½ å¯ä»¥ä¸€é”®è¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼Œæˆ–æŒ‰ç…§ä¸‹é¢çš„æ­¥éª¤å°† Xinference éƒ¨ç½²åœ¨è®¡ç®—é›†ç¾¤ã€‚ 

#### æœ¬åœ°éƒ¨ç½²
è¿è¡Œä¸‹é¢çš„å‘½ä»¤åœ¨æœ¬åœ°éƒ¨ç½² Xinferenceï¼š
```bash
$ xinference-local
```

#### åˆ†å¸ƒå¼éƒ¨ç½²
åˆ†å¸ƒå¼åœºæ™¯ä¸‹ï¼Œä½ éœ€è¦åœ¨ä¸€å°æœåŠ¡å™¨ä¸Šéƒ¨ç½²ä¸€ä¸ª Xinference supervisorï¼Œå¹¶åœ¨å…¶ä½™æœåŠ¡å™¨ä¸Šåˆ†åˆ«éƒ¨ç½²ä¸€ä¸ª Xinference workerã€‚ å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š

**å¯åŠ¨ supervisor**: æ‰§è¡Œ:
```bash
$ xinference-supervisor -H "${supervisor_host}"
```
æ›¿æ¢ `${supervisor_host}` ä¸º supervisor æ‰€åœ¨æœåŠ¡å™¨çš„å®é™…ä¸»æœºåæˆ– IP åœ°å€ã€‚

**å¯åŠ¨ workers**: åœ¨å…¶ä½™æœåŠ¡å™¨ä¸Šï¼Œæ‰§è¡Œï¼š
```bash
$ xinference-worker -e "http://${supervisor_host}:9997"
```

Xinference å¯åŠ¨åï¼Œå°†ä¼šæ‰“å°æœåŠ¡çš„ endpointã€‚è¿™ä¸ª endpoint ç”¨äºé€šè¿‡å‘½ä»¤è¡Œå·¥å…·æˆ–ç¼–ç¨‹æ¥å£è¿›è¡Œæ¨¡å‹çš„ç®¡ç†ã€‚

- æœ¬åœ°éƒ¨ç½²ä¸‹, endpoint é»˜è®¤ä¸º `http://localhost:9997`.
- é›†ç¾¤éƒ¨ç½²ä¸‹, endpoint é»˜è®¤ä¸º `http://${supervisor_host}:9997`ã€‚å…¶ä¸­ `${supervisor_host}` ä¸ºsupervisor æ‰€åœ¨æœåŠ¡å™¨çš„ä¸»æœºåæˆ– IP åœ°å€ã€‚

ä½ è¿˜å¯ä»¥é€šè¿‡ web UI å¯åŠ¨å¹¶ç®¡ç†æ¨¡å‹ï¼Œä½¿ç”¨ä»»æ„å†…ç½®æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æˆ–å¯¹è¯ã€‚

![web UI](assets/index.jpg)

### Xinference å‘½ä»¤è¡Œ
Xinference æä¾›äº†å‘½ä»¤è¡Œå·¥å…·ç”¨äºæ¨¡å‹ç®¡ç†ã€‚æ”¯æŒçš„å‘½ä»¤åŒ…æ‹¬ï¼š

- å¯åŠ¨ä¸€ä¸ªæ¨¡å‹ (å°†ä¼šè¿”å›ä¸€ä¸ªæ¨¡å‹ UID)ï¼š`xinference launch`
- æŸ¥çœ‹æ‰€æœ‰è¿è¡Œä¸­çš„æ¨¡å‹ï¼š`xinference list`
- æŸ¥çœ‹æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹ï¼š`xinference registrations`
- ç»“æŸæ¨¡å‹ï¼š`xinference terminate --model-uid ${model_uid}`

### Xinference ç¼–ç¨‹æ¥å£
Xinference åŒæ ·æä¾›äº†ç¼–ç¨‹æ¥å£ï¼š

```python
from xinference.client import Client

client = Client("http://localhost:9997")
model_uid = client.launch_model(model_name="chatglm2")
model = client.get_model(model_uid)

chat_history = []
prompt = "What is the largest animal?"
model.chat(
    prompt,
    chat_history,
    generate_config={"max_tokens": 1024}
)
```

è¿”å›å€¼ï¼š
```json
{
  "id": "chatcmpl-8d76b65a-bad0-42ef-912d-4a0533d90d61",
  "model": "56f69622-1e73-11ee-a3bd-9af9f16816c6",
  "object": "chat.completion",
  "created": 1688919187,
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "The largest animal that has been scientifically measured is the blue whale, which has a maximum length of around 23 meters (75 feet) for adult animals and can weigh up to 150,000 pounds (68,000 kg). However, it is important to note that this is just an estimate and that the largest animal known to science may be larger still. Some scientists believe that the largest animals may not have a clear \"size\" in the same way that humans do, as their size can vary depending on the environment and the stage of their life."
      },
      "finish_reason": "None"
    }
  ],
  "usage": {
    "prompt_tokens": -1,
    "completion_tokens": -1,
    "total_tokens": -1
  }
}
```

è¯·å‚è€ƒ [æ›´å¤šæ¡ˆä¾‹](examples)ã€‚


## å†…ç½®æ¨¡å‹
è¿è¡Œä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹å†…ç½®æ¨¡å‹åˆ—è¡¨ï¼š
```bash
$ xinference registrations
```

| Type | Name                | Language     | Ability                |
|------|---------------------|--------------|------------------------|
| LLM  | baichuan            | ['en', 'zh'] | ['embed', 'generate']  |
| LLM  | baichuan-2          | ['en', 'zh'] | ['embed', 'generate']  |
| LLM  | baichuan-chat       | ['en', 'zh'] | ['embed', 'chat']      |
| LLM  | baichuan-2-chat     | ['en', 'zh'] | ['embed', 'chat']      |
| LLM  | chatglm             | ['en', 'zh'] | ['embed', 'chat']      |
| LLM  | chatglm2            | ['en', 'zh'] | ['embed', 'chat']      |
| LLM  | chatglm2-32k        | ['en', 'zh'] | ['embed', 'chat']      |
| LLM  | code-llama          | ['en']       | ['generate']           |
| LLM  | code-llama-instruct | ['en']       | ['chat']               |
| LLM  | code-llama-python   | ['en']       | ['generate']           |
| LLM  | falcon              | ['en']       | ['embed', 'generate']  |
| LLM  | falcon-instruct     | ['en']       | ['embed', 'chat']      |
| LLM  | glaive-coder        | ['en']       | ['chat']              |
| LLM  | gpt-2               | ['en']       | ['generate']           |
| LLM  | internlm-7b         | ['en', 'zh'] | ['embed', 'generate']  |
| LLM  | internlm-chat-7b    | ['en', 'zh'] | ['embed', 'chat']      |
| LLM  | internlm-chat-20b   | ['en', 'zh'] | ['embed', 'chat']      |
| LLM  | llama-2             | ['en']       | ['embed', 'generate']  |
| LLM  | llama-2-chat        | ['en']       | ['embed', 'chat']      |
| LLM  | opt                 | ['en']       | ['embed', 'generate']  |
| LLM  | orca                | ['en']       | ['embed', 'chat']      |
| LLM  | qwen-chat           | ['en', 'zh'] | ['embed', 'chat']      |
| LLM  | starchat-beta       | ['en']       | ['embed', 'chat']      |
| LLM  | starcoder           | ['en']       | ['generate']           |
| LLM  | starcoderplus       | ['en']       | ['embed', 'generate']  |
| LLM  | vicuna-v1.3         | ['en']       | ['embed', 'chat']      |
| LLM  | vicuna-v1.5         | ['en']       | ['embed', 'chat']      |
| LLM  | vicuna-v1.5-16k     | ['en']       | ['embed', 'chat']      |
| LLM  | wizardlm-v1.0       | ['en']       | ['embed', 'chat']      |
| LLM  | wizardmath-v1.0     | ['en']       | ['embed', 'chat']      |
| LLM  | OpenBuddy           | ['en', 'zh'] | ['embed', 'chat']      |

æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ [å†…ç½®æ¨¡å‹](https://inference.readthedocs.io/en/latest/models/builtin/index.html)ã€‚

**æ³¨æ„**:
- Xinference ä¼šè‡ªåŠ¨ä¸ºä½ ä¸‹è½½æ¨¡å‹ï¼Œé»˜è®¤çš„æ¨¡å‹å­˜æ”¾è·¯å¾„ä¸º `${USER}/.xinference/cache`ã€‚
- å¦‚æœæ‚¨åœ¨Hugging Faceä¸‹è½½æ¨¡å‹æ—¶é‡åˆ°é—®é¢˜ï¼Œè¯·è¿è¡Œ `export XINFERENCE_MODEL_SRC=modelscope`ï¼Œé»˜è®¤ä¼˜å…ˆä» modelscope ä¸‹è½½ã€‚ç›®å‰ modelscope æ”¯æŒçš„æ¨¡å‹æœ‰ï¼š
  - llama-2
  - llama-2-chat
  - baichuan-2
  - baichuan-2-chat
  - chatglm2
  - chatglm2-32k
  - internlm-chat-20b

## è‡ªå®šä¹‰æ¨¡å‹
è¯·å‚è€ƒ [è‡ªå®šä¹‰æ¨¡å‹](https://inference.readthedocs.io/en/latest/models/custom.html)ã€‚
