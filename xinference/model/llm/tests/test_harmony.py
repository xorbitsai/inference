# Copyright 2022-2026 XProbe Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import re

from ..harmony import HarmonyStreamParser, async_stream_harmony_chat_completion


def test_streaming_parser_multiple_texts():
    parser = HarmonyStreamParser()

    texts = [
        'analysisThe user says "ä½ å¥½" (Hello). We need to respond in Chinese presumably. '
        "Should greet back and ask how can help.\n"
        "We should keep it short but friendly.assistantfinalä½ å¥½ï¼å¾ˆé«˜å…´å’Œä½ èŠèŠå¤©ã€‚"
        "è¯·é—®æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„ï¼Ÿç¥ä½ ä»Šå¤©æ„‰å¿«ï¼",
        'analysisThe user wrote "ä½ å¥½" (Hello in Chinese). We need to respond appropriately, '
        "presumably in Chinese. The system instructions say we should produce natural, friendly tone, "
        "follow style guidelines.\n\n"
        "We can greet back and ask how we can help. "  # noqa: codespell
        "Possibly ask if they want assistance with something specific. Use Chinese. "
        "Also note use of markdown optionally.\n\n"
        "Will respond with a friendly greeting and ask what they need."
        "assistantfinalä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼ŸğŸ˜Šå¦‚æœæœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦è®¨è®ºçš„è¯é¢˜ï¼Œå°½ç®¡å‘Šè¯‰æˆ‘ã€‚",
    ]

    all_segments = []
    for text in texts:
        for seg in parser.feed(text):
            all_segments.append(seg)

    # We expect 4 segments total: 2 per text
    assert len(all_segments) == 4

    # Check channels
    assert all_segments[0]["channel"] == "analysis"
    assert all_segments[1]["channel"] == "final"
    assert all_segments[2]["channel"] == "analysis"
    assert all_segments[3]["channel"] == "final"

    # Optionally, check some content keywords
    assert "The user says" in all_segments[0]["content"]
    assert "ä½ å¥½ï¼å¾ˆé«˜å…´" in all_segments[1]["content"]
    assert "The user wrote" in all_segments[2]["content"]
    assert "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™" in all_segments[3]["content"]


async def test_harmony_streaming_and_nonstreaming():
    texts = [
        'analysisThe user says "ä½ å¥½" (Hello). We need to respond in Chinese presumably. '
        "Should greet back and ask how can help.\n"
        "We should keep it short but friendly.assistantfinalä½ å¥½ï¼å¾ˆé«˜å…´å’Œä½ èŠèŠå¤©ã€‚"
        "è¯·é—®æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„ï¼Ÿç¥ä½ ä»Šå¤©æ„‰å¿«ï¼",
        'analysisThe user wrote "ä½ å¥½" (Hello in Chinese). We need to respond appropriately, '
        "presumably in Chinese. The system instructions say we should produce natural, friendly tone, "
        "follow style guidelines.\n\n"
        "We can greet back and ask how we can help. "
        "Possibly ask if they want assistance with something specific. Use Chinese. "
        "Also note use of markdown optionally.\n\nWill respond with a friendly greeting and ask what they need."
        "assistantfinalä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼ŸğŸ˜Šå¦‚æœæœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦è®¨è®ºçš„è¯é¢˜ï¼Œå°½ç®¡å‘Šè¯‰æˆ‘ã€‚",
    ]

    ### --- Non-streaming test ---
    full_chat = {
        "id": "full_test",
        "object": "chat.completion",
        "created": 0,
        "model": "test-model",
        "choices": [
            {"index": 0, "message": {"content": t, "reasoning_content": ""}}
            for t in texts
        ],
        "usage": {},
    }

    parsed_full = []
    async for chunk in async_stream_harmony_chat_completion(full_chat):
        parsed_full.append(chunk)

    assert len(parsed_full) == 1
    for i, choice in enumerate(parsed_full[0]["choices"]):
        msg = choice["message"]
        content = msg.get("content", "")
        reasoning = msg.get("reasoning_content", "")
        assert "ä½ å¥½" in content, "Final content missing"
        assert "The user" in reasoning, "Reasoning content missing"
        assert not content.startswith(
            "analysis"
        ), "Final content incorrectly contains analysis prefix"

    ### --- Streaming test (realistic incremental chunks) ---
    import asyncio

    async def async_chunk_generator():
        chunk_size = 15  # simulate token-level streaming

        for t in texts:
            # Split by key prefixes, keep prefixes with content
            segments = re.split(r"(analysis|assistantfinal)", t)
            segs_with_prefix = []

            i = 0
            while i < len(segments):
                if segments[i] in ("analysis", "assistantfinal"):
                    segs_with_prefix.append(segments[i] + segments[i + 1])
                    i += 2
                else:
                    if segments[i]:
                        segs_with_prefix.append(segments[i])
                    i += 1

            # Now cut each segment into smaller chunks
            for seg in segs_with_prefix:
                for j in range(0, len(seg), chunk_size):
                    piece = seg[j : j + chunk_size]
                    yield {
                        "id": "stream_test",
                        "model": "test-model",
                        "object": "chat.completion.chunk",
                        "created": 0,
                        "choices": [
                            {
                                "index": 0,
                                "delta": {"content": piece},
                                "finish_reason": None,
                            }
                        ],
                    }
                    await asyncio.sleep(
                        0
                    )  # Small delay to simulate real async behavior

    incremental_results = []
    async for chunk in async_stream_harmony_chat_completion(async_chunk_generator()):
        incremental_results.append(chunk)

    # Streaming test: accumulate manually
    accum_content = ""
    accum_reasoning = ""

    for chunk in incremental_results:
        delta = chunk["choices"][0]["delta"]
        accum_content += delta.get("content", "") or ""
        reasoning_content = delta.get("reasoning_content", "")
        if reasoning_content is not None:
            accum_reasoning += reasoning_content

    # Verify accumulated result
    assert "ä½ å¥½" in accum_content, "Streaming final content missing"
    assert "The user" in accum_reasoning, "Streaming reasoning content missing"
    assert not accum_content.startswith(
        "analysis"
    ), "Streaming content incorrectly contains analysis prefix"


stream_chunks = [
    {
        "id": "chat1",
        "model": "gpt-oss",
        "created": 1755090509,
        "object": "chat.completion.chunk",
        "choices": [
            {
                "index": 0,
                "delta": {"role": "assistant", "content": ""},
                "finish_reason": None,
            }
        ],
    },
    {
        "id": "chat2",
        "model": "gpt-oss",
        "created": 1755090509,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": "analysis"}, "finish_reason": None}
        ],
    },
    {
        "id": "chat3",
        "model": "gpt-oss",
        "created": 1755090509,
        "object": "chat.completion.chunk",
        "choices": [{"index": 0, "delta": {"content": "The"}, "finish_reason": None}],
    },
    {
        "id": "chat4",
        "model": "gpt-oss",
        "created": 1755090509,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": " user says"}, "finish_reason": None}
        ],
    },
    {
        "id": "chat5",
        "model": "gpt-oss",
        "created": 1755090510,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": ' "ä½ å¥½'}, "finish_reason": None}
        ],
    },
    {
        "id": "chat6",
        "model": "gpt-oss",
        "created": 1755090510,
        "object": "chat.completion.chunk",
        "choices": [{"index": 0, "delta": {"content": '" ('}, "finish_reason": None}],
    },
    {
        "id": "chat7",
        "model": "gpt-oss",
        "created": 1755090510,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": "Hello)."}, "finish_reason": None}
        ],
    },
    {
        "id": "chat8",
        "model": "gpt-oss",
        "created": 1755090510,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": " We should"}, "finish_reason": None}
        ],
    },
    {
        "id": "chat9",
        "model": "gpt-oss",
        "created": 1755090510,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": " respond in"}, "finish_reason": None}
        ],
    },
    {
        "id": "chat10",
        "model": "gpt-oss",
        "created": 1755090510,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": " Chinese,"}, "finish_reason": None}
        ],
    },
    {
        "id": "chat11",
        "model": "gpt-oss",
        "created": 1755090510,
        "object": "chat.completion.chunk",
        "choices": [
            {
                "index": 0,
                "delta": {"content": " friendly greeting"},
                "finish_reason": None,
            }
        ],
    },
    {
        "id": "chat12",
        "model": "gpt-oss",
        "created": 1755090511,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": ".\n\n We"}, "finish_reason": None}
        ],
    },
    {
        "id": "chat13",
        "model": "gpt-oss",
        "created": 1755090511,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": " can say"}, "finish_reason": None}
        ],
    },
    {
        "id": "chat14",
        "model": "gpt-oss",
        "created": 1755090511,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": " something like"}, "finish_reason": None}
        ],
    },
    {
        "id": "chat15",
        "model": "gpt-oss",
        "created": 1755090511,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": ' "ä½ å¥½'}, "finish_reason": None}
        ],
    },
    {
        "id": "chat16",
        "model": "gpt-oss",
        "created": 1755090511,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": "ï¼æœ‰ä»€ä¹ˆ"}, "finish_reason": None}
        ],
    },
    {
        "id": "chat17",
        "model": "gpt-oss",
        "created": 1755090511,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": "å¯ä»¥å¸®"}, "finish_reason": None}
        ],
    },
    {
        "id": "chat18",
        "model": "gpt-oss",
        "created": 1755090511,
        "object": "chat.completion.chunk",
        "choices": [{"index": 0, "delta": {"content": 'æ‚¨?"'}, "finish_reason": None}],
    },
    {
        "id": "chat19",
        "model": "gpt-oss",
        "created": 1755090511,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": " Keep brief"}, "finish_reason": None}
        ],
    },
    {
        "id": "chat20",
        "model": "gpt-oss",
        "created": 1755090512,
        "object": "chat.completion.chunk",
        "choices": [{"index": 0, "delta": {"content": "."}, "finish_reason": None}],
    },
    {
        "id": "chat21",
        "model": "gpt-oss",
        "created": 1755090512,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": "assistant"}, "finish_reason": None}
        ],
    },
    {
        "id": "chat22",
        "model": "gpt-oss",
        "created": 1755090512,
        "object": "chat.completion.chunk",
        "choices": [{"index": 0, "delta": {"content": "final"}, "finish_reason": None}],
    },
    {
        "id": "chat23",
        "model": "gpt-oss",
        "created": 1755090512,
        "object": "chat.completion.chunk",
        "choices": [{"index": 0, "delta": {"content": "ä½ å¥½"}, "finish_reason": None}],
    },
    {
        "id": "chat24",
        "model": "gpt-oss",
        "created": 1755090512,
        "object": "chat.completion.chunk",
        "choices": [{"index": 0, "delta": {"content": "ï¼å¾ˆ"}, "finish_reason": None}],
    },
    {
        "id": "chat25",
        "model": "gpt-oss",
        "created": 1755090512,
        "object": "chat.completion.chunk",
        "choices": [{"index": 0, "delta": {"content": "é«˜å…´"}, "finish_reason": None}],
    },
    {
        "id": "chat26",
        "model": "gpt-oss",
        "created": 1755090512,
        "object": "chat.completion.chunk",
        "choices": [{"index": 0, "delta": {"content": "è§åˆ°"}, "finish_reason": None}],
    },
    {
        "id": "chat27",
        "model": "gpt-oss",
        "created": 1755090513,
        "object": "chat.completion.chunk",
        "choices": [{"index": 0, "delta": {"content": "ä½ ã€‚"}, "finish_reason": None}],
    },
    {
        "id": "chat28",
        "model": "gpt-oss",
        "created": 1755090513,
        "object": "chat.completion.chunk",
        "choices": [{"index": 0, "delta": {"content": "è¯·é—®"}, "finish_reason": None}],
    },
    {
        "id": "chat29",
        "model": "gpt-oss",
        "created": 1755090513,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": "æœ‰ä»€ä¹ˆæˆ‘"}, "finish_reason": None}
        ],
    },
    {
        "id": "chat30",
        "model": "gpt-oss",
        "created": 1755090513,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": "å¯ä»¥å¸®åŠ©"}, "finish_reason": None}
        ],
    },
    {
        "id": "chat31",
        "model": "gpt-oss",
        "created": 1755090513,
        "object": "chat.completion.chunk",
        "choices": [
            {"index": 0, "delta": {"content": "ä½ çš„ï¼Ÿ"}, "finish_reason": None}
        ],
    },
    {
        "id": "chat32",
        "model": "gpt-oss",
        "created": 1755090513,
        "object": "chat.completion.chunk",
        "choices": [{"index": 0, "delta": {"content": ""}, "finish_reason": "stop"}],
    },
]


async def test_async_stream_chunks():
    """Test with async generator instead of sync list"""
    import asyncio

    async def async_chunk_generator():
        for chunk in stream_chunks:
            yield chunk
            await asyncio.sleep(0)  # Small delay to simulate real async behavior

    content_accum = ""
    reasoning_accum = ""
    all_chunks = []

    # Test that reasoning_content is None when content has value but reasoning_content is empty
    chunk_with_content_count = 0
    async for chunk in async_stream_harmony_chat_completion(async_chunk_generator()):
        all_chunks.append(chunk)
        delta = chunk["choices"][0].get("delta", {})
        if delta.get("content"):
            chunk_with_content_count += 1
            # å½“ content ä¸ä¸ºç©ºæ—¶ï¼Œreasoning_content åº”è¯¥ä¸º None
            assert (
                delta.get("reasoning_content") is None
            ), f"Chunk {chunk_with_content_count}: reasoning_content should be None when content has value"

        if delta.get("reasoning_content"):
            reasoning_accum += delta.get("reasoning_content", "")
        else:
            content_accum += delta.get("content", "")

    # Check that the last chunk has finish_reason: "stop"
    last_chunk = all_chunks[-1]
    last_choice = last_chunk["choices"][0]
    assert (
        last_choice.get("finish_reason") == "stop"
    ), f"Last chunk should have finish_reason 'stop', got {last_choice.get('finish_reason')}"

    expected_content = "ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚è¯·é—®æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„ï¼Ÿ"
    expected_reasoning = (
        'The user says "ä½ å¥½" (Hello). We should respond in Chinese, friendly greeting.\n\n'
        ' We can say something like "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨?" Keep brief.'
    )

    assert content_accum == expected_content
    assert reasoning_accum == expected_reasoning
    assert chunk_with_content_count > 0, "Should have chunks with content"
