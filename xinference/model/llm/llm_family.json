[
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "codestral-v0.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Codestrall-22B-v0.1 is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mistralai/Codestral-22B-v0.1",
            "model_revision": "8f5fe23af91885222a1563283c87416745a5e212"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_K_S",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "bartowski/Codestral-22B-v0.1-GGUF",
            "model_file_name_template": "Codestral-22B-v0.1-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Codestral-22B-v0.1-4bit",
            "model_revision": "544626b38eb1c9524f0fa570ec7b29550c26b78d"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Codestral-22B-v0.1-8bit",
            "model_revision": "0399a53970663950d57010e61a2796af524a1588"
          }
        }
      }
    ],
    "updated_at": 1769418470,
    "featured": false,
    "architectures": [
      "MistralForCausalLM"
    ],
    "model_type": "mistral",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "gorilla-openfunctions-v2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "OpenFunctions is designed to extend Large Language Model (LLM) Chat Completion feature to formulate executable APIs call given natural language instructions and API context.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "gorilla-llm/gorilla-openfunctions-v2",
            "model_revision": "0f91d705e64b77fb55e35a7eab5d03bf965c9b5c"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K"
            ],
            "model_id": "gorilla-llm//gorilla-openfunctions-v2-GGUF",
            "model_file_name_template": "gorilla-openfunctions-v2.{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}\n{% set add_generation_prompt = false %}\n{% endif %}\n{%- set ns = namespace(found=false) -%}\n{%- for message in messages -%}\n    {%- if message['role'] == 'system' -%}\n        {%- set ns.found = true -%}\n    {%- endif -%}\n{%- endfor -%}\n{{'<｜begin▁of▁sentence｜>'}}{%- if not ns.found -%}\n{{'You are an AI programming assistant, utilizing the Gorilla LLM model, developed by Gorilla LLM, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n'}}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'system' %}\n{{ message['content'] }}\n    {%- else %}\n        {%- if message['role'] == 'user' %}\n{{'### Instruction:\n' + message['content'] + '\n'}}\n        {%- else %}\n{{'### Response:\n' + message['content'] + '\n<|EOT|>\n'}}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{% if add_generation_prompt %}\n{{'### Response:'}}\n{% endif %}",
    "stop_token_ids": [
      100015,
      100001
    ],
    "stop": [
      "<|EOT|>",
      "<｜end▁of▁sentence｜>"
    ],
    "updated_at": 1769418471,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 1024,
    "model_name": "gpt-2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "GPT-2 is a Transformer-based LLM that is trained on WebTest, a 40 GB dataset of Reddit posts with 3+ upvotes.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "openai-community/gpt2",
            "model_revision": "607a30d783dfa663caf39e06633721c8d4cfcd7e"
          }
        }
      }
    ],
    "updated_at": 1769418472,
    "featured": false,
    "architectures": [
      "GPT2LMHeadModel"
    ],
    "model_type": "gpt2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "llama-2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Llama-2 is the second generation of Llama, open-source and trained on a larger amount of data.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_0",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_0",
              "Q5_K_S",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/Llama-2-7B-GGUF",
            "model_file_name_template": "llama-2-7b.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Llama-2-7B-GPTQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Llama-2-7B-AWQ"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_0",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_0",
              "Q5_K_S",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/Llama-2-13B-GGUF",
            "model_file_name_template": "llama-2-13b.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_0",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_0",
              "Q5_K_S",
              "Q5_K_M"
            ],
            "quantization_parts": {
              "Q6_K": [
                "split-a",
                "split-b"
              ],
              "Q8_0": [
                "split-a",
                "split-b"
              ]
            },
            "model_id": "TheBloke/Llama-2-70B-GGUF",
            "model_file_name_template": "llama-2-70b.{quantization}.gguf",
            "model_file_name_split_template": "llama-2-70b.{quantization}.gguf-{part}"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Llama-2-7b-hf",
            "model_revision": "6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Llama-2-13b-hf",
            "model_revision": "db6b8eb1feabb38985fdf785a89895959e944936"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Llama-2-13B-GPTQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Llama-2-13B-AWQ"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Llama-2-70b-hf",
            "model_revision": "cc8aa03a000ff08b4d5c5b39673321a2a396c396"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Llama-2-70B-GPTQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Llama-2-70B-AWQ"
          }
        }
      }
    ],
    "updated_at": 1769418473,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "mistral-instruct-v0.3",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an improved instruct fine-tuned version of Mistral-7B-Instruct-v0.1.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mistralai/Mistral-7B-Instruct-v0.3",
            "model_revision": "83e9aa141f2e28c82232fea5325f54edf17c43de"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "solidrust/Mistral-7B-Instruct-v0.3-AWQ"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_K_S",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "fp16"
            ],
            "model_id": "MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF",
            "model_file_name_template": "Mistral-7B-Instruct-v0.3.{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS] [\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- '{\"type\": \"function\", \"function\": {' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- '\"' + key + '\": \"' + val + '\"' }}\n                    {%- else %}\n                        {{- '\"' + key + '\": ' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST] \" + system_message + \"\n\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST] \" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\n        {{- \"[TOOL_CALLS] [\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + '</s>' }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- \" \" + message[\"content\"]|trim + '</s>'}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- '[TOOL_RESULTS] {\"content\": ' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ],
    "updated_at": 1769418474,
    "featured": false,
    "architectures": [
      "MistralForCausalLM"
    ],
    "model_type": "mistral",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 65536,
    "model_name": "mixtral-8x22B-instruct-v0.1",
    "model_lang": [
      "en",
      "fr",
      "it",
      "de",
      "es"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Mixtral-8x22B-Instruct-v0.1 Large Language Model (LLM) is an instruct fine-tuned version of the Mixtral-8x22B-v0.1, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "141",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mistralai/Mixtral-8x22B-Instruct-v0.1",
            "model_revision": "ebb919ac9e9f7f9a900644621bae7963bc593f4f"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "141",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-AWQ"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "141",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "jarrelscy/Mixtral-8x22B-Instruct-v0.1-GPTQ-4bit"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "141",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6",
              "Q8_0",
              "fp16"
            ],
            "model_id": "MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF",
            "model_file_name_template": "Mixtral-8x22B-Instruct-{quantization}.gguf",
            "model_file_name_split_template": "Mixtral-8x22B-Instruct-v0.1.{quantization}-{part}.gguf",
            "quantization_parts": {
              "Q2_K": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q3_K_L": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q3_K_M": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q3_K_S": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_K_M": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q4_K_S": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q5_K_M": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q5_K_S": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q6": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q8_0": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "fp16": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ]
            }
          }
        }
      }
    ],
    "chat_template": "{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS] [\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- '{\"type\": \"function\", \"function\": {' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- '\"' + key + '\": \"' + val + '\"' }}\n                    {%- else %}\n                        {{- '\"' + key + '\": ' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST] \" + system_message + \"\n\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST] \" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\n        {{- \"[TOOL_CALLS] [\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + '</s>' }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- \" \" + message[\"content\"]|trim + '</s>'}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- '[TOOL_RESULTS] {\"content\": ' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ],
    "updated_at": 1769418475,
    "featured": false,
    "architectures": [
      "MixtralForCausalLM"
    ],
    "model_type": "mixtral",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 8192,
    "model_name": "openhermes-2.5",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Openhermes 2.5 is a fine-tuned version of Mistral-7B-v0.1 on primarily GPT-4 generated data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "teknium/OpenHermes-2.5-Mistral-7B",
            "model_revision": "91ed666be78da7556f3d79abbb26fff0ee26cb54"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_0",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_0",
              "Q5_K_S",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/OpenHermes-2.5-Mistral-7B-GGUF",
            "model_file_name_template": "openhermes-2.5-mistral-7b.{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      32000
    ],
    "stop": [
      "<|im_end|>"
    ],
    "updated_at": 1769418476,
    "featured": false,
    "architectures": [
      "MistralForCausalLM"
    ],
    "model_type": "mistral",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 2048,
    "model_name": "opt",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Opt is an open-source, decoder-only, Transformer based LLM that was designed to replicate GPT-3.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 1,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "facebook/opt-125m",
            "model_revision": "3d2b5f275bdf882b8775f902e1bfdb790e2cfc32"
          }
        }
      }
    ],
    "updated_at": 1769418477,
    "featured": false,
    "architectures": [
      "OPTForCausalLM"
    ],
    "model_type": "opt",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 2048,
    "model_name": "phi-2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Phi-2 is a 2.7B Transformer based LLM used for research on model safety, trained with data similar to Phi-1.5 but augmented with synthetic texts and curated websites.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_0",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_0",
              "Q5_K_S",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/phi-2-GGUF",
            "model_file_name_template": "phi-2.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "microsoft/phi-2",
            "model_revision": "d3186761bf5c4409f7679359284066c25ab668ee"
          }
        }
      }
    ],
    "updated_at": 1769418478,
    "featured": false,
    "architectures": [
      "PhiForCausalLM"
    ],
    "model_type": "phi",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 8192,
    "model_name": "seallm_v2",
    "model_lang": [
      "en",
      "zh",
      "vi",
      "id",
      "th",
      "ms",
      "km",
      "lo",
      "my",
      "tl"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "We introduce SeaLLM-7B-v2, the state-of-the-art multilingual LLM for Southeast Asian (SEA) languages",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "SeaLLMs/SeaLLM-7B-v2",
            "model_revision": "f1bd48e0d75365c24a3c5ad006b2d0a0c9dca30f"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q4_0",
              "Q8_0"
            ],
            "model_id": "SeaLLMs/SeaLLM-7B-v2-gguf",
            "model_file_name_template": "SeaLLM-7B-v2.{quantization}.gguf"
          }
        }
      }
    ],
    "updated_at": 1769418479,
    "featured": false,
    "architectures": [
      "MistralForCausalLM"
    ],
    "model_type": "mistral",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 8192,
    "model_name": "seallm_v2.5",
    "model_lang": [
      "en",
      "zh",
      "vi",
      "id",
      "th",
      "ms",
      "km",
      "lo",
      "my",
      "tl"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "We introduce SeaLLM-7B-v2.5, the state-of-the-art multilingual LLM for Southeast Asian (SEA) languages",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "SeaLLMs/SeaLLM-7B-v2.5",
            "model_revision": "c54a8eb8e2d58c5a680bfbbe3a7ae71753bb644b"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q4_K_M",
              "Q8_0"
            ],
            "model_id": "SeaLLMs/SeaLLM-7B-v2.5-GGUF",
            "model_file_name_template": "SeaLLM-7B-v2.5.{quantization}.gguf"
          }
        }
      }
    ],
    "updated_at": 1769418480,
    "featured": false,
    "architectures": [
      "GemmaForCausalLM"
    ],
    "model_type": "gemma",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "DianJin-R1",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Tongyi DianJin is a financial intelligence solution platform built by Alibaba Cloud, dedicated to providing financial business developers with a convenient artificial intelligence application development environment.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "DianJin/DianJin-R1-7B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "DianJin/DianJin-R1-7B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "DianJin/DianJin-R1-32B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "DianJin/DianJin-R1-32B"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "IQ4_XS",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_K_S",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "f16"
            ],
            "model_id": "mradermacher/DianJin-R1-7B-GGUF",
            "model_file_name_template": "DianJin-R1-7B.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "i1-IQ1_S",
              "i1-IQ1_M",
              "i1-IQ2_XXS",
              "i1-IQ2_XS",
              "i1-IQ2_S",
              "i1-IQ2_M",
              "i1-Q2_K_S",
              "i1-Q2_K",
              "i1-IQ3_XXS",
              "i1-IQ3_XS",
              "i1-Q3_K_S",
              "i1-IQ3_S",
              "i1-IQ3_M",
              "i1-Q3_K_M",
              "i1-Q3_K_L",
              "i1-IQ4_XS",
              "i1-IQ4_NL",
              "i1-Q4_0",
              "i1-Q4_K_S",
              "i1-Q4_K_M",
              "i1-Q4_1",
              "i1-Q5_K_S",
              "i1-Q5_K_M",
              "i1-Q6_K"
            ],
            "model_id": "mradermacher/DianJin-R1-7B-i1-GGUF",
            "model_file_name_template": "DianJin-R1-7B.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "IQ4_XS",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_K_S",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "mradermacher/DianJin-R1-32B-GGUF",
            "model_file_name_template": "DianJin-R1-32B.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "i1-IQ1_S",
              "i1-IQ1_M",
              "i1-IQ2_XXS",
              "i1-IQ2_XS",
              "i1-IQ2_S",
              "i1-IQ2_M",
              "i1-Q2_K_S",
              "i1-Q2_K",
              "i1-IQ3_XXS",
              "i1-IQ3_XS",
              "i1-Q3_K_S",
              "i1-IQ3_S",
              "i1-IQ3_M",
              "i1-Q3_K_M",
              "i1-Q3_K_L",
              "i1-IQ4_XS",
              "i1-Q4_0",
              "i1-Q4_K_S",
              "i1-Q4_K_M",
              "i1-Q4_1",
              "i1-Q5_K_S",
              "i1-Q5_K_M",
              "i1-Q6_K"
            ],
            "model_id": "mradermacher/DianJin-R1-32B-i1-GGUF",
            "model_file_name_template": "DianJin-R1-32B.{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "tool_parser": "qwen",
    "updated_at": 1769418481,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "HuatuoGPT-o1-LLaMA-3.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "HuatuoGPT-o1 is a medical LLM designed for advanced medical reasoning. It generates a complex thought process, reflecting and refining its reasoning, before providing a final response.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "FreedomIntelligence/HuatuoGPT-o1-8B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "FreedomIntelligence/HuatuoGPT-o1-8B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "FreedomIntelligence/HuatuoGPT-o1-70B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "FreedomIntelligence/HuatuoGPT-o1-70B"
          }
        }
      }
    ],
    "chat_template": "{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\"' + arg_val + '\"' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n            {{- '\"parameters\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      128001,
      128008,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>",
      "<|eom_id|>"
    ],
    "tool_parser": "llama3",
    "updated_at": 1769418481,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "HuatuoGPT-o1-Qwen2.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "HuatuoGPT-o1 is a medical LLM designed for advanced medical reasoning. It generates a complex thought process, reflecting and refining its reasoning, before providing a final response.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "FreedomIntelligence/HuatuoGPT-o1-7B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "FreedomIntelligence/HuatuoGPT-o1-7B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "FreedomIntelligence/HuatuoGPT-o1-72B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "FreedomIntelligence/HuatuoGPT-o1-72B"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "tool_parser": "qwen",
    "updated_at": 1769418482,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 8192,
    "model_name": "InternVL3",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "InternVL3, an advanced multimodal large language model (MLLM) series that demonstrates superior overall performance.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 1,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenGVLab/InternVL3-1B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenGVLab/InternVL3-1B",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 1,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OpenGVLab/InternVL3-1B-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OpenGVLab/InternVL3-1B-AWQ"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenGVLab/InternVL3-2B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenGVLab/InternVL3-2B",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OpenGVLab/InternVL3-2B-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OpenGVLab/InternVL3-2B-AWQ"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenGVLab/InternVL3-8B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenGVLab/InternVL3-8B",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OpenGVLab/InternVL3-8B-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OpenGVLab/InternVL3-8B-AWQ"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenGVLab/InternVL3-9B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenGVLab/InternVL3-9B",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OpenGVLab/InternVL3-9B-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OpenGVLab/InternVL3-9B-AWQ"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenGVLab/InternVL3-14B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenGVLab/InternVL3-14B",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OpenGVLab/InternVL3-14B-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OpenGVLab/InternVL3-14B-AWQ"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 38,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenGVLab/InternVL3-38B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenGVLab/InternVL3-38B",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 38,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OpenGVLab/InternVL3-38B-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OpenGVLab/InternVL3-38B-AWQ"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 78,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenGVLab/InternVL3-78B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenGVLab/InternVL3-78B",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 78,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OpenGVLab/InternVL3-78B-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OpenGVLab/InternVL3-78B-AWQ"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151645
    ],
    "stop": [
      "<|im_end|>"
    ],
    "updated_at": 1769418483,
    "featured": false,
    "architectures": [
      "InternVLChatModel"
    ],
    "model_type": "internvl_chat",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "MiniCPM-V-2.6",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "MiniCPM-V 2.6 is the latest model in the MiniCPM-V series. The model is built on SigLip-400M and Qwen2-7B with a total of 8B parameters.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "openbmb/MiniCPM-V-2_6",
            "model_revision": "3f7a8da1b7a8b928b5ee229fae33cf43fd64cf31"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenBMB/MiniCPM-V-2_6",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "openbmb/MiniCPM-V-2_6-int4",
            "model_revision": "051e2df6505f1fc4305f2c9bd42ed90db8bf4874"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenBMB/MiniCPM-V-2_6",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ],
    "updated_at": 1769418484,
    "featured": false,
    "architectures": [
      "MiniCPMV"
    ],
    "model_type": "minicpmv",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "Ovis2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Ovis (Open VISion) is a novel Multimodal Large Language Model (MLLM) architecture, designed to structurally align visual and textual embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 1,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "AIDC-AI/Ovis2-1B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AIDC-AI/Ovis2-1B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "AIDC-AI/Ovis2-2B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AIDC-AI/Ovis2-2B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "AIDC-AI/Ovis2-4B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AIDC-AI/Ovis2-4B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "AIDC-AI/Ovis2-8B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AIDC-AI/Ovis2-8B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 16,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "AIDC-AI/Ovis2-16B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AIDC-AI/Ovis2-16B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "AIDC-AI/Ovis2-34B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AIDC-AI/Ovis2-34B"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "AIDC-AI/Ovis2-2B-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "AIDC-AI/Ovis2-2B-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "AIDC-AI/Ovis2-4B-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "AIDC-AI/Ovis2-4B-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "AIDC-AI/Ovis2-8B-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "AIDC-AI/Ovis2-8B-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 16,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "AIDC-AI/Ovis2-16B-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "AIDC-AI/Ovis2-16B-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "AIDC-AI/Ovis2-34B-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "AIDC-AI/Ovis2-34B-GPTQ-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ],
    "updated_at": 1769418485,
    "featured": false,
    "architectures": [
      "Ovis"
    ],
    "model_type": "ovis",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "QvQ-72B-Preview",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "QVQ-72B-Preview is an experimental research model developed by the Qwen team, focusing on enhancing visual reasoning capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/QVQ-72B-Preview"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/QVQ-72B-Preview"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/QVQ-72B-Preview-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/QVQ-72B-Preview-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ],
    "updated_at": 1769418486,
    "featured": false,
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "model_type": "qwen2_vl",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "QwQ-32B",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "tools"
    ],
    "model_description": "QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/QwQ-32B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/QwQ-32B"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/QwQ-32B-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/QwQ-32B-AWQ"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/QwQ-32B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/QwQ-32B-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "Q8_0"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "unsloth/QwQ-32B-GGUF",
            "model_file_name_template": "QwQ-32B-{quantization}.gguf",
            "model_file_name_split_template": "BF16/QwQ-32B-{quantization}-{part}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "Q8_0"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "unsloth/QwQ-32B-GGUF",
            "model_file_name_template": "QwQ-32B-{quantization}.gguf",
            "model_file_name_split_template": "BF16/QwQ-32B-{quantization}-{part}.gguf"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- '' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n  {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" and not message.tool_calls %}\n        {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n<think>\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "tool_parser": "qwen",
    "updated_at": 1769418487,
    "featured": true,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "QwQ-32B-Preview",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "QwQ-32B-Preview is an experimental research model developed by the Qwen Team, focused on advancing AI reasoning capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/QwQ-32B-Preview"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/QwQ-32B-Preview"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "KirillR/QwQ-32B-Preview-AWQ"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q3_K_L",
              "Q4_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "lmstudio-community/QwQ-32B-Preview-GGUF",
            "model_file_name_template": "QwQ-32B-Preview-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q3_K_L",
              "Q4_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "AI-ModelScope/QwQ-32B-Preview-GGUF",
            "model_file_name_template": "QwQ-32B-Preview-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen_QwQ-32B-Preview_MLX-4bit"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "okwinds/QwQ-32B-Preview-MLX-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen_QwQ-32B-Preview_MLX-8bit"
          },
          "modelscope": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "okwinds/QwQ-32B-Preview-MLX-8bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mlx-community/QwQ-32B-Preview-bf16"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "updated_at": 1769418488,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "Skywork",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Skywork is a series of large models developed by the Kunlun Group · Skywork team.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "skywork/Skywork-13B-base",
            "model_revision": "bc35915066fbbf15b77a1a4a74e9b574ab167816"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "skywork/Skywork-13B-base",
            "model_revision": "master"
          }
        }
      }
    ],
    "updated_at": 1769418489,
    "featured": false,
    "architectures": [
      "SkyworkForCausalLM"
    ],
    "model_type": "skywork",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "Skywork-Math",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Skywork is a series of large models developed by the Kunlun Group · Skywork team.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "skywork/Skywork-13B-Math",
            "model_revision": "70d1740208c8ba39f9ba250b22117ec25311ab33"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "skywork/Skywork-13B-Math",
            "model_revision": "master"
          }
        }
      }
    ],
    "updated_at": 1769418490,
    "featured": false,
    "architectures": [
      "SkyworkForCausalLM"
    ],
    "model_type": "skywork",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "XiYanSQL-QwenCoder-2504",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "The XiYanSQL-QwenCoder models, as multi-dialect SQL base models, demonstrating robust SQL generation capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "XGenerationLab/XiYanSQL-QwenCoder-7B-2504"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "XGenerationLab/XiYanSQL-QwenCoder-7B-2504"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "XGenerationLab/XiYanSQL-QwenCoder-32B-2504"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "XGenerationLab/XiYanSQL-QwenCoder-32B-2504"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "tool_parser": "qwen",
    "updated_at": 1769418491,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "Yi",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/Yi-34B-GGUF",
            "model_file_name_template": "yi-34b.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "01-ai/Yi-6B",
            "model_revision": "25beebcb1166b9f49458459eb7b68130b9f9cf4d"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "01ai/Yi-6B",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "wuhaicc/Yi-6B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "01-ai/Yi-9B",
            "model_revision": "f70a5ff8b2e51c5d5b20e649d7b5f4238ffe6d5b"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "01ai/Yi-9B",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "wuhaicc/Yi-9B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "01-ai/Yi-34B",
            "model_revision": "168c48e05e1429779a896c7ef0d2e01b85e6bd8d"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "01ai/Yi-34B",
            "model_revision": "master"
          }
        }
      }
    ],
    "updated_at": 1769418492,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "Yi-1.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "01-ai/Yi-1.5-6B",
            "model_revision": "741a657c42d2081f777ce4c6c5572090f8b8c886"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "01ai/Yi-1.5-6B",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "HangZhou_Ascend/Yi-1.5-6B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "01-ai/Yi-1.5-9B",
            "model_revision": "9a6839c5b9db3dbb245fb98a072bfabc242621f2"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "01ai/Yi-1.5-9B",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "HangZhou_Ascend/Yi-1.5-9B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "01-ai/Yi-1.5-34B",
            "model_revision": "4f83007957ec3eec76d87df19ad061eb0f57b5c5"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "01ai/Yi-1.5-34B",
            "model_revision": "master"
          }
        }
      }
    ],
    "updated_at": 1769418493,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "Yi-1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "01-ai/Yi-1.5-6B-Chat",
            "model_revision": "d68dab90947a3c869e28c9cb2806996af99a6080"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "01ai/Yi-1.5-6B-Chat",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "01-ai/Yi-1.5-9B-Chat",
            "model_revision": "1dc6e2b8dcfc12b95bede8dec67e6b6332ac64c6"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "01ai/Yi-1.5-9B-Chat",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "01-ai/Yi-1.5-34B-Chat",
            "model_revision": "fa695ee438bfcd0ec2b378fa1c7e0dea1b40393e"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "01ai/Yi-1.5-34B-Chat",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 6,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q3_K_L",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "f32"
            ],
            "model_id": "lmstudio-community/Yi-1.5-6B-Chat-GGUF",
            "model_file_name_template": "Yi-1.5-6B-Chat-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q3_K_L",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "f32"
            ],
            "model_id": "lmstudio-community/Yi-1.5-9B-Chat-GGUF",
            "model_file_name_template": "Yi-1.5-9B-Chat-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "lmstudio-community/Yi-1.5-34B-Chat-GGUF",
            "model_file_name_template": "Yi-1.5-34B-Chat-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 6,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "modelscope/Yi-1.5-6B-Chat-GPTQ",
            "model_revision": "2ad3a602e64d1c79e28e6e92beced2935047367c"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "AI-ModelScope/Yi-1.5-6B-Chat-GPTQ",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "modelscope/Yi-1.5-9B-Chat-GPTQ",
            "model_revision": "76f47d16982923f7b6674c4e23ddac7c3b1d2e03"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "AI-ModelScope/Yi-1.5-9B-Chat-GPTQ",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "modelscope/Yi-1.5-34B-Chat-GPTQ",
            "model_revision": "173fb4036265b2dac1d6296a8e2fd2f652c19968"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "AI-ModelScope/Yi-1.5-34B-Chat-GPTQ",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 6,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "modelscope/Yi-1.5-6B-Chat-AWQ",
            "model_revision": "23bf37f1666874e15e239422de0d3948d8735fa9"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "AI-ModelScope/Yi-1.5-6B-Chat-AWQ",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "modelscope/Yi-1.5-9B-Chat-AWQ",
            "model_revision": "2605f388332672789eae1f422644add2901b433f"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "AI-ModelScope/Yi-1.5-9B-Chat-AWQ",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "modelscope/Yi-1.5-34B-Chat-AWQ",
            "model_revision": "26234fea6ac49d456f32f8017289021fb1087a04"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "AI-ModelScope/Yi-1.5-34B-Chat-AWQ",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 6,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Yi-1.5-6B-Chat-4bit",
            "model_revision": "0177c9a12b869d6bc73f772b5a1981a7c966adb6"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 6,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Yi-1.5-6B-Chat-8bit",
            "model_revision": "7756e65d1bf1e2e6e97aef6bc9484307225f536b"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Yi-1.5-9B-Chat-4bit",
            "model_revision": "e15f886479c44e7d90f0ac13ace69b2319b71c2f"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Yi-1.5-9B-Chat-8bit",
            "model_revision": "c1f742fcf3683edbe2d2c2fd1ad7ac2bb6c5ca36"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Yi-1.5-34B-Chat-4bit",
            "model_revision": "945e3b306ef37c46ab444fdc857d1f3ea7247374"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Yi-1.5-34B-Chat-8bit",
            "model_revision": "3c12761a2c6663f216caab6dff84b0dd29b472ac"
          }
        }
      }
    ],
    "chat_template": "{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\n' + content + '<|im_end|>\n<|im_start|>assistant\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\n' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      2,
      6,
      7,
      8
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>",
      "<|im_sep|>"
    ],
    "updated_at": 1769418494,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 16384,
    "model_name": "Yi-1.5-chat-16k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "01-ai/Yi-1.5-9B-Chat-16K",
            "model_revision": "551220fb24d69b6bfec5defceeb160395ce5da8d"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "01ai/Yi-1.5-9B-Chat-16K",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "01-ai/Yi-1.5-34B-Chat-16K",
            "model_revision": "dfdbc67be750972bfcc1ac7ffd7fe48689c856fd"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "01ai/Yi-1.5-34B-Chat-16K",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_1",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "QuantFactory/Yi-1.5-9B-Chat-16K-GGUF",
            "model_file_name_template": "Yi-1.5-9B-Chat-16K.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "bartowski/Yi-1.5-34B-Chat-16K-GGUF",
            "model_file_name_template": "Yi-1.5-34B-Chat-16K-{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\n' + content + '<|im_end|>\n<|im_start|>assistant\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\n' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      2,
      6,
      7,
      8
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>",
      "<|im_sep|>"
    ],
    "updated_at": 1769418495,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 262144,
    "model_name": "Yi-200k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "01-ai/Yi-6B-200K",
            "model_revision": "70649e36d43f91dff1357b576e6713cac03c1d4c"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "01ai/Yi-6B-200K",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "wuhaicc/Yi-6B-200K"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "01-ai/Yi-34B-200K",
            "model_revision": "591ae83b8f9c269700ef27f9dbd548934d800302"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "01ai/Yi-34B-200K",
            "model_revision": "master"
          }
        }
      }
    ],
    "updated_at": 1769418496,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "Yi-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
    "model_specs": [
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bits"
            ],
            "model_id": "01-ai/Yi-34B-Chat-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "8bits"
            ],
            "model_id": "01ai/Yi-34B-Chat-{quantization}",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "01-ai/Yi-6B-Chat",
            "model_revision": "1c20c960895e4c3877cf478bc2df074221b81d7b"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "01ai/Yi-6B-Chat",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "01-ai/Yi-34B-Chat",
            "model_revision": "a99ec35331cbfc9da596af7d4538fe2efecff03c"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "01ai/Yi-34B-Chat",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/Yi-34B-Chat-GGUF",
            "model_file_name_template": "yi-34b-chat.{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      2,
      6,
      7,
      8
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>",
      "<|im_sep|>"
    ],
    "updated_at": 1769418497,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "baichuan-2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Baichuan2 is an open-source Transformer based LLM that is trained on both Chinese and English data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "baichuan-inc/Baichuan2-7B-Base",
            "model_revision": "f2cc3a689c5eba7dc7fd3757d0175d312d167604"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "baichuan-inc/Baichuan2-7B-Base",
            "model_revision": "v1.0.2"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "PyTorch-NPU/baichuan2_7b_base"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "baichuan-inc/Baichuan2-13B-Base",
            "model_revision": "fa88072fee36e36282287410e00897df2f59e09b"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "baichuan-inc/Baichuan2-13B-Base",
            "model_revision": "v1.0.3"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "Baichuan/Baichuan2_13b_base_pt"
          }
        }
      }
    ],
    "updated_at": 1769418498,
    "featured": false,
    "architectures": [
      "BaichuanForCausalLM"
    ],
    "model_type": "baichuan",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "baichuan-2-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Baichuan2-chat is a fine-tuned version of the Baichuan LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "baichuan-inc/Baichuan2-7B-Chat",
            "model_revision": "2ce891951e000c36c65442608a0b95fd09b405dc"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "baichuan-inc/Baichuan2-7B-Chat",
            "model_revision": "v1.0.4"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "Baichuan/Baichuan2_7b_chat_pt"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "baichuan-inc/Baichuan2-13B-Chat",
            "model_revision": "a56c793eb7a721ab6c270f779024e0375e8afd4a"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "baichuan-inc/Baichuan2-13B-Chat",
            "model_revision": "v1.0.3"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "Baichuan/Baichuan2_13b_chat_pt"
          }
        }
      }
    ],
    "chat_template": "{{ (messages|selectattr('role', 'equalto', 'system')|list|last).content|trim if (messages|selectattr('role', 'equalto', 'system')|list) else '' }}\n\n{% for message in messages %}\n{% if message['role'] == 'user' %}\n<reserved_106>\n{{ message['content']|trim -}}\n{% if not loop.last %}\n\n\n{% endif %}\n{% elif message['role'] == 'assistant' %}\n<reserved_107>\n{{ message['content']|trim -}}\n{% if not loop.last %}\n\n\n{% endif %}\n{% endif %}\n{% endfor %}\n{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}\n<reserved_107>\n{% endif %}",
    "stop_token_ids": [
      2,
      195
    ],
    "stop": [],
    "updated_at": 1769418499,
    "featured": false,
    "architectures": [
      "BaichuanForCausalLM"
    ],
    "model_type": "baichuan",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 100000,
    "model_name": "code-llama",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Code-Llama is an open-source LLM trained by fine-tuning LLaMA2 for generating and discussing code.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "TheBloke/CodeLlama-7B-fp16",
            "model_revision": "ce09049eb9140a19cf78051cb5d849607b6fa8ec"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-ModelScope/CodeLlama-7b-hf",
            "model_revision": "v1.0.2"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "TheBloke/CodeLlama-13B-fp16",
            "model_revision": "d67ca1183da991d0d97927bdaaf35599556dfd76"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-ModelScope/CodeLlama-13b-hf",
            "model_revision": "v1.0.1"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "TheBloke/CodeLlama-34B-fp16",
            "model_revision": "f91d0cf7fc338cdc726f9c72d5ea15fe51bb16e9"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-ModelScope/CodeLlama-34b-hf",
            "model_revision": "v1.0.1"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/CodeLlama-7B-GGUF",
            "model_file_name_template": "codellama-7b.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/CodeLlama-13B-GGUF",
            "model_file_name_template": "codellama-13b.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/CodeLlama-34B-GGUF",
            "model_file_name_template": "codellama-34b.{quantization}.gguf"
          }
        }
      }
    ],
    "updated_at": 1769418500,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 100000,
    "model_name": "code-llama-instruct",
    "model_description": "Code-Llama-Instruct is an instruct-tuned version of the Code-Llama LLM.",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "codellama/CodeLlama-7b-Instruct-hf",
            "model_revision": "6114dd1e16f69e0765ccbd7a64d33d04b265fbd2"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-ModelScope/CodeLlama-7b-Instruct-hf",
            "model_revision": "v1.0.1"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "codellama/CodeLlama-13b-Instruct-hf",
            "model_revision": "ff0983bc4267bb98ead4fb5168fe2f049b442787"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-ModelScope/CodeLlama-13b-Instruct-hf",
            "model_revision": "v1.0.1"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "codellama/CodeLlama-34b-Instruct-hf",
            "model_revision": "38a1e15d8524a1f0a7760a7acf8242b81ae4eb87"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-ModelScope/CodeLlama-34b-Instruct-hf",
            "model_revision": "v1.0.2"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/CodeLlama-7B-Instruct-GGUF",
            "model_file_name_template": "codellama-7b-instruct.{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q4_K_M"
            ],
            "model_id": "Xorbits/CodeLlama-7B-Instruct-GGUF",
            "model_file_name_template": "codellama-7b-instruct.{quantization}.gguf",
            "model_revision": "v0.0.1"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/CodeLlama-13B-Instruct-GGUF",
            "model_file_name_template": "codellama-13b-instruct.{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q4_K_M"
            ],
            "model_id": "Xorbits/CodeLlama-13B-Instruct-GGUF",
            "model_file_name_template": "codellama-13b-instruct.{quantization}.gguf",
            "model_revision": "v0.0.1"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/CodeLlama-34B-Instruct-GGUF",
            "model_file_name_template": "codellama-34b-instruct.{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q4_K_M"
            ],
            "model_id": "Xorbits/CodeLlama-34B-Instruct-GGUF",
            "model_file_name_template": "codellama-34b-instruct.{quantization}.gguf",
            "model_revision": "v0.1.0"
          }
        }
      }
    ],
    "chat_template": "{% if messages[0]['role'] == 'system' %}{% set system_message = '<<SYS>>\n' + messages[0]['content'] | trim + '\n<</SYS>>\n\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ '<s>' + '[INST] ' + content | trim + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + content | trim + ' ' + '</s>' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ],
    "updated_at": 1769418501,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 100000,
    "model_name": "code-llama-python",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Code-Llama-Python is a fine-tuned version of the Code-Llama LLM, specializing in Python.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "TheBloke/CodeLlama-7B-Python-fp16",
            "model_revision": "d51c51e625bc24b9a7a0616e82681b4859e2cfe4"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Xorbits/CodeLlama-7B-Python-fp16",
            "model_revision": "v1.0.0"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "TheBloke/CodeLlama-13B-Python-fp16",
            "model_revision": "442282f4207442b828953a72c51a919c332cba5c"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Xorbits/CodeLlama-13B-Python-fp16",
            "model_revision": "v1.0.0"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "TheBloke/CodeLlama-34B-Python-fp16",
            "model_revision": "875f9d97fb6c9619d8867887dd1d80918ff0f593"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/CodeLlama-7B-Python-GGUF",
            "model_file_name_template": "codellama-7b-python.{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "Xorbits/CodeLlama-7B-Python-GGUF",
            "model_revision": "v1.0.0",
            "model_file_name_template": "codellama-7b-python.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/CodeLlama-13B-Python-GGUF",
            "model_file_name_template": "codellama-13b-python.{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "Xorbits/CodeLlama-13B-Python-GGUF",
            "model_file_name_template": "codellama-13b-python.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/CodeLlama-34B-Python-GGUF",
            "model_file_name_template": "codellama-34b-python.{quantization}.gguf"
          }
        }
      }
    ],
    "updated_at": 1769418502,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "codegeex4",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "the open-source version of the latest CodeGeeX4 model series",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/codegeex4-all-9b",
            "model_revision": "8c4ec1d2f2888412640825a7aa23355939a8f4c6"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/codegeex4-all-9b",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "IQ2_M",
              "IQ3_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K_L",
              "Q8_0"
            ],
            "model_file_name_template": "codegeex4-all-9b-{quantization}.gguf",
            "model_id": "zai-org/codegeex4-all-9b-GGUF",
            "model_revision": "6a04071c54c943949826d4815ee00717ed8cf153"
          },
          "modelscope": {
            "quantizations": [
              "IQ2_M",
              "IQ3_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K_L",
              "Q8_0"
            ],
            "model_file_name_template": "codegeex4-all-9b-{quantization}.gguf",
            "model_id": "ZhipuAI/codegeex4-all-9b-GGUF"
          }
        }
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ '<|system|>\n' + item['content'] }}{% elif loop.first %}{{ '<|system|>\n你是一位智能编程助手，你叫CodeGeeX。你会为用户回答关于编程、代码、计算机方面的任何问题，并提供格式规范、可以执行、准确安全的代码，并在必要时提供详细的解释。' }}{% endif %}{% if item['role'] == 'user' %}{{ '<|user|>\n' + item['content'] }}{% elif item['role'] == 'assistant' %}{{ '<|assistant|>\n' + item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n' }}{% endif %}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ],
    "updated_at": 1769418503,
    "featured": false,
    "architectures": [
      "ChatGLMModel"
    ],
    "model_type": "chatglm",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 65536,
    "model_name": "codeqwen1.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "CodeQwen1.5 is the Code-Specific version of Qwen1.5. It is a transformer-based decoder-only language model pretrained on a large amount of data of codes.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/CodeQwen1.5-7B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/CodeQwen1.5-7B"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "HangZhou_Ascend/CodeQwen1.5-7B"
          }
        }
      }
    ],
    "updated_at": 1769418503,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 65536,
    "model_name": "codeqwen1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "CodeQwen1.5 is the Code-Specific version of Qwen1.5. It is a transformer-based decoder-only language model pretrained on a large amount of data of codes.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Qwen/CodeQwen1.5-7B-Chat-GGUF",
            "model_file_name_template": "codeqwen-1_5-7b-chat-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/CodeQwen1.5-7B-Chat-GGUF",
            "model_file_name_template": "codeqwen-1_5-7b-chat-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/CodeQwen1.5-7B-Chat"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/CodeQwen1.5-7B-Chat"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "HangZhou_Ascend/CodeQwen1.5-7B-Chat"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/CodeQwen1.5-7B-Chat-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/CodeQwen1.5-7B-Chat-AWQ"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "updated_at": 1769418504,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 8194,
    "model_name": "codeshell",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "CodeShell is a multi-language code LLM developed by the Knowledge Computing Lab of Peking University. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "WisdomShell/CodeShell-7B",
            "model_revision": "1c79ab7fd316a62ab41d764facd3548a23fa5dee"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "WisdomShell/CodeShell-7B",
            "model_revision": "master"
          }
        }
      }
    ],
    "updated_at": 1769418505,
    "featured": false,
    "architectures": [
      "CodeShellForCausalLM"
    ],
    "model_type": "kclgpt",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 8194,
    "model_name": "codeshell-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "CodeShell is a multi-language code LLM developed by the Knowledge Computing Lab of Peking University.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "WisdomShell/CodeShell-7B-Chat",
            "model_revision": "3cb06f589b7b1e2f8e728c77280b1114191d24de"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "WisdomShell/CodeShell-7B-Chat",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% for item in messages %}{% if item['role'] == 'user' %}{{ '## human: ' + item['content'] + '|<end>|' }}{% elif item['role'] == 'assistant' %}{{ '## assistant: ' + item['content'] + '|<end>|' }}{% endif %}{% endfor %}{{ '## assistant: ' }}",
    "stop_token_ids": [
      70000
    ],
    "stop": [
      "<|endoftext|>",
      "|||",
      "|<end>|"
    ],
    "updated_at": 1769418506,
    "featured": false,
    "architectures": [
      "CodeShellForCausalLM"
    ],
    "model_type": "codeshell",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "cogagent",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "The CogAgent-9B-20241220 model is based on GLM-4V-9B, a bilingual open-source VLM base model. Through data collection and optimization, multi-stage training, and strategy improvements, CogAgent-9B-20241220 achieves significant advancements in GUI perception, inference prediction accuracy, action space completeness, and task generalizability. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "9",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/cogagent-9b-20241220"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/cogagent-9b-20241220"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ],
    "updated_at": 1769418507,
    "featured": false,
    "architectures": [
      "ChatGLMForConditionalGeneration"
    ],
    "model_type": "chatglm",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "deepseek",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "DeepSeek LLM, trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-llm-7b-base",
            "model_revision": "7683fea62db869066ddaff6a41d032262c490d4f"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-llm-7b-base"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 67,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-llm-67b-base",
            "model_revision": "c3f813a1121c95488a20132d3a4da89f4a46452f"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-llm-67b-base"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/deepseek-llm-7B-chat-GGUF",
            "model_file_name_template": "deepseek-llm-7b-chat.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 67,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/deepseek-llm-67b-chat-GGUF",
            "model_file_name_template": "deepseek-llm-67b-chat.{quantization}.gguf"
          }
        }
      }
    ],
    "updated_at": 1769418508,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "deepseek-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek LLM is an advanced language model comprising 67 billion parameters. It has been trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-llm-7b-chat",
            "model_revision": "afbda8b347ec881666061fa67447046fc5164ec8"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-llm-7b-chat"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 67,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-llm-67b-chat",
            "model_revision": "79648bef7658bb824e4630740f6e1484c1b0620b"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-llm-67b-chat"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/deepseek-llm-7B-chat-GGUF",
            "model_file_name_template": "deepseek-llm-7b-chat.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 67,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/deepseek-llm-67b-chat-GGUF",
            "model_file_name_template": "deepseek-llm-67b-chat.{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ '<｜begin▁of▁sentence｜>' }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n\n' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + '<｜end▁of▁sentence｜>' }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}",
    "stop_token_ids": [
      100001
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "updated_at": 1769418509,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 16384,
    "model_name": "deepseek-coder",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_3",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-coder-1.3b-base",
            "model_revision": "c919139c3a9b4070729c8b2cca4847ab29ca8d94"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-coder-1.3b-base"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "6_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-coder-6.7b-base",
            "model_revision": "ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-coder-6.7b-base"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-coder-7b-base-v1.5",
            "model_revision": "98f0904cee2237e235f10408ae12292037b21dac"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 33,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-coder-33b-base",
            "model_revision": "45c85cadf3720ef3e85a492e24fd4b8c5d21d8ac"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-coder-33b-base"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_3",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/deepseek-coder-1.3b-base-GGUF",
            "model_file_name_template": "deepseek-coder-1.3b-base.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "6_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/deepseek-coder-6.7B-base-GGUF",
            "model_file_name_template": "deepseek-coder-6.7b-base.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "dagbs/deepseek-coder-7b-base-v1.5-GGUF",
            "model_file_name_template": "deepseek-coder-7b-base-v1.5.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 33,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/deepseek-coder-33B-base-GGUF",
            "model_file_name_template": "deepseek-coder-33b-base.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_3",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/deepseek-coder-1.3b-base-GPTQ",
            "model_revision": "a5bf3b76d70cda53327311a631b1003024d5de29"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "6_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/deepseek-coder-6.7B-base-GPTQ",
            "model_revision": "6476ea3d6e623a1313d363dbc6e172773e031bb1"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 33,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/deepseek-coder-33B-base-GPTQ",
            "model_revision": "f527d7325e463a5cb091d044e4f2b15902674a70"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_3",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/deepseek-coder-1.3b-base-AWQ",
            "model_revision": "ffb66f1a2a194401b4f29025edcd261d7f0a08a7"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "6_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/deepseek-coder-6.7B-base-AWQ",
            "model_revision": "e3d4bdf39712665f5e9d5c05c9df6f20fe1e2d5a"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 33,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/deepseek-coder-33B-base-AWQ",
            "model_revision": "c7edb2d5868d61a5dcf2591933a8992c8cbe3ef4"
          }
        }
      }
    ],
    "updated_at": 1769418510,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 16384,
    "model_name": "deepseek-coder-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "deepseek-coder-instruct is a model initialized from deepseek-coder-base and fine-tuned on 2B tokens of instruction data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_3",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-coder-1.3b-instruct",
            "model_revision": "2df081ceaca101a867fef2844e44f4d6a4857039"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-coder-1.3b-instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "6_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-coder-6.7b-instruct",
            "model_revision": "cbb77d7448ea3168d884758817e7f895e3828d1c"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-coder-6.7b-instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-coder-7b-instruct-v1.5",
            "model_revision": "2a050a4c59d687a85324d32e147517992117ed30"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 33,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-coder-33b-instruct",
            "model_revision": "ea15d17db84d1fc94ac5cba8e6fa97764c9549d3"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-coder-33b-instruct"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_3",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/deepseek-coder-1.3b-instruct-GGUF",
            "model_file_name_template": "deepseek-coder-1.3b-instruct.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "6_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/deepseek-coder-6.7B-instruct-GGUF",
            "model_file_name_template": "deepseek-coder-6.7b-instruct.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "LoneStriker/deepseek-coder-7b-instruct-v1.5-GGUF",
            "model_file_name_template": "deepseek-coder-7b-instruct-v1.5-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 33,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/deepseek-coder-33B-instruct-GGUF",
            "model_file_name_template": "deepseek-coder-33b-instruct.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_3",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/deepseek-coder-1.3b-instruct-GPTQ",
            "model_revision": "9c002e9af6cbdf3bd9244e2d7264b6a35d1dcacf"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "6_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/deepseek-coder-6.7B-instruct-GPTQ",
            "model_revision": "13ccea6e3a43dcfdcb655d92097610018b431a17"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 33,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/deepseek-coder-33B-instruct-GPTQ",
            "model_revision": "08372729d98dfc248f9531a412fe69e14e607027"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_3",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/deepseek-coder-1.3b-instruct-AWQ",
            "model_revision": "a2a484da6e4146d055316a9a63cf5b13955715a4"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "6_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/deepseek-coder-6.7B-instruct-AWQ",
            "model_revision": "502ae3e19e57ae78dc30a791ba33c565da72dc62"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 33,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/deepseek-coder-33B-instruct-AWQ",
            "model_revision": "c40b499bac2712cd3c445cf1b05d2c6558ab0d29"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}\n{% set add_generation_prompt = false %}\n{% endif %}\n{%- set ns = namespace(found=false) -%}\n{%- for message in messages -%}\n    {%- if message['role'] == 'system' -%}\n        {%- set ns.found = true -%}\n    {%- endif -%}\n{%- endfor -%}\n{{'<｜begin▁of▁sentence｜>'}}{%- if not ns.found -%}\n{{'You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n'}}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'system' %}\n{{ message['content'] }}\n    {%- else %}\n        {%- if message['role'] == 'user' %}\n{{'### Instruction:\n' + message['content'] + '\n'}}\n        {%- else %}\n{{'### Response:\n' + message['content'] + '\n<|EOT|>\n'}}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{% if add_generation_prompt %}\n{{'### Response:'}}\n{% endif %}",
    "stop_token_ids": [
      32021
    ],
    "stop": [
      "<|EOT|>"
    ],
    "updated_at": 1769418511,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 163840,
    "model_name": "deepseek-prover-v2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning"
    ],
    "model_description": "We introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3's step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-Prover-V2-671B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-Prover-V2-671B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-Prover-V2-7B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-Prover-V2-7B"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/DeepSeek-Prover-V2-7B-4bit"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/DeepSeek-Prover-V2-7B-4bit"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\n\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{'<｜User｜>' + message['content'] + '<｜Assistant｜>'}}{%- endif %}{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{%- endif %}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- set ns.is_output_first = true %}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<｜tool▁call▁end｜>'}}{%- else %}{{message['content'] + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- endfor %}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none)%}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{{content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_last_user and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}",
    "stop_token_ids": [
      1
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "updated_at": 1769418511,
    "featured": false,
    "architectures": [
      "DeepseekV3ForCausalLM"
    ],
    "model_type": "deepseek_v3",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 163840,
    "model_name": "deepseek-r1",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning"
    ],
    "model_description": "DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1",
            "model_revision": "8a58a132790c9935686eb97f042afa8013451c9f"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "cognitivecomputations/DeepSeek-R1-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "cognitivecomputations/DeepSeek-R1-awq",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "UD-IQ1_S",
              "UD-IQ1_M",
              "UD-IQ2_XXS",
              "UD-Q2_K_XL",
              "Q2_K",
              "Q2_K_L",
              "Q2_K_XS",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16"
            ],
            "model_id": "unsloth/DeepSeek-R1-GGUF",
            "model_file_name_template": "DeepSeek-R1-{quantization}/DeepSeek-R1-{quantization}.gguf",
            "model_file_name_split_template": "DeepSeek-R1-{quantization}/DeepSeek-R1-{quantization}-{part}.gguf",
            "quantization_parts": {
              "UD-IQ1_S": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "UD-IQ1_M": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "UD-IQ2_XXS": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "UD-Q2_K_XL": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q2_K": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q2_K_L": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q2_K_XS": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q3_K_M": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "Q4_K_M": [
                "00001-of-00009",
                "00002-of-00009",
                "00003-of-00009",
                "00004-of-00009",
                "00005-of-00009",
                "00006-of-00009",
                "00007-of-00009",
                "00008-of-00009",
                "00009-of-00009"
              ],
              "Q5_K_M": [
                "00001-of-00010",
                "00002-of-00010",
                "00003-of-00010",
                "00004-of-00010",
                "00005-of-00010",
                "00006-of-00010",
                "00007-of-00010",
                "00008-of-00010",
                "00009-of-00010",
                "00010-of-00010"
              ],
              "Q6_K": [
                "00001-of-00012",
                "00002-of-00012",
                "00003-of-00012",
                "00004-of-00012",
                "00005-of-00012",
                "00006-of-00012",
                "00007-of-00012",
                "00008-of-00012",
                "00009-of-00012",
                "00010-of-00012",
                "00011-of-00012",
                "00012-of-00012"
              ],
              "Q8_0": [
                "00001-of-00015",
                "00002-of-00015",
                "00003-of-00015",
                "00004-of-00015",
                "00005-of-00015",
                "00006-of-00015",
                "00007-of-00015",
                "00008-of-00015",
                "00009-of-00015",
                "00010-of-00015",
                "00011-of-00015",
                "00012-of-00015",
                "00013-of-00015",
                "00014-of-00015",
                "00015-of-00015"
              ],
              "BF16": [
                "00001-of-00030",
                "00002-of-00030",
                "00003-of-00030",
                "00004-of-00030",
                "00005-of-00030",
                "00006-of-00030",
                "00007-of-00030",
                "00008-of-00030",
                "00009-of-00030",
                "00010-of-00030",
                "00011-of-00030",
                "00012-of-00030",
                "00013-of-00030",
                "00014-of-00030",
                "00015-of-00030",
                "00016-of-00030",
                "00017-of-00030",
                "00018-of-00030",
                "00019-of-00030",
                "00020-of-00030",
                "00021-of-00030",
                "00022-of-00030",
                "00023-of-00030",
                "00024-of-00030",
                "00025-of-00030",
                "00026-of-00030",
                "00027-of-00030",
                "00028-of-00030",
                "00029-of-00030",
                "00030-of-00030"
              ]
            }
          },
          "modelscope": {
            "quantizations": [
              "UD-IQ1_S",
              "UD-IQ1_M",
              "UD-IQ2_XXS",
              "UD-Q2_K_XL",
              "Q2_K",
              "Q2_K_L",
              "Q2_K_XS",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16"
            ],
            "model_id": "unsloth/DeepSeek-R1-GGUF",
            "model_file_name_template": "DeepSeek-R1-{quantization}/DeepSeek-R1-{quantization}.gguf",
            "model_file_name_split_template": "DeepSeek-R1-{quantization}/DeepSeek-R1-{quantization}-{part}.gguf",
            "quantization_parts": {
              "UD-IQ1_S": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "UD-IQ1_M": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "UD-IQ2_XXS": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "UD-Q2_K_XL": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q2_K": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q2_K_L": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q2_K_XS": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q3_K_M": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "Q4_K_M": [
                "00001-of-00009",
                "00002-of-00009",
                "00003-of-00009",
                "00004-of-00009",
                "00005-of-00009",
                "00006-of-00009",
                "00007-of-00009",
                "00008-of-00009",
                "00009-of-00009"
              ],
              "Q5_K_M": [
                "00001-of-00010",
                "00002-of-00010",
                "00003-of-00010",
                "00004-of-00010",
                "00005-of-00010",
                "00006-of-00010",
                "00007-of-00010",
                "00008-of-00010",
                "00009-of-00010",
                "00010-of-00010"
              ],
              "Q6_K": [
                "00001-of-00012",
                "00002-of-00012",
                "00003-of-00012",
                "00004-of-00012",
                "00005-of-00012",
                "00006-of-00012",
                "00007-of-00012",
                "00008-of-00012",
                "00009-of-00012",
                "00010-of-00012",
                "00011-of-00012",
                "00012-of-00012"
              ],
              "Q8_0": [
                "00001-of-00015",
                "00002-of-00015",
                "00003-of-00015",
                "00004-of-00015",
                "00005-of-00015",
                "00006-of-00015",
                "00007-of-00015",
                "00008-of-00015",
                "00009-of-00015",
                "00010-of-00015",
                "00011-of-00015",
                "00012-of-00015",
                "00013-of-00015",
                "00014-of-00015",
                "00015-of-00015"
              ],
              "BF16": [
                "00001-of-00030",
                "00002-of-00030",
                "00003-of-00030",
                "00004-of-00030",
                "00005-of-00030",
                "00006-of-00030",
                "00007-of-00030",
                "00008-of-00030",
                "00009-of-00030",
                "00010-of-00030",
                "00011-of-00030",
                "00012-of-00030",
                "00013-of-00030",
                "00014-of-00030",
                "00015-of-00030",
                "00016-of-00030",
                "00017-of-00030",
                "00018-of-00030",
                "00019-of-00030",
                "00020-of-00030",
                "00021-of-00030",
                "00022-of-00030",
                "00023-of-00030",
                "00024-of-00030",
                "00025-of-00030",
                "00026-of-00030",
                "00027-of-00030",
                "00028-of-00030",
                "00029-of-00030",
                "00030-of-00030"
              ]
            }
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "2bit",
              "3bit",
              "4bit"
            ],
            "model_id": "mlx-community/DeepSeek-R1-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "2bit",
              "3bit",
              "4bit"
            ],
            "model_id": "mlx-community/DeepSeek-R1-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}",
    "stop_token_ids": [
      1
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "updated_at": 1769418512,
    "featured": false,
    "architectures": [
      "DeepseekV3ForCausalLM"
    ],
    "model_type": "deepseek_v3",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 163840,
    "model_name": "deepseek-r1-0528",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "tools"
    ],
    "model_description": "DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-0528"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-0528"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4-Int8Mix-Lite",
              "Int4-Int8Mix-Compact",
              "Int4-Int8Mix-Medium"
            ],
            "model_id": "QuantTrio/DeepSeek-R1-0528-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4-Int8Mix-Lite",
              "Int4-Int8Mix-Compact",
              "Int4-Int8Mix-Medium"
            ],
            "model_id": "tclf90/DeepSeek-R1-0528-GPTQ-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{%- if not add_generation_prompt is defined %}{%- set add_generation_prompt = false %}{%- endif %}{%- set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{%- set ns.system_prompt = ns.system_prompt + message['content'] %}{%- set ns.is_first_sp = false %}{%- else %}{%- set ns.system_prompt = ns.system_prompt + '\n\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{%- if tools is defined and tools is not none %}{%- set tool_ns = namespace(text='You are a helpful assistant with tool calling capabilities. ' + 'When a tool call is needed, you MUST use the following format to issue the call:\n' + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>function<｜tool▁sep｜>FUNCTION_NAME\n' + '```json\n{\"param1\": \"value1\", \"param2\": \"value2\"}\n```<｜tool▁call▁end｜><｜tool▁calls▁end｜>\n\n' + 'Make sure the JSON is valid.' + '## Tools\n\n### Function\n\nYou have the following functions available:\n\n') %}{%- for tool in tools %}{%- set tool_ns.text = tool_ns.text + '\n```json\n' + (tool | tojson) + '\n```\n' %}{%- endfor %}{%- if ns.system_prompt|length != 0 %}{%- set ns.system_prompt = ns.system_prompt + '\n\n' + tool_ns.text %}{%- else %}{%- set ns.system_prompt = tool_ns.text %}{%- endif %}{%- endif %}{{- bos_token }}{{- ns.system_prompt }}{%- set last_index = (messages|length - 1) %}{%- for message in messages %}{%- set content = message['content'] %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{%- if loop.index0 == last_index %}{{- '<｜User｜>' + content }}{%- else %}{{- '<｜User｜>' + content + '<｜Assistant｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'assistant' %}{%- if '</think>' in content %}{%- set content = (content.split('</think>')|last) %}{%- endif %}{%- endif %}{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{- '<｜tool▁outputs▁end｜>'}}{%- endif %}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- set ns.is_output_first = true %}{%- for tool in message['tool_calls'] %}{%- set arguments = tool['function']['arguments'] %}{%- if arguments is not string %}{%- set arguments = arguments|tojson %}{%- endif %}{%- if not ns.is_first %}{%- if content is none %}{{- '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\n' + '```json' + '\n' + arguments + '\n' + '```' + '<｜tool▁call▁end｜>'}}{%- else %}{{- content + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\n' + '```json' + '\n' + arguments + '\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{- '\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\n' + '```json' + '\n' + arguments + '\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- endfor %}{{- '<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none) %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{- '<｜tool▁outputs▁end｜>' + content + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{{- content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{- '<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + content + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{- '\n<｜tool▁output▁begin｜>' + content + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{%- if ns.is_tool %}{{- '<｜tool▁outputs▁end｜>'}}{%- endif %}{%- if add_generation_prompt and not ns.is_tool %}{{- '<｜Assistant｜>'}}{%- endif %}",
    "stop_token_ids": [
      1
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "tool_parser": "deepseek-r1",
    "updated_at": 1769418513,
    "featured": false,
    "architectures": [
      "DeepseekV3ForCausalLM"
    ],
    "model_type": "deepseek_v3",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "deepseek-r1-0528-qwen3",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning"
    ],
    "model_description": "The DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4-W4A16",
              "Int8-W8A16"
            ],
            "model_id": "QuantTrio/DeepSeek-R1-0528-Qwen3-8B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4-W4A16",
              "Int8-W8A16"
            ],
            "model_id": "okwinds/DeepSeek-R1-0528-Qwen3-8B-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "QuantTrio/DeepSeek-R1-0528-Qwen3-8B-GPTQ-Int4-Int8Mix"
          },
          "modelscope": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "tclf90/DeepSeek-R1-0528-Qwen3-8B-GPTQ-Int4-Int8Mix"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\n\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{% set content = message['content'] %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{'<｜User｜>' + content + '<｜Assistant｜>'}}{%- endif %}{%- if message['role'] == 'assistant' %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{% endif %}{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{%- endif %}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- set ns.is_output_first = true %}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if content is none %}{{'<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<｜tool▁call▁end｜>'}}{%- else %}{{content + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- endfor %}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none)%}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + content + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{{content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + content + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\n<｜tool▁output▁begin｜>' + content + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_last_user and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}",
    "stop_token_ids": [
      151645
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "updated_at": 1769418514,
    "featured": false,
    "architectures": [
      "Qwen3ForCausalLM"
    ],
    "model_type": "qwen3",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "deepseek-r1-distill-llama",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning"
    ],
    "model_description": "deepseek-r1-distill-llama is distilled from DeepSeek-R1 based on Llama",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/DeepSeek-R1-Distill-Llama-8B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "okwinds/DeepSeek-R1-Distill-Llama-8B-MLX-{quantization}"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "F16"
            ],
            "quantization_parts": {
              "Q6_K": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q8_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "F16": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ]
            },
            "model_id": "unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF",
            "model_file_name_template": "DeepSeek-R1-Distill-Qwen-7B-{quantization}.gguf",
            "model_file_name_split_template": "DeepSeek-R1-Distill-Llama-70B-{quantization}/DeepSeek-R1-Distill-Llama-70B-{quantization}-{part}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "F16"
            ],
            "quantization_parts": {
              "Q6_K": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q8_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "F16": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ]
            },
            "model_id": "unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF",
            "model_file_name_template": "DeepSeek-R1-Distill-Qwen-7B-{quantization}.gguf",
            "model_file_name_split_template": "DeepSeek-R1-Distill-Llama-70B-{quantization}/DeepSeek-R1-Distill-Llama-70B-{quantization}-{part}.gguf"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/DeepSeek-R1-Distill-Llama-70B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit"
            ],
            "model_id": "okwinds/DeepSeek-R1-Distill-Llama-70B-MLX-{quantization}"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "jakiAJK/DeepSeek-R1-Distill-Llama-8B_AWQ"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "jakiAJK/DeepSeek-R1-Distill-Llama-8B_GPTQ-int4"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "F16"
            ],
            "model_id": "unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF",
            "model_file_name_template": "DeepSeek-R1-Distill-Llama-8B-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "casperhansen/deepseek-r1-distill-llama-70b-awq"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "empirischtech/DeepSeek-R1-Distill-Llama-70B-gptq-4bit"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "model_src": {
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "F16"
            ],
            "model_id": "unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF",
            "model_file_name_template": "DeepSeek-R1-Distill-Llama-8B-{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}",
    "stop_token_ids": [
      151643
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "tool_parser": "deepseek-r1",
    "updated_at": 1769418515,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "deepseek-r1-distill-qwen",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning"
    ],
    "model_description": "deepseek-r1-distill-qwen is distilled from DeepSeek-R1 based on Qwen",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF",
            "model_file_name_template": "DeepSeek-R1-Distill-Qwen-1.5B-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF",
            "model_file_name_template": "DeepSeek-R1-Distill-Qwen-1.5B-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-{quantization}"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "jakiAJK/DeepSeek-R1-Distill-Qwen-7B_GPTQ-int4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/deepseek-r1-distill-qwen-7b-gptq-int4"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "F16"
            ],
            "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF",
            "model_file_name_template": "DeepSeek-R1-Distill-Qwen-7B-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "F16"
            ],
            "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF",
            "model_file_name_template": "DeepSeek-R1-Distill-Qwen-7B-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "F16"
            ],
            "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF",
            "model_file_name_template": "DeepSeek-R1-Distill-Qwen-14B-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "F16"
            ],
            "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF",
            "model_file_name_template": "DeepSeek-R1-Distill-Qwen-14B-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "F16"
            ],
            "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF",
            "model_file_name_template": "DeepSeek-R1-Distill-Qwen-32B-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "F16"
            ],
            "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF",
            "model_file_name_template": "DeepSeek-R1-Distill-Qwen-32B-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "casperhansen/deepseek-r1-distill-qwen-1.5b-awq"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "jakiAJK/DeepSeek-R1-Distill-Qwen-1.5B_GPTQ-int4"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "jakiAJK/DeepSeek-R1-Distill-Qwen-7B_AWQ"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/DeepSeek-R1-Distill-Qwen-7B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit"
            ],
            "model_id": "okwinds/DeepSeek-R1-Distill-Qwen-7B-MLX-{quantization}"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "casperhansen/deepseek-r1-distill-qwen-14b-awq"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/DeepSeek-R1-Distill-Qwen-14B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit"
            ],
            "model_id": "okwinds/DeepSeek-R1-Distill-Qwen-14B-MLX-{quantization}"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "casperhansen/deepseek-r1-distill-qwen-32b-awq"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/DeepSeek-R1-Distill-Qwen-32B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "2bit",
              "3bit",
              "4bit",
              "6bit",
              "8bit"
            ],
            "model_id": "okwinds/DeepSeek-R1-Distill-Qwen-32B-MLX-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "model_src": {
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/deepseek-r1-distill-qwen-32b-gptq-int4"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' in message %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- else %}{{'<｜Assistant｜>' + message['content'] + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- endfor %}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' not in message %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}",
    "stop_token_ids": [
      151643
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "updated_at": 1769418516,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 128000,
    "model_name": "deepseek-v2-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 16,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-V2-Lite-Chat",
            "model_revision": "85864749cd611b4353ce1decdb286193298f64c7"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-V2-Lite-Chat",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 236,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-V2-Chat",
            "model_revision": "8e3f5f6c2226787e41ba3e9283a06389d178c926"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-V2-Chat",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ '<｜begin▁of▁sentence｜>' }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n\n' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + '<｜end▁of▁sentence｜>' }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}",
    "stop_token_ids": [
      100001
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "updated_at": 1769418517,
    "featured": false,
    "architectures": [
      "DeepseekV2ForCausalLM"
    ],
    "model_type": "deepseek_v2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 128000,
    "model_name": "deepseek-v2-chat-0628",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek-V2-Chat-0628 is an improved version of DeepSeek-V2-Chat. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 236,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-V2-Chat-0628",
            "model_revision": "5d09e272c2b223830f4e84359cd9dd047a5d7c78"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-V2-Chat-0628",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ '<｜begin▁of▁sentence｜>' }}{% for message in messages %}{% if message['role'] == 'user' %}{{ '<｜User｜>' + message['content'] }}{% elif message['role'] == 'assistant' %}{{ '<｜Assistant｜>' + message['content'] + '<｜end▁of▁sentence｜>' }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<｜Assistant｜>' }}{% endif %}",
    "stop_token_ids": [
      100001
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "updated_at": 1769418518,
    "featured": false,
    "architectures": [
      "DeepseekV2ForCausalLM"
    ],
    "model_type": "deepseek_v2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 128000,
    "model_name": "deepseek-v2.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. The new model integrates the general and coding abilities of the two previous versions.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 236,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-V2.5",
            "model_revision": "24b08cb750e0c2757de112d2e16327cb21ed4833"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-V2.5",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}    {%- if message['role'] == 'system' %}        {% set ns.system_prompt = message['content'] %}    {%- endif %}{%- endfor %}{{'<｜begin▁of▁sentence｜>'}}{{ns.system_prompt}}{%- for message in messages %}    {%- if message['role'] == 'user' %}    {%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}    {%- endif %}    {%- if message['role'] == 'assistant' and message['content'] is none %}        {%- set ns.is_tool = false -%}        {%- for tool in message['tool_calls']%}            {%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}            {%- set ns.is_first = true -%}            {%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}                   {%- endif %}        {%- endfor %}    {%- endif %}    {%- if message['role'] == 'assistant' and message['content'] is not none %}        {%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}        {%- set ns.is_tool = false -%}        {%- else %}{{'<｜Assistant｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}        {%- endif %}    {%- endif %}    {%- if message['role'] == 'tool' %}        {%- set ns.is_tool = true -%}        {%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}        {%- set ns.is_output_first = false %}        {%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}        {%- endif %}    {%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}",
    "stop_token_ids": [
      100001
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "updated_at": 1769418519,
    "featured": false,
    "architectures": [
      "DeepseekV2ForCausalLM"
    ],
    "model_type": "deepseek_v2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 163840,
    "model_name": "deepseek-v3",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-V3",
            "model_revision": "1d044fd82b15f1cedb197a288e50cc96a2c27205"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-V3",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "cognitivecomputations/DeepSeek-V3-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "cognitivecomputations/DeepSeek-V3-awq",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K_L",
              "Q2_K_XS",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "unsloth/DeepSeek-V3-GGUF",
            "model_file_name_template": "DeepSeek-V3-{quantization}/DeepSeek-V3-{quantization}.gguf",
            "model_file_name_split_template": "DeepSeek-V3-{quantization}/DeepSeek-V3-{quantization}-{part}.gguf",
            "quantization_parts": {
              "Q2_K_L": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q2_K_XS": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q3_K_M": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "Q4_K_M": [
                "00001-of-00009",
                "00002-of-00009",
                "00003-of-00009",
                "00004-of-00009",
                "00005-of-00009",
                "00006-of-00009",
                "00007-of-00009",
                "00008-of-00009",
                "00009-of-00009"
              ],
              "Q5_K_M": [
                "00001-of-00010",
                "00002-of-00010",
                "00003-of-00010",
                "00004-of-00010",
                "00005-of-00010",
                "00006-of-00010",
                "00007-of-00010",
                "00008-of-00010",
                "00009-of-00010",
                "00010-of-00010"
              ],
              "Q6_K": [
                "00001-of-00012",
                "00002-of-00012",
                "00003-of-00012",
                "00004-of-00012",
                "00005-of-00012",
                "00006-of-00012",
                "00007-of-00012",
                "00008-of-00012",
                "00009-of-00012",
                "00010-of-00012",
                "00011-of-00012",
                "00012-of-00012"
              ],
              "Q8_0": [
                "00001-of-00016",
                "00002-of-00016",
                "00003-of-00016",
                "00004-of-00016",
                "00005-of-00016",
                "00006-of-00016",
                "00007-of-00016",
                "00008-of-00016",
                "00009-of-00016",
                "00010-of-00016",
                "00011-of-00016",
                "00012-of-00016",
                "00013-of-00016",
                "00014-of-00016",
                "00015-of-00016",
                "00016-of-00016"
              ]
            }
          },
          "modelscope": {
            "quantizations": [
              "Q2_K_L",
              "Q2_K_XS",
              "Q3_K_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "unsloth/DeepSeek-V3-GGUF",
            "model_file_name_template": "DeepSeek-V3-{quantization}/DeepSeek-V3-{quantization}.gguf",
            "model_file_name_split_template": "DeepSeek-V3-{quantization}/DeepSeek-V3-{quantization}-{part}.gguf",
            "quantization_parts": {
              "Q2_K_L": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q2_K_XS": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q3_K_M": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "Q4_K_M": [
                "00001-of-00009",
                "00002-of-00009",
                "00003-of-00009",
                "00004-of-00009",
                "00005-of-00009",
                "00006-of-00009",
                "00007-of-00009",
                "00008-of-00009",
                "00009-of-00009"
              ],
              "Q5_K_M": [
                "00001-of-00010",
                "00002-of-00010",
                "00003-of-00010",
                "00004-of-00010",
                "00005-of-00010",
                "00006-of-00010",
                "00007-of-00010",
                "00008-of-00010",
                "00009-of-00010",
                "00010-of-00010"
              ],
              "Q6_K": [
                "00001-of-00012",
                "00002-of-00012",
                "00003-of-00012",
                "00004-of-00012",
                "00005-of-00012",
                "00006-of-00012",
                "00007-of-00012",
                "00008-of-00012",
                "00009-of-00012",
                "00010-of-00012",
                "00011-of-00012",
                "00012-of-00012"
              ],
              "Q8_0": [
                "00001-of-00016",
                "00002-of-00016",
                "00003-of-00016",
                "00004-of-00016",
                "00005-of-00016",
                "00006-of-00016",
                "00007-of-00016",
                "00008-of-00016",
                "00009-of-00016",
                "00010-of-00016",
                "00011-of-00016",
                "00012-of-00016",
                "00013-of-00016",
                "00014-of-00016",
                "00015-of-00016",
                "00016-of-00016"
              ]
            }
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit"
            ],
            "model_id": "mlx-community/DeepSeek-V3-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit"
            ],
            "model_id": "mlx-community/DeepSeek-V3-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{% if messages %} {% if system or tools %} {% if system %} {{ system }} {% endif %} {% if tools %} {# Handle tools here if needed #} {% endif %} {% endif %} {% for message in messages %} {% set last = loop.index == loop.length %} {% if message.role == \"user\" %} <｜User｜> {% if tools and last %} Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.  Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. Do not use variables.  {{ tools }} {% endif %} {{ message.content }} {% if last %} <｜Assistant｜> {% endif %} {% elif message.role == \"assistant\" %} <｜Assistant｜> {% if message.tool_calls %} <｜tool▁calls▁begin｜> {% for tool in message.tool_calls %} <｜tool▁call▁begin｜> {\"name\": \"{{ tool.function.name }}\", \"parameters\": {{ tool.function.arguments }}} <｜tool▁call▁end｜> {% endfor %} <｜tool▁calls▁end｜> {% else %} {{ message.content }} {% if not last %} <｜end▁of▁sentence｜> {% endif %} {% endif %} {% elif message.role == \"tool\" %} <｜tool▁outputs▁begin｜> <｜tool▁output▁begin｜> {{ message.content }} <｜tool▁output▁end｜> <｜tool▁outputs▁end｜> {% if last and message.role != \"assistant\" %} <｜Assistant｜> {% endif %} {% endif %} {% endfor %} {% else %} {% if system %} {{ system }} {% endif %} {% if prompt %} <｜User｜> {{ prompt }} {% endif %} <｜Assistant｜> {{ response }} {% if response %} {{ response }} {% endif %} {% endif %}",
    "stop_token_ids": [
      1
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "tool_parser": "deepseek_v3",
    "updated_at": 1769418520,
    "featured": false,
    "architectures": [
      "DeepseekV3ForCausalLM"
    ],
    "model_type": "deepseek_v3",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 163840,
    "model_name": "deepseek-v3-0324",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-V3-0324"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-V3-0324"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "cognitivecomputations/DeepSeek-V3-0324-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "cognitivecomputations/DeepSeek-V3-0324-AWQ"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "5bit",
              "8bit"
            ],
            "model_id": "mlx-community/DeepSeek-V3-0324-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\n\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{'<｜User｜>' + message['content'] + '<｜Assistant｜>'}}{%- endif %}{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{%- endif %}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- set ns.is_output_first = true %}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<｜tool▁call▁end｜>'}}{%- else %}{{message['content'] + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- endfor %}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none)%}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{{content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_last_user and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}",
    "stop_token_ids": [
      1
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "updated_at": 1769418521,
    "featured": false,
    "architectures": [
      "DeepseekV3ForCausalLM"
    ],
    "model_type": "deepseek_v3",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "deepseek-vl2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 27,
        "activated_size_in_billions": "4_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-vl2"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-vl2"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 16,
        "activated_size_in_billions": "2_8",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-vl2-small"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-vl2-small"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 3,
        "activated_size_in_billions": 1,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-vl2-tiny"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/deepseek-vl2-tiny"
          }
        }
      }
    ],
    "chat_template": "",
    "stop_token_ids": [
      1
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "updated_at": 1769418522,
    "featured": false,
    "architectures": [
      "DeepseekV2ForCausalLM"
    ],
    "model_type": "deepseek_vl_v2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "fin-r1",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Fin-R1 is a large language model specifically designed for the field of financial reasoning",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "SUFE-AIFLM-Lab/Fin-R1"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-ModelScope/Fin-R1"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "JunHowie/Fin-R1-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "JunHowie/Fin-R1-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "FP8"
            ],
            "model_id": "JunHowie/Fin-R1-FP8-Dynamic"
          },
          "modelscope": {
            "quantizations": [
              "FP8"
            ],
            "model_id": "JunHowie/Fin-R1-FP8-Dynamic"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "updated_at": 1769418523,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "gemma-3-1b-it",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 1,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "google/gemma-3-1b-it"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/gemma-3-1b-it"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 1,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "fp16"
            ],
            "model_id": "mlx-community/gemma-3-1b-it-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "fp16"
            ],
            "model_id": "mlx-community/gemma-3-1b-it-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 1,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "IQ2_M",
              "IQ3_M",
              "IQ3_XS",
              "IQ3_XXS",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0",
              "bf16"
            ],
            "model_id": "bartowski/google_gemma-3-1b-it-GGUF",
            "model_file_name_template": "google_gemma-3-1b-it-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0",
              "bf16"
            ],
            "model_id": "bartowski/google_gemma-3-1b-it-GGUF",
            "model_file_name_template": "google_gemma-3-1b-it-{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{{ bos_token }}\n{%- if messages[0]['role'] == 'system' -%}\n    {%- if messages[0]['content'] is string -%}\n        {%- set first_user_prefix = messages[0]['content'] + '\n\n' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n\n' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message['role'] == 'assistant') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message['role'] -%}\n    {%- endif -%}\n    {{ '<start_of_turn>' + role + '\n' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message['content'] is string -%}\n        {{ message['content'] | trim }}\n    {%- elif message['content'] is iterable -%}\n        {%- for item in message['content'] -%}\n            {%- if item['type'] == 'image' -%}\n                {{ '<start_of_image>' }}\n            {%- elif item['type'] == 'text' -%}\n                {{ item['text'] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ '<end_of_turn>\n' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{'<start_of_turn>model\n'}}\n{%- endif -%}\n",
    "stop_token_ids": [
      1,
      105,
      106
    ],
    "stop": [
      "<eos>",
      "<end_of_turn>",
      "<start_of_turn>"
    ],
    "updated_at": 1769418524,
    "featured": false,
    "architectures": [
      "Gemma3ForCausalLM"
    ],
    "model_type": "gemma3_text",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "gemma-3-it",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "google/gemma-3-4b-it"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/gemma-3-4b-it"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "google/gemma-3-12b-it"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/gemma-3-12b-it"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 27,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "google/gemma-3-27b-it"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/gemma-3-27b-it"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "fp16"
            ],
            "model_id": "mlx-community/gemma-3-4b-it-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "fp16"
            ],
            "model_id": "mlx-community/gemma-3-4b-it-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "fp16"
            ],
            "model_id": "mlx-community/gemma-3-12b-it-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "fp16"
            ],
            "model_id": "mlx-community/gemma-3-12b-it-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 27,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "fp16"
            ],
            "model_id": "mlx-community/gemma-3-27b-it-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "fp16"
            ],
            "model_id": "mlx-community/gemma-3-27b-it-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "IQ2_M",
              "IQ3_M",
              "IQ3_XS",
              "IQ3_XXS",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0",
              "bf16"
            ],
            "multimodal_projectors": [
              "mmproj-google_gemma-3-4b-it-f16.gguf",
              "mmproj-google_gemma-3-4b-it-f32.gguf",
              "mmproj-google_gemma-3-4b-it-bf16.gguf"
            ],
            "model_id": "bartowski/google_gemma-3-4b-it-GGUF",
            "model_file_name_template": "google_gemma-3-4b-it-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0",
              "bf16"
            ],
            "multimodal_projectors": [
              "mmproj-google_gemma-3-4b-it-f16.gguf",
              "mmproj-google_gemma-3-4b-it-f32.gguf",
              "mmproj-google_gemma-3-4b-it-bf16.gguf"
            ],
            "model_id": "bartowski/google_gemma-3-4b-it-GGUF",
            "model_file_name_template": "google_gemma-3-4b-it-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "IQ2_M",
              "IQ3_M",
              "IQ3_XS",
              "IQ3_XXS",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0",
              "bf16"
            ],
            "multimodal_projectors": [
              "mmproj-google_gemma-3-12b-it-f16.gguf",
              "mmproj-google_gemma-3-12b-it-f32.gguf",
              "mmproj-google_gemma-3-12b-it-bf16.gguf"
            ],
            "model_id": "bartowski/google_gemma-3-12b-it-GGUF",
            "model_file_name_template": "google_gemma-3-12b-it-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0",
              "bf16"
            ],
            "multimodal_projectors": [
              "mmproj-google_gemma-3-12b-it-f16.gguf",
              "mmproj-google_gemma-3-12b-it-f32.gguf",
              "mmproj-google_gemma-3-12b-it-bf16.gguf"
            ],
            "model_id": "bartowski/google_gemma-3-12b-it-GGUF",
            "model_file_name_template": "google_gemma-3-12b-it-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 27,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "IQ2_M",
              "IQ3_M",
              "IQ3_XS",
              "IQ3_XXS",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0",
              "bf16"
            ],
            "multimodal_projectors": [
              "mmproj-google_gemma-3-27b-it-f16.gguf",
              "mmproj-google_gemma-3-27b-it-f32.gguf",
              "mmproj-google_gemma-3-27b-it-bf16.gguf"
            ],
            "model_id": "bartowski/google_gemma-3-27b-it-GGUF",
            "model_file_name_template": "google_gemma-3-27b-it-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0",
              "bf16"
            ],
            "multimodal_projectors": [
              "mmproj-google_gemma-3-27b-it-f16.gguf",
              "mmproj-google_gemma-3-27b-it-f32.gguf",
              "mmproj-google_gemma-3-27b-it-bf16.gguf"
            ],
            "model_id": "bartowski/google_gemma-3-27b-it-GGUF",
            "model_file_name_template": "google_gemma-3-27b-it-{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{{ bos_token }}\n{%- if messages[0]['role'] == 'system' -%}\n    {%- if messages[0]['content'] is string -%}\n        {%- set first_user_prefix = messages[0]['content'] + '\n\n' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n\n' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message['role'] == 'assistant') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message['role'] -%}\n    {%- endif -%}\n    {{ '<start_of_turn>' + role + '\n' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message['content'] is string -%}\n        {{ message['content'] | trim }}\n    {%- elif message['content'] is iterable -%}\n        {%- for item in message['content'] -%}\n            {%- if item['type'] == 'image' -%}\n                {{ '<start_of_image>' }}\n            {%- elif item['type'] == 'text' -%}\n                {{ item['text'] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ '<end_of_turn>\n' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{'<start_of_turn>model\n'}}\n{%- endif -%}\n",
    "stop_token_ids": [
      1,
      105,
      106
    ],
    "stop": [
      "<eos>",
      "<end_of_turn>",
      "<start_of_turn>"
    ],
    "updated_at": 1769418525,
    "featured": true,
    "architectures": [
      "Gemma3ForConditionalGeneration"
    ],
    "model_type": "gemma3",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 8192,
    "model_name": "glm-4v",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "GLM4 is the open source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/glm-4v-9b",
            "model_revision": "01328faefe122fe605c1c127b62e6031d3ffebf7"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/glm-4v-9b",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-Research/glm-4v-9b"
          }
        }
      }
    ],
    "chat_template": "",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ],
    "updated_at": 1769418526,
    "featured": false,
    "architectures": [
      "ChatGLMModel"
    ],
    "model_type": "chatglm",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 8192,
    "model_name": "glm-edge-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The GLM-Edge series is our attempt to face the end-side real-life scenarios, which consists of two sizes of large-language dialogue models and multimodal comprehension models (GLM-Edge-1.5B-Chat, GLM-Edge-4B-Chat, GLM-Edge-V-2B, GLM-Edge-V-5B). Among them, the 1.5B / 2B model is mainly for platforms such as mobile phones and cars, and the 4B / 5B model is mainly for platforms such as PCs.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/glm-edge-1.5b-chat"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/glm-edge-1.5b-chat"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "4",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/glm-edge-4b-chat"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/glm-edge-4b-chat"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q4_0",
              "Q4_1",
              "Q4_K",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_1",
              "Q5_K",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_file_name_template": "ggml-model-{quantization}.gguf",
            "model_id": "zai-org/glm-edge-1.5b-chat-gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q4_0",
              "Q4_1",
              "Q4_K",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_1",
              "Q5_K",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_file_name_template": "ggml-model-{quantization}.gguf",
            "model_id": "ZhipuAI/glm-edge-1.5b-chat-gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "F16"
            ],
            "model_file_name_template": "glm-edge-1.5B-chat-{quantization}.gguf",
            "model_id": "zai-org/glm-edge-1.5b-chat-gguf"
          },
          "modelscope": {
            "quantizations": [
              "F16"
            ],
            "model_file_name_template": "glm-edge-1.5B-chat-{quantization}.gguf",
            "model_id": "ZhipuAI/glm-edge-1.5b-chat-gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "4",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q4_0",
              "Q4_1",
              "Q4_K",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_1",
              "Q5_K",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_file_name_template": "ggml-model-{quantization}.gguf",
            "model_id": "zai-org/glm-edge-4b-chat-gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q4_0",
              "Q4_1",
              "Q4_K",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_1",
              "Q5_K",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_file_name_template": "ggml-model-{quantization}.gguf",
            "model_id": "ZhipuAI/glm-edge-4b-chat-gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "4",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "F16"
            ],
            "model_file_name_template": "glm-edge-4B-chat-{quantization}.gguf",
            "model_id": "zai-org/glm-edge-4b-chat-gguf"
          },
          "modelscope": {
            "quantizations": [
              "F16"
            ],
            "model_file_name_template": "glm-edge-4B-chat-{quantization}.gguf",
            "model_id": "ZhipuAI/glm-edge-4b-chat-gguf"
          }
        }
      }
    ],
    "chat_template": "{% for item in messages %}{% if item['role'] == 'system' %}<|system|>\n{{ item['content'] }}{% elif item['role'] == 'user' %}<|user|>\n{{ item['content'] }}{% elif item['role'] == 'assistant' %}<|assistant|>\n{{ item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>\n{% endif %}",
    "stop_token_ids": [
      59246,
      59253,
      59255
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ],
    "updated_at": 1769418527,
    "featured": false,
    "architectures": [
      "GlmForCausalLM"
    ],
    "model_type": "glm",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "glm4-0414",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "The GLM family welcomes new members, the GLM-4-32B-0414 series models, featuring 32 billion parameters. Its performance is comparable to OpenAI’s GPT series and DeepSeek’s V3/R1 series",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/GLM-4-9B-0414"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/GLM-4-9B-0414"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/GLM-4-32B-0414"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/GLM-4-32B-0414"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/GLM-4-9B-0414-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/GLM-4-9B-0414-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/GLM-4-32B-0414-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/GLM-4-32B-0414-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "IQ2_M",
              "IQ3_M",
              "IQ3_XS",
              "IQ3_XXS",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q3_K_XL",
              "Q4_0",
              "Q4_1",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0",
              "bf16"
            ],
            "model_id": "bartowski/THUDM_GLM-4-9B-0414-GGUF",
            "model_file_name_template": "THUDM_GLM-4-9B-0414-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "IQ2_M",
              "IQ3_M",
              "IQ3_XS",
              "IQ3_XXS",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q3_K_XL",
              "Q4_0",
              "Q4_1",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0",
              "bf16"
            ],
            "model_id": "bartowski/THUDM_GLM-4-9B-0414-GGUF",
            "model_file_name_template": "THUDM_GLM-4-9B-0414-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "IQ2_M",
              "IQ2_S",
              "IQ2_XS",
              "IQ3_M",
              "IQ3_XS",
              "IQ3_XXS",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q3_K_XL",
              "Q4_0",
              "Q4_1",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0"
            ],
            "model_id": "bartowski/THUDM_GLM-4-9B-0414-GGUF",
            "model_file_name_template": "THUDM_GLM-4-9B-0414-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "IQ2_M",
              "IQ2_S",
              "IQ2_XS",
              "IQ3_M",
              "IQ3_XS",
              "IQ3_XXS",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q3_K_XL",
              "Q4_0",
              "Q4_1",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0"
            ],
            "model_id": "bartowski/THUDM_GLM-4-9B-0414-GGUF",
            "model_file_name_template": "THUDM_GLM-4-9B-0414-{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "[gMASK]<sop>{%- if tools -%}<|system|>\n# 可用工具\n{% for tool in tools %}{%- set function = tool.function if tool.get(\"function\") else tool %}\n\n## {{ function.name }}\n\n{{ function | tojson(indent=4, ensure_ascii=False) }}\n在调用上述函数时，请使用 Json 格式表示调用的参数。{%- endfor %}{%- endif -%}{%- for msg in messages %}{%- if msg.role == 'system' %}<|system|>\n{{ msg.content }}{%- endif %}{%- endfor %}{%- for message in messages if message.role != 'system' %}{%- set role = message['role'] %}{%- set content = message['content'] %}{%- set meta = message.get(\"metadata\", \"\") %}{%- if role == 'user' %}<|user|>\n{{ content }}{%- elif role == 'assistant' and not meta %}<|assistant|>\n{{ content }}{%- elif role == 'assistant' and meta %}<|assistant|>{{ meta }} \n{{ content }}{%- elif role == 'observation' %}<|observation|>\n{{ content }}{%- endif %}{%- endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ],
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    },
    "updated_at": 1769418528,
    "featured": false,
    "architectures": [
      "Glm4ForCausalLM"
    ],
    "model_type": "glm4"
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "glm4-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "GLM4 is the open source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/glm-4-9b-chat-hf",
            "model_revision": "c7f73fd9e0f378c87f3c8f2c25aec6ad705043cd"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/glm-4-9b-chat-hf",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-Research/glm-4-9b-chat"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "IQ3_XS",
              "IQ3_S",
              "IQ3_M",
              "Q3_K_S",
              "Q3_K_L",
              "Q3_K",
              "IQ4_XS",
              "IQ4_NL",
              "Q4_K_S",
              "Q4_K",
              "Q5_K_S",
              "Q5_K",
              "Q6_K",
              "Q8_0",
              "BF16",
              "FP16"
            ],
            "model_file_name_template": "glm-4-9b-chat.{quantization}.gguf",
            "model_id": "legraphista/glm-4-9b-chat-GGUF",
            "model_revision": "0155a14edf0176863e9a003cdd78ce599e4d62c0"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "IQ3_XS",
              "IQ3_S",
              "IQ3_M",
              "Q3_K_S",
              "Q3_K_L",
              "Q3_K",
              "IQ4_XS",
              "IQ4_NL",
              "Q4_K_S",
              "Q4_K",
              "Q5_K_S",
              "Q5_K",
              "Q6_K",
              "Q8_0",
              "BF16",
              "FP16"
            ],
            "model_file_name_template": "glm-4-9b-chat.{quantization}.gguf",
            "model_id": "LLM-Research/glm-4-9b-chat-GGUF",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "[gMASK]<sop>{% for item in messages %}{% if item['tools'] is defined %}<|system|>\n你是一个名为 ChatGLM 的人工智能助手。你是基于智谱AI训练的语言模型 GLM-4 模型开发的，你的任务是针对用户的问题和要求提供适当的答复和支持。\n\n# 可用工具{% set tools = item['tools'] %}{% for tool in tools %}{% if tool['type'] == 'function' %}\n\n## {{ tool['function']['name'] }}\n\n{{ tool['function'] | tojson(indent=4) }}\n在调用上述函数时，请使用 Json 格式表示调用的参数。{% elif tool['type'] == 'python' %}\n\n## python\n\n当你向 `python` 发送包含 Python 代码的消息时，该代码将会在一个有状态的 Jupyter notebook 环境中执行。\n`python` 返回代码执行的输出，或在执行 60 秒后返回超时。\n`/mnt/data` 将会持久化存储你的文件。在此会话中，`python` 无法访问互联网。不要使用 `python` 进行任何网络请求或者在线 API 调用，这些在线内容的访问将不会成功。{% elif tool['type'] == 'simple_browser' %}\n\n## simple_browser\n\n你可以使用 `simple_browser` 工具。该工具支持以下函数：\n`search(query: str, recency_days: int)`：使用搜索引擎进行查询并显示结果，可以使用 `recency_days` 参数控制搜索内容的时效性。\n`mclick(ids: list[int])`：获取一系列指定 id 的页面内容。每次调用时，须选择3-10个页面。选择多个角度的页面，同时尽可能选择可信任的信息来源。考虑到部分页面是无法加载的，你也可以多打开一些可能有用的页面而不用担心内容过多。\n`open_url(url: str)`：打开指定的 URL。\n\n使用 `【{引用 id}†{引用文本}】` 来引用内容。\n\n操作步骤：1. 使用 `search` 来获得信息列表; 2. 使用 `mclick` 来获取指定 ID 页面的内容; 3. 根据获得的内容进行回复。在回复中应当引用信息来源。\n 如果用户提供了 URL，也可以用 `open_url` 直接打开页面。\n如果初次搜索结果没有找到合适的信息，也可以再次使用 `search` 进行搜索。{% elif tool['type'] == 'cogview' %}\n\n## cogview\n\n如果用户的请求中包含了对图像的描述，你可以使用 `cogview` 来生成图像并展示给用户。你需要向 `cogview` 发送图像描述，规则：\n- 发送给 `cogview` 的消息必须使用英语。用户的中文描述必须完全翻译为英语。\n- 应当尽可能详细地描述图像生成的需求，需求描述约 100 英文单词。\n- 保持用户原始描述的意图。不要虚构内容或者没见过的人物。\n- 如无特殊说明，所在地为中国，持有中国立场并遵循中国社会主义价值观。{% endif %}{% endfor %}{% endif %}{% if item['content'] %}<|{{ item['role'] }}|>{{ item['metadata'] }}\n{{ item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ],
    "tool_parser": "glm4",
    "updated_at": 1769418529,
    "featured": false,
    "architectures": [
      "GlmForCausalLM"
    ],
    "model_type": "glm",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 1048576,
    "model_name": "glm4-chat-1m",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "GLM4 is the open source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/glm-4-9b-chat-1m-hf",
            "model_revision": "0588cb62942f0f0a5545c695e5c1b019d64eabdc"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/glm-4-9b-chat-1m-hf",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-Research/glm-4-9b-chat-1m"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "IQ3_XS",
              "IQ3_S",
              "IQ3_M",
              "Q3_K_S",
              "Q3_K_L",
              "Q3_K",
              "IQ4_XS",
              "IQ4_NL",
              "Q4_K_S",
              "Q4_K",
              "Q5_K_S",
              "Q5_K",
              "Q6_K",
              "Q8_0",
              "BF16",
              "FP16"
            ],
            "model_file_name_template": "glm-4-9b-chat-1m.{quantization}.gguf",
            "model_id": "legraphista/glm-4-9b-chat-1m-GGUF",
            "model_revision": "782e28bd5eee3c514c07108da15e0b5e06dcf776"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "IQ3_XS",
              "IQ3_S",
              "IQ3_M",
              "Q3_K_S",
              "Q3_K_L",
              "Q3_K",
              "IQ4_XS",
              "IQ4_NL",
              "Q4_K_S",
              "Q4_K",
              "Q5_K_S",
              "Q5_K",
              "Q6_K",
              "Q8_0",
              "BF16",
              "FP16"
            ],
            "model_file_name_template": "glm-4-9b-chat-1m.{quantization}.gguf",
            "model_id": "LLM-Research/glm-4-9b-chat-1m-GGUF",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "[gMASK]<sop>{% for item in messages %}{% if item['tools'] is defined %}<|system|>\n你是一个名为 GLM-4 的人工智能助手。你是基于智谱AI训练的语言模型 GLM-4 模型开发的，你的任务是针对用户的问题和要求提供适当的答复和支持。\n\n# 可用工具{% set tools = item['tools'] %}{% for tool in tools %}{% if tool['type'] == 'function' %}\n\n## {{ tool['function']['name'] }}\n\n{{ tool['function'] | tojson(indent=4) }}\n在调用上述函数时，请使用 Json 格式表示调用的参数。{% elif tool['type'] == 'python' %}\n\n## python\n\n当你向 `python` 发送包含 Python 代码的消息时，该代码将会在一个有状态的 Jupyter notebook 环境中执行。\n`python` 返回代码执行的输出，或在执行 60 秒后返回超时。\n`/mnt/data` 将会持久化存储你的文件。在此会话中，`python` 无法访问互联网。不要使用 `python` 进行任何网络请求或者在线 API 调用，这些在线内容的访问将不会成功。{% elif tool['type'] == 'simple_browser' %}\n\n## simple_browser\n\n你可以使用 `simple_browser` 工具。该工具支持以下函数：\n`search(query: str, recency_days: int)`：使用搜索引擎进行查询并显示结果，可以使用 `recency_days` 参数控制搜索内容的时效性。\n`mclick(ids: list[int])`：获取一系列指定 id 的页面内容。每次调用时，须选择3-10个页面。选择多个角度的页面，同时尽可能选择可信任的信息来源。考虑到部分页面是无法加载的，你也可以多打开一些可能有用的页面而不用担心内容过多。\n`open_url(url: str)`：打开指定的 URL。\n\n使用 `【{引用 id}†{引用文本}】` 来引用内容。\n\n操作步骤：1. 使用 `search` 来获得信息列表; 2. 使用 `mclick` 来获取指定 ID 页面的内容; 3. 根据获得的内容进行回复。在回复中应当引用信息来源。\n 如果用户提供了 URL，也可以用 `open_url` 直接打开页面。\n如果初次搜索结果没有找到合适的信息，也可以再次使用 `search` 进行搜索。{% elif tool['type'] == 'cogview' %}\n\n## cogview\n\n如果用户的请求中包含了对图像的描述，你可以使用 `cogview` 来生成图像并展示给用户。你需要向 `cogview` 发送图像描述，规则：\n- 发送给 `cogview` 的消息必须使用英语。用户的中文描述必须完全翻译为英语。\n- 应当尽可能详细地描述图像生成的需求，需求描述约 100 英文单词。\n- 保持用户原始描述的意图。不要虚构内容或者没见过的人物。\n- 如无特殊说明，所在地为中国，持有中国立场并遵循中国社会主义价值观。{% endif %}{% endfor %}{% endif %}{% if item['content'] %}<|{{ item['role'] }}|>{{ item['metadata'] }}\n{{ item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ],
    "tool_parser": "glm4",
    "updated_at": 1769418530,
    "featured": false,
    "architectures": [
      "GlmForCausalLM"
    ],
    "model_type": "glm",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "internlm3-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "InternLM3 has open-sourced an 8-billion parameter instruction model, InternLM3-8B-Instruct, designed for general-purpose usage and advanced reasoning.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "internlm/internlm3-8b-instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Shanghai_AI_Laboratory/internlm3-8b-instruct"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "internlm/internlm3-8b-instruct-gptq-int4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Shanghai_AI_Laboratory/internlm3-8b-instruct-gptq-int4"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "internlm/internlm3-8b-instruct-awq"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Shanghai_AI_Laboratory/internlm3-8b-instruct-awq"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "internlm/internlm3-8b-instruct-gguf",
            "model_file_name_template": "internlm3-8b-instruct-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Shanghai_AI_Laboratory/internlm3-8b-instruct-gguf",
            "model_file_name_template": "internlm3-8b-instruct-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/internlm3-8b-instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/internlm3-8b-instruct-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{{ bos_token }}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      2,
      128131
    ],
    "stop": [
      "</s>",
      "<|im_end|>"
    ],
    "updated_at": 1769418531,
    "featured": false,
    "architectures": [
      "InternLM3ForCausalLM"
    ],
    "model_type": "internlm3",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "llama-2-chat",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Llama-2-Chat is a fine-tuned version of the Llama-2 LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Llama-2-7b-chat-hf",
            "model_revision": "08751db2aca9bf2f7f80d2e516117a53d7450235"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "modelscope/Llama-2-7b-chat-ms",
            "model_revision": "v1.0.5"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Llama-2-13b-chat-hf",
            "model_revision": "0ba94ac9b9e1d5a0037780667e8b219adde1908c"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "modelscope/Llama-2-13b-chat-ms",
            "model_revision": "v1.0.2"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Llama-2-70b-chat-hf",
            "model_revision": "36d9a7388cc80e5f4b3e9701ca2f250d21a96c30"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "modelscope/Llama-2-70b-chat-ms",
            "model_revision": "v1.0.1"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_0",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_0",
              "Q5_K_S",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/Llama-2-7B-Chat-GGUF",
            "model_file_name_template": "llama-2-7b-chat.{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q4_K_M"
            ],
            "model_id": "Xorbits/Llama-2-7b-Chat-GGUF",
            "model_file_name_template": "llama-2-7b-chat.{quantization}.gguf",
            "model_revision": "v0.0.1"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_0",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_0",
              "Q5_K_S",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/Llama-2-13B-chat-GGUF",
            "model_file_name_template": "llama-2-13b-chat.{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q4_K_M"
            ],
            "model_id": "Xorbits/Llama-2-13b-Chat-GGUF",
            "model_file_name_template": "llama-2-13b-chat.{quantization}.gguf",
            "model_revision": "v0.0.1"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_0",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_0",
              "Q5_K_S",
              "Q5_K_M"
            ],
            "quantization_parts": {
              "Q6_K": [
                "split-a",
                "split-b"
              ],
              "Q8_0": [
                "split-a",
                "split-b"
              ]
            },
            "model_id": "TheBloke/Llama-2-70B-Chat-GGUF",
            "model_file_name_template": "llama-2-70b-chat.{quantization}.gguf",
            "model_file_name_split_template": "llama-2-70b-chat.{quantization}.gguf-{part}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Llama-2-7B-Chat-GPTQ"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Llama-2-70B-Chat-GPTQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Llama-2-70B-Chat-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Llama-2-7B-Chat-AWQ"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Llama-2-13B-chat-GPTQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Llama-2-13B-chat-AWQ"
          }
        }
      }
    ],
    "chat_template": "{% if messages[0]['role'] == 'system' %}{% set system_message = '<<SYS>>\n' + messages[0]['content'] | trim + '\n<</SYS>>\n\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ '<s>' + '[INST] ' + content | trim + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + content | trim + ' ' + '</s>' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      2
    ],
    "stop": [],
    "featured": false,
    "updated_at": 1769418532,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 8192,
    "model_name": "llama-3",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Meta-Llama-3-8B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Meta-Llama-3-8B"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_1",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "QuantFactory/Meta-Llama-3-8B-GGUF",
            "model_file_name_template": "Meta-Llama-3-8B.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Meta-Llama-3-70B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Meta-Llama-3-70B"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q4_K_M",
              "Q5_K_M"
            ],
            "model_id": "NousResearch/Meta-Llama-3-70B-GGUF",
            "model_file_name_template": "Meta-Llama-3-70B-{quantization}.gguf"
          }
        }
      }
    ],
    "updated_at": 1769418533,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 8192,
    "model_name": "llama-3-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks..",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Meta-Llama-3-8B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Meta-Llama-3-8B-Instruct"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "wuhaicc/Meta-Llama-3-8B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Meta-Llama-3-70B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Meta-Llama-3-70B-Instruct"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "wuhaicc/Meta-Llama-3-70B-Instruct"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "IQ3_M",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF",
            "model_file_name_template": "Meta-Llama-3-8B-Instruct-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "IQ1_M",
              "IQ2_XS",
              "Q4_K_M"
            ],
            "model_id": "lmstudio-community/Meta-Llama-3-70B-Instruct-GGUF",
            "model_file_name_template": "Meta-Llama-3-70B-Instruct-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Meta-Llama-3-8B-Instruct-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Meta-Llama-3-8B-Instruct-8bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mlx-community/Meta-Llama-3-8B-Instruct"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Meta-Llama-3-70B-Instruct-4bit-mlx"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Meta-Llama-3-70B-Instruct-8bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mlx-community/Meta-Llama-3-70B-Instruct-mlx-unquantized"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TechxGenus/Meta-Llama-3-8B-Instruct-GPTQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "swift/Meta-Llama-3-8B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TechxGenus/Meta-Llama-3-70B-Instruct-GPTQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "swift/Meta-Llama-3-70B-Instruct-GPTQ-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = '<|begin_of_text|>' + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}",
    "stop_token_ids": [
      128001,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>"
    ],
    "updated_at": 1769418534,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "llama-3.1",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Meta-Llama-3.1-8B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Meta-Llama-3.1-8B"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-Research/Meta-Llama-3.1-8B"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_1",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "QuantFactory/Meta-Llama-3.1-8B-GGUF",
            "model_file_name_template": "Meta-Llama-3.1-8B.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Meta-Llama-3.1-70B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Meta-Llama-3.1-70B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 405,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Meta-Llama-3.1-405B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Meta-Llama-3.1-405B"
          }
        }
      }
    ],
    "updated_at": 1769418535,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "llama-3.1-instruct",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "The Llama 3.1 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks..",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-Research/Meta-Llama-3.1-8B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct-AWQ-INT4"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Meta-Llama-3.1-70B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Meta-Llama-3.1-70B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "unsloth/Meta-Llama-3.1-70B-Instruct-bnb-4bit"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Meta-Llama-3.1-70B-Instruct"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "hugging-quants/Meta-Llama-3.1-70B-Instruct-GPTQ-INT4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "LLM-Research/Meta-Llama-3.1-70B-Instruct-GPTQ-INT4"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "LLM-Research/Meta-Llama-3.1-70B-Instruct-AWQ-INT4"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 405,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Meta-Llama-3.1-405B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Meta-Llama-3.1-405B-Instruct"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 405,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "hugging-quants/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "LLM-Research/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 405,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "LLM-Research/Meta-Llama-3.1-405B-Instruct-AWQ-INT4"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q3_K_L",
              "IQ4_XS",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF",
            "model_file_name_template": "Meta-Llama-3.1-8B-Instruct-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q3_K_L",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct-GGUF",
            "model_file_name_template": "Meta-Llama-3.1-8B-Instruct-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "IQ2_M",
              "IQ4_XS",
              "Q2_K",
              "Q3_K_S",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "quantization_parts": {
              "Q5_K_M": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q6_K": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q8_0": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "lmstudio-community/Meta-Llama-3.1-70B-Instruct-GGUF",
            "model_file_name_template": "Meta-Llama-3.1-70B-Instruct-{quantization}.gguf",
            "model_file_name_split_template": "Meta-Llama-3.1-70B-Instruct-{quantization}-{part}.gguf"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Meta-Llama-3.1-8B-Instruct-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Meta-Llama-3.1-8B-Instruct-8bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mlx-community/Meta-Llama-3.1-8B-Instruct"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Meta-Llama-3.1-70B-Instruct-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Meta-Llama-3.1-70B-Instruct-8bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mlx-community/Meta-Llama-3.1-70B-Instruct-bf16"
          }
        }
      }
    ],
    "chat_template": "{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\"' + arg_val + '\"' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n            {{- '\"parameters\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      128001,
      128008,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>",
      "<|eom_id|>"
    ],
    "tool_parser": "llama3",
    "updated_at": 1769418535,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "llama-3.2-vision",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "generate",
      "vision"
    ],
    "model_description": "The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image...",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 11,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Meta-Llama-3.2-11B-Vision"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Llama-3.2-11B-Vision"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 90,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Meta-Llama-3.2-90B-Vision"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Llama-3.2-90B-Vision"
          }
        }
      }
    ],
    "updated_at": 1769418536,
    "featured": false,
    "architectures": [
      "MllamaForConditionalGeneration"
    ],
    "model_type": "mllama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "llama-3.2-vision-instruct",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image...",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 11,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Llama-3.2-11B-Vision-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Llama-3.2-11B-Vision-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 90,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Llama-3.2-90B-Vision-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Llama-3.2-90B-Vision-Instruct"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n' }}{% if message['content'] is string %}{{ message['content'] }}{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' %}{{ '<|image|>' }}{% elif content['type'] == 'text' %}{{ content['text'] }}{% endif %}{% endfor %}{% endif %}{{ '<|eot_id|>' }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}",
    "stop_token_ids": [
      128001,
      128008,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>",
      "<|eom_id|>"
    ],
    "updated_at": 1769418537,
    "featured": false,
    "architectures": [
      "MllamaForConditionalGeneration"
    ],
    "model_type": "mllama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "llama-3.3-instruct",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "The Llama 3.3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks..",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "meta-llama/Llama-3.3-70B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Llama-3.3-70B-Instruct"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "shuyuej/Llama-3.3-70B-Instruct-GPTQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "casperhansen/llama-3.3-70b-instruct-awq"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "fp16"
            ],
            "model_id": "mlx-community/Llama-3.3-70B-Instruct-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q3_K_L",
              "Q4_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "quantization_parts": {
              "Q6_K": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q8_0": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "lmstudio-community/Llama-3.3-70B-Instruct-GGUF",
            "model_file_name_template": "Llama-3.3-70B-Instruct-{quantization}.gguf",
            "model_file_name_split_template": "Llama-3.3-70B-Instruct-{quantization}-{part}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q3_K_L",
              "Q4_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "quantization_parts": {
              "Q6_K": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q8_0": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "lmstudio-community/Llama-3.3-70B-Instruct-GGUF",
            "model_file_name_template": "Llama-3.3-70B-Instruct-{quantization}.gguf",
            "model_file_name_split_template": "Llama-3.3-70B-Instruct-{quantization}-{part}.gguf"
          }
        }
      }
    ],
    "chat_template": "{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\"' + arg_val + '\"' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n            {{- '\"parameters\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      128001,
      128008,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>",
      "<|eom_id|>"
    ],
    "updated_at": 1769418538,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "marco-o1",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "AIDC-AI/Marco-o1"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AIDC-AI/Marco-o1"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_1",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "QuantFactory/Marco-o1-GGUF",
            "model_file_name_template": "Marco-o1.{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_1",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_file_name_template": "Marco-o1.{quantization}.gguf",
            "model_id": "QuantFactory/Marco-o1-GGUF"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n\n你是一个经过良好训练的AI助手，你的名字是Marco-o1.由阿里国际数字商业集团的AI Business创造.\n        \n## 重要！！！！！\n当你回答问题时，你的思考应该在<Thought>内完成，<Output>内输出你的结果。\n<Thought>应该尽可能是英文，但是有2个特例，一个是对原文中的引用，另一个是是数学应该使用markdown格式，<Output>内的输出需要遵循用户输入的语言。\n        <|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "updated_at": 1769418539,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "minicpm-2b-dpo-bf16",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "openbmb/MiniCPM-2B-dpo-bf16",
            "model_revision": "f4a3ba49f3f18695945c2a7c12400d4da99da498"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenBMB/MiniCPM-2B-dpo-bf16",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-Research/MiniCPM-2B-dpo-bf16"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{'<用户>' + message['content'].strip() + '<AI>'}}{% else %}{{message['content'].strip()}}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ],
    "updated_at": 1769418540,
    "featured": false,
    "architectures": [
      "MiniCPMForCausalLM"
    ],
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "minicpm-2b-dpo-fp16",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "openbmb/MiniCPM-2B-dpo-fp16",
            "model_revision": "e7a50289e4f839674cf8d4a5a2ce032ccacf64ac"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenBMB/MiniCPM-2B-dpo-fp16",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{'<用户>' + message['content'].strip() + '<AI>'}}{% else %}{{message['content'].strip()}}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ],
    "updated_at": 1769418541,
    "featured": false,
    "architectures": [
      "MiniCPMForCausalLM"
    ],
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "minicpm-2b-dpo-fp32",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "openbmb/MiniCPM-2B-dpo-fp32",
            "model_revision": "b560a1593779b735a84a6daf72fba96ae38da288"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenBMB/MiniCPM-2B-dpo-fp32",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{'<用户>' + message['content'].strip() + '<AI>'}}{% else %}{{message['content'].strip()}}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ],
    "updated_at": 1769418542,
    "featured": false,
    "architectures": [
      "MiniCPMForCausalLM"
    ],
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "minicpm-2b-sft-bf16",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "openbmb/MiniCPM-2B-sft-bf16",
            "model_revision": "fe1d74027ebdd81cef5f815fa3a2d432a6b5de2a"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenBMB/miniCPM-bf16",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-Research/MiniCPM-2B-sft-bf16"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{'<用户>' + message['content'].strip() + '<AI>'}}{% else %}{{message['content'].strip()}}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ],
    "updated_at": 1769418543,
    "featured": false,
    "architectures": [
      "MiniCPMForCausalLM"
    ],
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "minicpm-2b-sft-fp32",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "openbmb/MiniCPM-2B-sft-fp32",
            "model_revision": "35b90dd57d977b6e5bc4907986fa5b77aa15a82e"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenBMB/MiniCPM-2B-sft-fp32",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{'<用户>' + message['content'].strip() + '<AI>'}}{% else %}{{message['content'].strip()}}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ],
    "updated_at": 1769418544,
    "featured": false,
    "architectures": [
      "MiniCPMForCausalLM"
    ],
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "minicpm3-4b",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM3-4B is the 3rd generation of MiniCPM series. The overall performance of MiniCPM3-4B surpasses Phi-3.5-mini-Instruct and GPT-3.5-Turbo-0125, being comparable with many recent 7B~9B models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "openbmb/MiniCPM3-4B",
            "model_revision": "75f9f1097d9d66d11f37fff49210bf940455f8ac"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenBMB/MiniCPM3-4B",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-Research/MiniCPM3-4B"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "openbmb/MiniCPM3-4B-GPTQ-Int4",
            "model_revision": "97a66a62f7d09c1ee35b087b42694716a8113dce"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OpenBMB/MiniCPM3-4B-GPTQ-Int4",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ],
    "updated_at": 1769418545,
    "featured": false,
    "architectures": [
      "MiniCPM3ForCausalLM"
    ],
    "model_type": "minicpm3",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "minicpm4",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM4 series are highly efficient large language models (LLMs) designed explicitly for end-side devices, which achieves this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "JunHowie/MiniCPM4-0.5B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "JunHowie/MiniCPM4-0.5B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "JunHowie/MiniCPM4-8B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "JunHowie/MiniCPM4-8B"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/MiniCPM4-8B-4bit"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/MiniCPM4-8B-4bit"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      2,
      73440
    ],
    "stop": [
      "</s>",
      "<|im_end|>"
    ],
    "updated_at": 1769418546,
    "featured": false,
    "architectures": [
      "MiniCPMForCausalLM"
    ],
    "model_type": "minicpm",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 8192,
    "model_name": "mistral-instruct-v0.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B LLM on public datasets, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mistralai/Mistral-7B-Instruct-v0.1",
            "model_revision": "54766df6d50e4d3d7ccd66758e5341ba105a6d36"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Xorbits/Mistral-7B-Instruct-v0.1",
            "model_revision": "v1.0.0"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Mistral-7B-Instruct-v0.1-AWQ"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Mistral-7B-Instruct-v0.1-GPTQ"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_0",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_0",
              "Q5_K_S",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/Mistral-7B-Instruct-v0.1-GGUF",
            "model_file_name_template": "mistral-7b-instruct-v0.1.{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n    {%- endif %}\n    {%- if message['role'] == 'user' %}\n        {%- if loop.first and system_message is defined %}\n            {{- ' [INST] ' + system_message + '\n\n' + message['content'] + ' [/INST]' }}\n        {%- else %}\n            {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n        {%- endif %}\n    {%- elif message['role'] == 'assistant' %}\n        {{- ' ' + message['content'] + '</s>'}}\n    {%- else %}\n        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ],
    "updated_at": 1769418547,
    "featured": false,
    "architectures": [
      "MistralForCausalLM"
    ],
    "model_type": "mistral",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 8192,
    "model_name": "mistral-instruct-v0.2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an improved instruct fine-tuned version of Mistral-7B-Instruct-v0.1.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mistralai/Mistral-7B-Instruct-v0.2",
            "model_revision": "b70aa86578567ba3301b21c8a27bea4e8f6d6d61"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-ModelScope/Mistral-7B-Instruct-v0.2"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Mistral-7B-Instruct-v0.2-GPTQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_0",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_0",
              "Q5_K_S",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
            "model_file_name_template": "mistral-7b-instruct-v0.2.{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q4_K_M"
            ],
            "model_id": "Xorbits/Mistral-7B-Instruct-v0.2-GGUF",
            "model_file_name_template": "mistral-7b-instruct-v0.2.{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n    {%- endif %}\n    {%- if message['role'] == 'user' %}\n        {%- if loop.first and system_message is defined %}\n            {{- ' [INST] ' + system_message + '\n\n' + message['content'] + ' [/INST]' }}\n        {%- else %}\n            {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n        {%- endif %}\n    {%- elif message['role'] == 'assistant' %}\n        {{- ' ' + message['content'] + '</s>'}}\n    {%- else %}\n        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ],
    "updated_at": 1769418548,
    "featured": false,
    "architectures": [
      "MistralForCausalLM"
    ],
    "model_type": "mistral",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "mistral-large-instruct",
    "model_lang": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "zh",
      "ru",
      "ja",
      "ko"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-Large-Instruct-2407 is an advanced dense Large Language Model (LLM) of 123B parameters with state-of-the-art reasoning, knowledge and coding capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 123,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mistralai/Mistral-Large-Instruct-2407"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Mistral-Large-Instruct-2407"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 123,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "unsloth/Mistral-Large-Instruct-2407-bnb-4bit"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Mistral-Large-Instruct-2407"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 123,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "ModelCloud/Mistral-Large-Instruct-2407-gptq-4bit"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 123,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TechxGenus/Mistral-Large-Instruct-2407-AWQ"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 123,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_K_S",
              "Q4_K_M"
            ],
            "model_id": "MaziyarPanahi/Mistral-Large-Instruct-2407-GGUF",
            "model_file_name_template": "Mistral-Large-Instruct-2407.{quantization}.gguf",
            "model_file_name_split_template": "Mixtral-8x22B-Instruct-v0.1.{quantization}-{part}.gguf",
            "quantization_parts": {
              "Q3_K_L": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "Q3_K_M": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "Q3_K_S": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "Q4_K_M": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "Q4_K_S": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ]
            }
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 123,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mlx-community/Mistral-Large-Instruct-2407-bf16"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 123,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Mistral-Large-Instruct-2407-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 123,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Mistral-Large-Instruct-2407-8bit"
          }
        }
      }
    ],
    "chat_template": "{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS][\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- '{\"type\": \"function\", \"function\": {' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- '\"' + key + '\": \"' + val + '\"' }}\n                    {%- else %}\n                        {{- '\"' + key + '\": ' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST]\" + system_message + \"\n\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST]\" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\n        {{- \"[TOOL_CALLS][\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + '</s>' }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- message[\"content\"] + '</s>'}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- '[TOOL_RESULTS]{\"content\": ' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ],
    "updated_at": 1769418548,
    "featured": false,
    "architectures": [
      "MistralForCausalLM"
    ],
    "model_type": "mistral",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 1024000,
    "model_name": "mistral-nemo-instruct",
    "model_lang": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "zh",
      "ru",
      "ja"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-Nemo-Base-2407",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mistralai/Mistral-Nemo-Instruct-2407",
            "model_revision": "05b1e4f3e189ec1b5189fb3c973d4cf3369c27af"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-ModelScope/Mistral-Nemo-Instruct-2407"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit",
            "model_revision": "1d85adc9e0fff0b8e4479a037bd75fe1346333ca"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-ModelScope/Mistral-Nemo-Instruct-2407"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "afrizalha/Mistral-Nemo-Instruct-2407-bnb-8bit",
            "model_revision": "1d2dacf18a486c745219317d1507441406bc7e25"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-ModelScope/Mistral-Nemo-Instruct-2407"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "ModelCloud/Mistral-Nemo-Instruct-2407-gptq-4bit"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "LLM-Research/Mistral-Nemo-Instruct-2407-gptq-4bit"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "casperhansen/mistral-nemo-instruct-2407-awq"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_K_S",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "fp16"
            ],
            "model_id": "MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF",
            "model_file_name_template": "Mistral-Nemo-Instruct-2407.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mlx-community/Mistral-Nemo-Instruct-2407-bf16"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Mistral-Nemo-Instruct-2407-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Mistral-Nemo-Instruct-2407-8bit"
          }
        }
      }
    ],
    "chat_template": "{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS][\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- '{\"type\": \"function\", \"function\": {' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- '\"' + key + '\": \"' + val + '\"' }}\n                    {%- else %}\n                        {{- '\"' + key + '\": ' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST]\" + system_message + \"\n\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST]\" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\n        {{- \"[TOOL_CALLS][\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + '</s>' }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- message[\"content\"] + '</s>'}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- '[TOOL_RESULTS]{\"content\": ' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ],
    "updated_at": 1769418549,
    "featured": false,
    "architectures": [
      "MistralForCausalLM"
    ],
    "model_type": "mistral",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 8192,
    "model_name": "mistral-v0.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Mistral-7B is a unmoderated Transformer based LLM claiming to outperform Llama2 on all benchmarks.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mistralai/Mistral-7B-v0.1",
            "model_revision": "ae9d75c6b4eb39515def78c685fb4d71d49fc2cf"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Xorbits/Mistral-7B-v0.1",
            "model_revision": "v1.0.0"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "PyTorch-NPU/mistral_7b_v0.1"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_0",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_0",
              "Q5_K_S",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/Mistral-7B-v0.1-GGUF",
            "model_file_name_template": "mistral-7b-v0.1.{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q3_K_S",
              "Q3_K_M",
              "Q3_K_L",
              "Q4_0",
              "Q4_K_S",
              "Q4_K_M",
              "Q5_0",
              "Q5_K_S",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "Xorbits/Mistral-7B-v0.1-GGUF",
            "model_file_name_template": "mistral-7b-v0.1.{quantization}.gguf",
            "model_revision": "v1.0.0"
          }
        }
      }
    ],
    "updated_at": 1769418550,
    "featured": false,
    "architectures": [
      "MistralForCausalLM"
    ],
    "model_type": "mistral",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "mixtral-instruct-v0.1",
    "model_lang": [
      "en",
      "fr",
      "it",
      "de",
      "es"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-8x7B-Instruct is a fine-tuned version of the Mistral-8x7B LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "46_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mistralai/Mixtral-8x7B-Instruct-v0.1",
            "model_revision": "125c431e2ff41a156b9f9076f744d2f35dd6e67a"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-ModelScope/Mixtral-8x7B-Instruct-v0.1",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "46_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "46_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "46_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_M",
              "Q4_0",
              "Q4_K_M",
              "Q5_0",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF",
            "model_file_name_template": "mixtral-8x7b-instruct-v0.1.{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n    {%- endif %}\n    {%- if message['role'] == 'user' %}\n        {%- if loop.first and system_message is defined %}\n            {{- ' [INST] ' + system_message + '\n\n' + message['content'] + ' [/INST]' }}\n        {%- else %}\n            {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n        {%- endif %}\n    {%- elif message['role'] == 'assistant' %}\n        {{- ' ' + message['content'] + '</s>'}}\n    {%- else %}\n        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ],
    "updated_at": 1769418551,
    "featured": false,
    "architectures": [
      "MixtralForCausalLM"
    ],
    "model_type": "mixtral",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "mixtral-v0.1",
    "model_lang": [
      "en",
      "fr",
      "it",
      "de",
      "es"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "46_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mistralai/Mixtral-8x7B-v0.1",
            "model_revision": "58301445dc1378584211722b7ebf8743ec4e192b"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-ModelScope/Mixtral-8x7B-v0.1",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "46_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "TheBloke/Mixtral-8x7B-v0.1-GPTQ"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "46_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_M",
              "Q4_0",
              "Q4_K_M",
              "Q5_0",
              "Q5_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/Mixtral-8x7B-v0.1-GGUF",
            "model_file_name_template": "mixtral-8x7b-v0.1.{quantization}.gguf"
          }
        }
      }
    ],
    "updated_at": 1769418552,
    "featured": false,
    "architectures": [
      "MixtralForCausalLM"
    ],
    "model_type": "mixtral",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 8192,
    "model_name": "moonlight-16b-a3b-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Kimi Muon is Scalable for LLM Training",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "moonshotai/Moonlight-16B-A3B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "moonshotai/Moonlight-16B-A3B-Instruct"
          }
        }
      }
    ],
    "chat_template": "{%- for message in messages -%}{%- if loop.first and messages[0]['role'] != 'system' -%}<|im_system|>system<|im_middle|>You are a helpful assistant<|im_end|>{%- endif -%}{%- if message['role'] == 'system' -%}<|im_system|>{%- endif -%}{%- if message['role'] == 'user' -%}<|im_user|>{%- endif -%}{%- if message['role'] == 'assistant' -%}<|im_assistant|>{%- endif -%}{{ message['role'] }}<|im_middle|>{{message['content']}}<|im_end|>{%- endfor -%}{%- if add_generation_prompt -%}<|im_assistant|>assistant<|im_middle|>{%- endif -%}",
    "stop_token_ids": [
      163586
    ],
    "stop": [
      "<|im_end|>"
    ],
    "updated_at": 1769418553,
    "featured": false,
    "architectures": [
      "DeepseekV3ForCausalLM"
    ],
    "model_type": "deepseek_v3",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "orion-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Orion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "OrionStarAI/Orion-14B-Chat",
            "model_revision": "ea6fb9b7e1917f3693935accbeb0bfecfd6552a7"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OrionStarAI/Orion-14B-Chat"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OrionStarAI/Orion-14B-Chat-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "OrionStarAI/Orion-14B-Chat-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.first %}{{ '<s>' }}{% endif %}{% if message['role'] == 'user' %}{{ 'Human: ' + message['content'] + '\n\nAssistant: ' + '</s>' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + '</s>' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2,
      0
    ],
    "stop": [
      "<s>",
      "</s>",
      "<unk>"
    ],
    "updated_at": 1769418554,
    "featured": false,
    "architectures": [
      "OrionForCausalLM"
    ],
    "model_type": "orion",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 128000,
    "model_name": "phi-3-mini-128k-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "microsoft/Phi-3-mini-128k-instruct",
            "model_revision": "ebee18c488086b396dde649f2aa6548b9b8d2404"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Phi-3-mini-128k-instruct",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n' + message['content'] + '<|end|>\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n' }}{% else %}{{ '<|endoftext|>' }}{% endif %}",
    "stop_token_ids": [
      32000,
      32001,
      32007
    ],
    "stop": [
      "<|endoftext|>",
      "<|assistant|>",
      "<|end|>"
    ],
    "updated_at": 1769418555,
    "featured": false,
    "architectures": [
      "Phi3ForCausalLM"
    ],
    "model_type": "phi3",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 4096,
    "model_name": "phi-3-mini-4k-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Phi-3-Mini-4k-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp16",
              "q4"
            ],
            "model_id": "microsoft/Phi-3-mini-4k-instruct-gguf",
            "model_file_name_template": "Phi-3-mini-4k-instruct-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "microsoft/Phi-3-mini-4k-instruct",
            "model_revision": "b86bcaf57ea4dfdec5dbe12a377028b2fab0d480"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "LLM-Research/Phi-3-mini-4k-instruct",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n' + message['content'] + '<|end|>\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n' }}{% else %}{{ '<|endoftext|>' }}{% endif %}",
    "stop_token_ids": [
      32000,
      32001,
      32007
    ],
    "stop": [
      "<|endoftext|>",
      "<|assistant|>",
      "<|end|>"
    ],
    "updated_at": 1769418556,
    "featured": false,
    "architectures": [
      "Phi3ForCausalLM"
    ],
    "model_type": "phi3",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "qwen-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Qwen-chat is a fine-tuned version of the Qwen LLM trained with alignment techniques, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q4_K_M"
            ],
            "model_id": "Xorbits/Qwen-7B-Chat-GGUF",
            "model_file_name_template": "Qwen-7B-Chat.{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q4_K_M"
            ],
            "model_id": "Xorbits/Qwen-7B-Chat-GGUF",
            "model_file_name_template": "Qwen-7B-Chat.{quantization}.gguf",
            "model_revision": "v0.0.1"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q4_K_M"
            ],
            "model_id": "Xorbits/Qwen-14B-Chat-GGUF",
            "model_file_name_template": "Qwen-14B-Chat.{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q4_K_M"
            ],
            "model_id": "Xorbits/Qwen-14B-Chat-GGUF",
            "model_file_name_template": "Qwen-14B-Chat.{quantization}.gguf",
            "model_revision": "v0.0.1"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_8",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen-1_8B-Chat",
            "model_revision": "c3db8007171847931da7efa4b2ed4309afcce021"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen-1_8B-Chat",
            "model_revision": "v1.0.0"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "HangZhou_Ascend/Qwen-1_8B-Chat"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen-7B-Chat",
            "model_revision": "218aa3240fd5a5d1e80bb6c47d5d774361913706"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen-7B-Chat",
            "model_revision": "v1.1.9"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "wuhaicc/Qwen-7B-Chat"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen-14B-Chat",
            "model_revision": "fab8385c8f7e7980ef61944729fe134ccbbca263"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen-14B-Chat",
            "model_revision": "v1.0.7"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "wuhaicc/Qwen-14B-Chat"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen-72B-Chat",
            "model_revision": "2cd9f76279337941ec1a4abeec6f8eb3c38d0f55"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen-72B-Chat",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen-7B-Chat-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen-7B-Chat-{quantization}",
            "model_revision": "v1.1.7"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_8",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen-1_8B-Chat-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen-1_8B-Chat-{quantization}",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen-14B-Chat-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen-14B-Chat-{quantization}",
            "model_revision": "v1.0.7"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen-72B-Chat-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen-72B-Chat-{quantization}",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ '<|im_start|>system\n' + item['content'] + '<|im_end|>\n' }}{% elif loop.first %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{% if item['role'] == 'user' %}{{ '<|im_start|>user\n' + item['content'] + '<|im_end|>' }}{% elif item['role'] == 'assistant' %}{{ '<|im_start|>assistant\n' + item['content'] + '<|im_end|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "updated_at": 1769418557,
    "featured": false,
    "architectures": [
      "QWenLMHeadModel"
    ],
    "model_type": "qwen",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "qwen1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen1.5-0.5B-Chat"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen1.5-0.5B-Chat"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "HangZhou_Ascend/Qwen1.5-0.5B-Chat"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_8",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen1.5-1.8B-Chat"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen1.5-1.8B-Chat"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen1.5-4B-Chat"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen1.5-4B-Chat"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "HangZhou_Ascend/Qwen1.5-4B-Chat"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen1.5-7B-Chat"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen1.5-7B-Chat"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "PyTorch-NPU/qwen1.5_7b_chat"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen1.5-14B-Chat"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen1.5-14B-Chat"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "State_Cloud/Qwen1.5-14B-Chat"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen1.5-32B-Chat"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen1.5-32B-Chat"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "State_Cloud/Qwen1.5-32b-chat"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen1.5-72B-Chat"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen1.5-72B-Chat"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "State_Cloud/Qwen1.5-72b-chat"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 110,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen1.5-110B-Chat"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen1.5-110B-Chat"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen1.5-0.5B-Chat-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen1.5-0.5B-Chat-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_8",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen1.5-1.8B-Chat-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen1.5-1.8B-Chat-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen1.5-4B-Chat-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen1.5-4B-Chat-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen1.5-7B-Chat-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen1.5-7B-Chat-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen1.5-14B-Chat-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen1.5-14B-Chat-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen1.5-32B-Chat-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen1.5-32B-Chat-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen1.5-72B-Chat-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen1.5-72B-Chat-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 110,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen1.5-110B-Chat-GPTQ-Int4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen1.5-110B-Chat-GPTQ-Int4"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen1.5-0.5B-Chat-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen1.5-0.5B-Chat-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_8",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen1.5-1.8B-Chat-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen1.5-1.8B-Chat-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen1.5-4B-Chat-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen1.5-4B-Chat-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen1.5-7B-Chat-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen1.5-7B-Chat-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen1.5-14B-Chat-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen1.5-14B-Chat-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen1.5-32B-Chat-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen1.5-32B-Chat-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen1.5-72B-Chat-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen1.5-72B-Chat-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 110,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen1.5-110B-Chat-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen1.5-110B-Chat-AWQ"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Qwen/Qwen1.5-0.5B-Chat-GGUF",
            "model_file_name_template": "qwen1_5-0_5b-chat-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/Qwen1.5-0.5B-Chat-GGUF",
            "model_file_name_template": "qwen1_5-0_5b-chat-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_8",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Qwen/Qwen1.5-1.8B-Chat-GGUF",
            "model_file_name_template": "qwen1_5-1_8b-chat-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/Qwen1.5-1.8B-Chat-GGUF",
            "model_file_name_template": "qwen1_5-1_8b-chat-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Qwen/Qwen1.5-4B-Chat-GGUF",
            "model_file_name_template": "qwen1_5-4b-chat-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/Qwen1.5-4B-Chat-GGUF",
            "model_file_name_template": "qwen1_5-4b-chat-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Qwen/Qwen1.5-7B-Chat-GGUF",
            "model_file_name_template": "qwen1_5-7b-chat-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/Qwen1.5-7B-Chat-GGUF",
            "model_file_name_template": "qwen1_5-7b-chat-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Qwen/Qwen1.5-14B-Chat-GGUF",
            "model_file_name_template": "qwen1_5-14b-chat-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/Qwen1.5-14B-Chat-GGUF",
            "model_file_name_template": "qwen1_5-14b-chat-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Qwen/Qwen1.5-32B-Chat-GGUF",
            "model_file_name_template": "qwen1_5-32b-chat-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/Qwen1.5-32B-Chat-GGUF",
            "model_file_name_template": "qwen1_5-32b-chat-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_k_m"
            ],
            "model_id": "Qwen/Qwen1.5-72B-Chat-GGUF",
            "model_file_name_template": "qwen1_5-72b-chat-{quantization}.gguf",
            "model_file_name_split_template": "qwen1_5-72b-chat-{quantization}.gguf.{part}",
            "quantization_parts": {
              "q4_k_m": [
                "a",
                "b"
              ]
            }
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_k_m"
            ],
            "model_id": "qwen/Qwen1.5-72B-Chat-GGUF",
            "model_file_name_template": "qwen1_5-72b-chat-{quantization}.gguf",
            "model_file_name_split_template": "qwen1_5-72b-chat-{quantization}.gguf.{part}",
            "quantization_parts": {
              "q4_k_m": [
                "a",
                "b"
              ]
            }
          }
        }
      }
    ],
    "chat_template": "{%- macro json_to_python_type(json_spec) %}\n    {%- set basic_type_map = {\n    \"string\": \"str\",\n    \"number\": \"float\",\n    \"integer\": \"int\",\n    \"boolean\": \"bool\"\n} %}\n    {%- if basic_type_map[json_spec.type] is defined %}\n        {{- basic_type_map[json_spec.type] }}\n    {%- elif json_spec.type == \"array\" %}\n        {{- \"list[\" +  json_to_python_type(json_spec|items) + \"]\" }}\n    {%- elif json_spec.type == \"object\" %}\n        {%- if json_spec.additionalProperties is defined %}\n            {{- \"dict[str, \" + json_to_python_type(json_spec.additionalProperties) + ']' }}\n        {%- else %}\n            {{- \"dict\" }}\n        {%- endif %}\n    {%- elif json_spec.type is iterable %}\n        {{- \"Union[\" }}\n        {%- for t in json_spec.type %}\n            {{- json_to_python_type({\"type\": t}) }}\n            {%- if not loop.last %}\n                {{- \",\" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \"]\" }}\n    {%- else %}\n        {{- \"Any\" }}\n    {%- endif %}\n{%- endmacro %}\n\n{%- if tools %}\n    {{- '<|im_start|>system\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] + '\n\n' }}\n    {%- endif %}\n    {{- '# Tools\n\n' }}\n    {{- \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- '{\"type\": \"function\", \"function\": ' }}\n        {{- '{\"name\": ' + tool.name + '\", ' }}\n        {{- '\"description\": \"' + tool.name + '(' }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {{- param_name + \": \" + json_to_python_type(param_fields) }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \")\" }}\n        {%- if tool.return is defined %}\n            {{- \" -> \" + json_to_python_type(tool.return) }}\n        {%- endif %}\n        {{- \" - \" + tool.description + \"\n\n\" }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {%- if loop.first %}\n                {{- \"    Args:\n\" }}\n            {%- endif %}\n            {{- \"        \" + param_name + \"(\" + json_to_python_type(param_fields) + \"): \" + param_fields.description|trim }}\n        {%- endfor %}\n        {%- if tool.return is defined and tool.return.description is defined %}\n            {{- \"\n    Returns:\n        \" + tool.return.description }}\n        {%- endif %}\n        {{- '\"' }}\n        {{- ', \"parameters\": ' }}\n        {%- if tool.parameters.properties | length == 0 %}\n            {{- \"{}\" }}\n        {%- else %}\n            {{- tool.parameters|tojson }}\n        {%- endif %}\n        {{- \"}\" }}\n        {%- if not loop.last %}\n            {{- \"\n\" }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \" </tools>\" }}\n    {{- 'Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"}\n' }}\n    {{- \"For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n\" }}\n    {{- \"<tool_call>\n\" }}\n    {{- '{\"name\": <function-name>, \"arguments\": <args-json-object>}\n' }}\n    {{- '</tool_call><|im_end|>\n' }}\n{%- else %}\n    {%- if messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and message.tool_calls is not defined) %}\n        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role + '\n<tool_call>\n' }}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '{' }}\n            {{- '\"name\": \"' }}\n            {{- tool_call.name }}\n            {%- if tool_call.arguments is defined %}\n                {{- ', ' }}\n                {{- '\"arguments\": ' }}\n                {{- tool_call.arguments|tojson }}\n            {%- endif %}\n            {{- '\"}' }}\n            {{- '\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if not message.name is defined %}\n            {{- raise_exception(\"Tool response dicts require a 'name' key indicating the name of the called function!\") }}\n        {%- endif %}\n        {{- '<|im_start|>user\n<tool_response>\n' }}\n        {{- '{\"name\": \"' }}\n        {{- message.name }}\n        {{- '\", \"content\": ' }}\n        {{- message.content|tojson + '}' }}\n        {{- '\n</tool_response><|im_end|>\n' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "tool_parser": "qwen",
    "updated_at": 1769418558,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "qwen1.5-moe-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen1.5-MoE is a transformer-based MoE decoder-only language model pretrained on a large amount of data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "2_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen1.5-MoE-A2.7B-Chat"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen1.5-MoE-A2.7B-Chat"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "2_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4"
          }
        }
      }
    ],
    "chat_template": "{%- macro json_to_python_type(json_spec) %}\n    {%- set basic_type_map = {\n    \"string\": \"str\",\n    \"number\": \"float\",\n    \"integer\": \"int\",\n    \"boolean\": \"bool\"\n} %}\n    {%- if basic_type_map[json_spec.type] is defined %}\n        {{- basic_type_map[json_spec.type] }}\n    {%- elif json_spec.type == \"array\" %}\n        {{- \"list[\" +  json_to_python_type(json_spec|items) + \"]\" }}\n    {%- elif json_spec.type == \"object\" %}\n        {%- if json_spec.additionalProperties is defined %}\n            {{- \"dict[str, \" + json_to_python_type(json_spec.additionalProperties) + ']' }}\n        {%- else %}\n            {{- \"dict\" }}\n        {%- endif %}\n    {%- elif json_spec.type is iterable %}\n        {{- \"Union[\" }}\n        {%- for t in json_spec.type %}\n            {{- json_to_python_type({\"type\": t}) }}\n            {%- if not loop.last %}\n                {{- \",\" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \"]\" }}\n    {%- else %}\n        {{- \"Any\" }}\n    {%- endif %}\n{%- endmacro %}\n\n{%- if tools %}\n    {{- '<|im_start|>system\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] + '\n\n' }}\n    {%- endif %}\n    {{- '# Tools\n\n' }}\n    {{- \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- '{\"type\": \"function\", \"function\": ' }}\n        {{- '{\"name\": ' + tool.name + '\", ' }}\n        {{- '\"description\": \"' + tool.name + '(' }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {{- param_name + \": \" + json_to_python_type(param_fields) }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \")\" }}\n        {%- if tool.return is defined %}\n            {{- \" -> \" + json_to_python_type(tool.return) }}\n        {%- endif %}\n        {{- \" - \" + tool.description + \"\n\n\" }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {%- if loop.first %}\n                {{- \"    Args:\n\" }}\n            {%- endif %}\n            {{- \"        \" + param_name + \"(\" + json_to_python_type(param_fields) + \"): \" + param_fields.description|trim }}\n        {%- endfor %}\n        {%- if tool.return is defined and tool.return.description is defined %}\n            {{- \"\n    Returns:\n        \" + tool.return.description }}\n        {%- endif %}\n        {{- '\"' }}\n        {{- ', \"parameters\": ' }}\n        {%- if tool.parameters.properties | length == 0 %}\n            {{- \"{}\" }}\n        {%- else %}\n            {{- tool.parameters|tojson }}\n        {%- endif %}\n        {{- \"}\" }}\n        {%- if not loop.last %}\n            {{- \"\n\" }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \" </tools>\" }}\n    {{- 'Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"}\n' }}\n    {{- \"For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n\" }}\n    {{- \"<tool_call>\n\" }}\n    {{- '{\"name\": <function-name>, \"arguments\": <args-json-object>}\n' }}\n    {{- '</tool_call><|im_end|>\n' }}\n{%- else %}\n    {%- if messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and message.tool_calls is not defined) %}\n        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role + '\n<tool_call>\n' }}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '{' }}\n            {{- '\"name\": \"' }}\n            {{- tool_call.name }}\n            {%- if tool_call.arguments is defined %}\n                {{- ', ' }}\n                {{- '\"arguments\": ' }}\n                {{- tool_call.arguments|tojson }}\n            {%- endif %}\n            {{- '\"}' }}\n            {{- '\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if not message.name is defined %}\n            {{- raise_exception(\"Tool response dicts require a 'name' key indicating the name of the called function!\") }}\n        {%- endif %}\n        {{- '<|im_start|>user\n<tool_response>\n' }}\n        {{- '{\"name\": \"' }}\n        {{- message.name }}\n        {{- '\", \"content\": ' }}\n        {{- message.content|tojson + '}' }}\n        {{- '\n</tool_response><|im_end|>\n' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "tool_parser": "qwen",
    "updated_at": 1769418559,
    "featured": false,
    "architectures": [
      "Qwen2MoeForCausalLM"
    ],
    "model_type": "qwen2_moe",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "qwen2-audio",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate",
      "audio"
    ],
    "model_description": "Qwen2-Audio: A large-scale audio-language model which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2-Audio-7B",
            "model_revision": "8577bc71d330c8fa32ffe9f8a1374100759f2466"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2-Audio-7B",
            "model_revision": "master"
          }
        }
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant",
      "roles": [
        "user",
        "assistant"
      ],
      "stop": [
        "<|im_end|>",
        "<|endoftext|>"
      ]
    },
    "updated_at": 1769418560,
    "featured": false,
    "architectures": [
      "Qwen2AudioForConditionalGeneration"
    ],
    "model_type": "qwen2_audio",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "qwen2-audio-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "audio"
    ],
    "model_description": "Qwen2-Audio: A large-scale audio-language model which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2-Audio-7B-Instruct",
            "model_revision": "bac62d2c6808845904c709c17a0402d817558c64"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2-Audio-7B-Instruct",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% set audio_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system You are a helpful assistant.<|im_end|> {% endif %}<|im_start|>{{ message['role'] }} {% if message['content'] is string %}{{ message['content'] }}<|im_end|> {% else %}{% for content in message['content'] %}{% if 'audio' in content or 'audio_url' in content or message['type'] == 'audio' or content['type'] == 'audio' %}{% set audio_count.value = audio_count.value + 1 %}Audio {{ audio_count.value }}: <|audio_bos|><|AUDIO|><|audio_eos|> {% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|> {% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant {% endif %}",
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant",
      "roles": [
        "user",
        "assistant"
      ],
      "stop": [
        "<|im_end|>",
        "<|endoftext|>"
      ]
    },
    "updated_at": 1769418561,
    "featured": false,
    "architectures": [
      "Qwen2AudioForConditionalGeneration"
    ],
    "model_type": "qwen2_audio",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "qwen2-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2 is the new series of Qwen large language models",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2-0.5B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2-0.5B-Instruct"
          },
          "csghub": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2-0.5B-Instruct"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "wuhaicc/Qwen2-0.5B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2-1.5B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2-1.5B-Instruct"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "HangZhou_Ascend/Qwen2-1.5B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2-7B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2-7B-Instruct"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "wuhaicc/Qwen2-7B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2-72B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2-72B-Instruct"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "State_Cloud/Qwen2-72B-Instruct"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2-0.5B-Instruct-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2-0.5B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2-1.5B-Instruct-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2-1.5B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2-7B-Instruct-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2-7B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2-72B-Instruct-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2-72B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2-0.5B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2-0.5B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2-1.5B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2-1.5B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2-7B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2-7B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2-72B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2-72B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "neuralmagic/Qwen2-0.5B-Instruct-FP8"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "neuralmagic/Qwen2-0.5B-Instruct-FP8"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "neuralmagic/Qwen2-1.5B-Instruct-FP8"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "neuralmagic/Qwen2-7B-Instruct-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "liuzhenghua/Qwen2-7B-FP8-Instruct"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "neuralmagic/Qwen2-72B-Instruct-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "liuzhenghua/Qwen2-72B-FP8-Instruct"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "Qwen/Qwen2-0.5B-Instruct-MLX"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "qwen/Qwen2-0.5B-Instruct-MLX"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "Qwen/Qwen2-1.5B-Instruct-MLX"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "qwen/Qwen2-1.5B-Instruct-MLX"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "Qwen/Qwen2-7B-Instruct-MLX"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "qwen/Qwen2-7B-Instruct-MLX"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen2-72B-Instruct-4bit"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0",
              "fp16"
            ],
            "model_id": "Qwen/Qwen2-0.5B-Instruct-GGUF",
            "model_file_name_template": "qwen2-0_5b-instruct-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0",
              "fp16"
            ],
            "model_id": "qwen/Qwen2-0.5B-Instruct-GGUF",
            "model_file_name_template": "qwen2-0_5b-instruct-{quantization}.gguf"
          },
          "csghub": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0",
              "fp16"
            ],
            "model_id": "qwen/Qwen2-0.5B-Instruct-GGUF",
            "model_file_name_template": "qwen2-0_5b-instruct-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0",
              "fp16"
            ],
            "model_id": "Qwen/Qwen2-1.5B-Instruct-GGUF",
            "model_file_name_template": "qwen2-1_5b-instruct-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0",
              "fp16"
            ],
            "model_id": "qwen/Qwen2-1.5B-Instruct-GGUF",
            "model_file_name_template": "qwen2-1_5b-instruct-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0",
              "fp16"
            ],
            "model_id": "Qwen/Qwen2-7B-Instruct-GGUF",
            "model_file_name_template": "qwen2-7b-instruct-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0",
              "fp16"
            ],
            "model_id": "qwen/Qwen2-7B-Instruct-GGUF",
            "model_file_name_template": "qwen2-7b-instruct-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0",
              "fp16"
            ],
            "model_id": "Qwen/Qwen2-72B-Instruct-GGUF",
            "model_file_name_template": "qwen2-72b-instruct-{quantization}.gguf",
            "model_file_name_split_template": "qwen2-72b-instruct-{quantization}-{part}.gguf",
            "quantization_parts": {
              "q5_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q5_k_m": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q6_k": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q8_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "fp16": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ]
            }
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0",
              "fp16"
            ],
            "model_id": "qwen/Qwen2-72B-Instruct-GGUF",
            "model_file_name_template": "qwen2-72b-instruct-{quantization}.gguf",
            "model_file_name_split_template": "qwen2-72b-instruct-{quantization}-{part}.gguf",
            "quantization_parts": {
              "q5_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q5_k_m": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q6_k": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q8_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "fp16": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ]
            }
          }
        }
      }
    ],
    "chat_template": "{%- macro json_to_python_type(json_spec) %}\n    {%- set basic_type_map = {\n    \"string\": \"str\",\n    \"number\": \"float\",\n    \"integer\": \"int\",\n    \"boolean\": \"bool\"\n} %}\n    {%- if basic_type_map[json_spec.type] is defined %}\n        {{- basic_type_map[json_spec.type] }}\n    {%- elif json_spec.type == \"array\" %}\n        {{- \"list[\" +  json_to_python_type(json_spec|items) + \"]\" }}\n    {%- elif json_spec.type == \"object\" %}\n        {%- if json_spec.additionalProperties is defined %}\n            {{- \"dict[str, \" + json_to_python_type(json_spec.additionalProperties) + ']' }}\n        {%- else %}\n            {{- \"dict\" }}\n        {%- endif %}\n    {%- elif json_spec.type is iterable %}\n        {{- \"Union[\" }}\n        {%- for t in json_spec.type %}\n            {{- json_to_python_type({\"type\": t}) }}\n            {%- if not loop.last %}\n                {{- \",\" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \"]\" }}\n    {%- else %}\n        {{- \"Any\" }}\n    {%- endif %}\n{%- endmacro %}\n\n{%- if tools %}\n    {{- '<|im_start|>system\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] + '\n\n' }}\n    {%- endif %}\n    {{- '# Tools\n\n' }}\n    {{- \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- '{\"type\": \"function\", \"function\": ' }}\n        {{- '{\"name\": ' + tool.name + '\", ' }}\n        {{- '\"description\": \"' + tool.name + '(' }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {{- param_name + \": \" + json_to_python_type(param_fields) }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \")\" }}\n        {%- if tool.return is defined %}\n            {{- \" -> \" + json_to_python_type(tool.return) }}\n        {%- endif %}\n        {{- \" - \" + tool.description + \"\n\n\" }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {%- if loop.first %}\n                {{- \"    Args:\n\" }}\n            {%- endif %}\n            {{- \"        \" + param_name + \"(\" + json_to_python_type(param_fields) + \"): \" + param_fields.description|trim }}\n        {%- endfor %}\n        {%- if tool.return is defined and tool.return.description is defined %}\n            {{- \"\n    Returns:\n        \" + tool.return.description }}\n        {%- endif %}\n        {{- '\"' }}\n        {{- ', \"parameters\": ' }}\n        {%- if tool.parameters.properties | length == 0 %}\n            {{- \"{}\" }}\n        {%- else %}\n            {{- tool.parameters|tojson }}\n        {%- endif %}\n        {{- \"}\" }}\n        {%- if not loop.last %}\n            {{- \"\n\" }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \" </tools>\" }}\n    {{- 'Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"}\n' }}\n    {{- \"For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n\" }}\n    {{- \"<tool_call>\n\" }}\n    {{- '{\"name\": <function-name>, \"arguments\": <args-json-object>}\n' }}\n    {{- '</tool_call><|im_end|>\n' }}\n{%- else %}\n    {%- if messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and message.tool_calls is not defined) %}\n        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role + '\n<tool_call>\n' }}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '{' }}\n            {{- '\"name\": \"' }}\n            {{- tool_call.name }}\n            {%- if tool_call.arguments is defined %}\n                {{- ', ' }}\n                {{- '\"arguments\": ' }}\n                {{- tool_call.arguments|tojson }}\n            {%- endif %}\n            {{- '\"}' }}\n            {{- '\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if not message.name is defined %}\n            {{- raise_exception(\"Tool response dicts require a 'name' key indicating the name of the called function!\") }}\n        {%- endif %}\n        {{- '<|im_start|>user\n<tool_response>\n' }}\n        {{- '{\"name\": \"' }}\n        {{- message.name }}\n        {{- '\", \"content\": ' }}\n        {{- message.content|tojson + '}' }}\n        {{- '\n</tool_response><|im_end|>\n' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "tool_parser": "qwen",
    "updated_at": 1769418562,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "qwen2-moe-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2 is the new series of Qwen large language models. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2-57B-A14B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2-57B-A14B-Instruct"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2-57B-A14B-Instruct-GPTQ-Int4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2-57B-A14B-Instruct-GPTQ-Int4"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0",
              "fp16"
            ],
            "model_id": "Qwen/Qwen2-57B-A14B-Instruct-GGUF",
            "model_file_name_template": "qwen2-57b-a14b-instruct-{quantization}.gguf",
            "model_file_name_split_template": "qwen2-57b-a14b-instruct-{quantization}-{part}.gguf",
            "quantization_parts": {
              "q8_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "fp16": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ]
            }
          },
          "modelscope": {
            "quantizations": [
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0",
              "fp16"
            ],
            "model_id": "qwen/Qwen2-57B-A14B-Instruct-GGUF",
            "model_file_name_template": "qwen2-57b-a14b-instruct-{quantization}.gguf",
            "model_file_name_split_template": "qwen2-57b-a14b-instruct-{quantization}-{part}.gguf",
            "quantization_parts": {
              "q8_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "fp16": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ]
            }
          }
        }
      }
    ],
    "chat_template": "{%- macro json_to_python_type(json_spec) %}\n    {%- set basic_type_map = {\n    \"string\": \"str\",\n    \"number\": \"float\",\n    \"integer\": \"int\",\n    \"boolean\": \"bool\"\n} %}\n    {%- if basic_type_map[json_spec.type] is defined %}\n        {{- basic_type_map[json_spec.type] }}\n    {%- elif json_spec.type == \"array\" %}\n        {{- \"list[\" +  json_to_python_type(json_spec|items) + \"]\" }}\n    {%- elif json_spec.type == \"object\" %}\n        {%- if json_spec.additionalProperties is defined %}\n            {{- \"dict[str, \" + json_to_python_type(json_spec.additionalProperties) + ']' }}\n        {%- else %}\n            {{- \"dict\" }}\n        {%- endif %}\n    {%- elif json_spec.type is iterable %}\n        {{- \"Union[\" }}\n        {%- for t in json_spec.type %}\n            {{- json_to_python_type({\"type\": t}) }}\n            {%- if not loop.last %}\n                {{- \",\" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \"]\" }}\n    {%- else %}\n        {{- \"Any\" }}\n    {%- endif %}\n{%- endmacro %}\n\n{%- if tools %}\n    {{- '<|im_start|>system\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] + '\n\n' }}\n    {%- endif %}\n    {{- '# Tools\n\n' }}\n    {{- \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- '{\"type\": \"function\", \"function\": ' }}\n        {{- '{\"name\": ' + tool.name + '\", ' }}\n        {{- '\"description\": \"' + tool.name + '(' }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {{- param_name + \": \" + json_to_python_type(param_fields) }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \")\" }}\n        {%- if tool.return is defined %}\n            {{- \" -> \" + json_to_python_type(tool.return) }}\n        {%- endif %}\n        {{- \" - \" + tool.description + \"\n\n\" }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {%- if loop.first %}\n                {{- \"    Args:\n\" }}\n            {%- endif %}\n            {{- \"        \" + param_name + \"(\" + json_to_python_type(param_fields) + \"): \" + param_fields.description|trim }}\n        {%- endfor %}\n        {%- if tool.return is defined and tool.return.description is defined %}\n            {{- \"\n    Returns:\n        \" + tool.return.description }}\n        {%- endif %}\n        {{- '\"' }}\n        {{- ', \"parameters\": ' }}\n        {%- if tool.parameters.properties | length == 0 %}\n            {{- \"{}\" }}\n        {%- else %}\n            {{- tool.parameters|tojson }}\n        {%- endif %}\n        {{- \"}\" }}\n        {%- if not loop.last %}\n            {{- \"\n\" }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \" </tools>\" }}\n    {{- 'Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"}\n' }}\n    {{- \"For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n\" }}\n    {{- \"<tool_call>\n\" }}\n    {{- '{\"name\": <function-name>, \"arguments\": <args-json-object>}\n' }}\n    {{- '</tool_call><|im_end|>\n' }}\n{%- else %}\n    {%- if messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and message.tool_calls is not defined) %}\n        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role + '\n<tool_call>\n' }}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '{' }}\n            {{- '\"name\": \"' }}\n            {{- tool_call.name }}\n            {%- if tool_call.arguments is defined %}\n                {{- ', ' }}\n                {{- '\"arguments\": ' }}\n                {{- tool_call.arguments|tojson }}\n            {%- endif %}\n            {{- '\"}' }}\n            {{- '\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if not message.name is defined %}\n            {{- raise_exception(\"Tool response dicts require a 'name' key indicating the name of the called function!\") }}\n        {%- endif %}\n        {{- '<|im_start|>user\n<tool_response>\n' }}\n        {{- '{\"name\": \"' }}\n        {{- message.name }}\n        {{- '\", \"content\": ' }}\n        {{- message.content|tojson + '}' }}\n        {{- '\n</tool_response><|im_end|>\n' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "tool_parser": "qwen",
    "updated_at": 1769418563,
    "featured": false,
    "architectures": [
      "Qwen2MoeForCausalLM"
    ],
    "model_type": "qwen2_moe",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "qwen2-vl-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Qwen2-VL: To See the World More Clearly.Qwen2-VL is the latest version of the vision language models in the Qwen model familities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2-VL-2B-Instruct",
            "model_revision": "096da3b96240e3d66d35be0e5ccbe282eea8d6b1"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2-VL-2B-Instruct",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "LlamaFactory/Qwen2-VL-2B-Instruct"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int8",
            "model_revision": "d15fb11857ccc566903e2e71341f9db7babb567b"
          },
          "modelscope": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "qwen/Qwen2-VL-2B-Instruct-GPTQ-Int8",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4",
            "model_revision": "800d396518c82960ce6d231adecd07bbc474f0a9"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2-VL-2B-Instruct-AWQ",
            "model_revision": "ea8c5854c0044e28626719292de0d9b1a671f6fc"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2-VL-2B-Instruct-AWQ",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen2-VL-2B-Instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen2-VL-2B-Instruct-{quantization}",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2-VL-7B-Instruct",
            "model_revision": "6010982c1010c3b222fa98afc81575f124aa9bd6"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2-VL-7B-Instruct",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "LlamaFactory/Qwen2-VL-7B-Instruct"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8",
            "model_revision": "3d152a77eaccfd72d59baedb0b183a1b8fd56e48"
          },
          "modelscope": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4",
            "model_revision": "5ab897112fa83b9699826be8753ef9184585c77d"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2-VL-7B-Instruct-AWQ",
            "model_revision": "f94216e8b513933bccd567bcd9b7350199f32538"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2-VL-7B-Instruct-AWQ",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2-VL-72B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2-VL-72B-Instruct"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2-VL-72B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2-VL-72B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2-VL-72B-Instruct-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2-VL-72B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen2-VL-72B-Instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "okwinds/Qwen2-VL-72B-Instruct-MLX-{quantization}",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen2-VL-7B-Instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "okwinds/Qwen2-VL-7B-Instruct-MLX-8bit",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ],
    "updated_at": 1769418564,
    "featured": false,
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "model_type": "qwen2_vl",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "qwen2.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-0.5B",
            "model_revision": "2630d3d2321bc1f1878f702166d1b2af019a7310"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-0.5B",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "Tianjin_Ascend/qwen2.5-0.5b"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-1.5B",
            "model_revision": "e5dfabbcffd9b0c7b31d89b82c5a6b72e663f32c"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-1.5B",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "Tianjin_Ascend/Qwen2.5-1.5B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-3B",
            "model_revision": "e4aa5ac50aa507415cda96cc99eb77ad0a3d2d34"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-3B",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "Tianjin_Ascend/Qwen2.5-3B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-7B",
            "model_revision": "09a0bac5707b43ec44508eab308b0846320c1ed4"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-7B",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-Research/Qwen2.5-7B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-14B",
            "model_revision": "d02b64ba1ce86bf9948668a13f82709600431ccc"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-14B",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-32B",
            "model_revision": "ff23665d01c3665be5fdb271d18a62090b65c06d"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-32B",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-Research/Qwen2.5-32B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-72B",
            "model_revision": "587cc4061cf6a7cc0d429d05c109447e5cf063af"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-72B",
            "model_revision": "master"
          }
        }
      }
    ],
    "updated_at": 1769418565,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "qwen2.5-coder",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-0.5B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-Coder-0.5B",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-1.5B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-Coder-1.5B",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "3",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-3B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-Coder-3B",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-7B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-Coder-7B",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-14B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-Coder-14B",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-32B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-Coder-32B",
            "model_revision": "master"
          }
        }
      }
    ],
    "updated_at": 1769418565,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "qwen2.5-coder-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-0.5B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-Coder-0.5B-Instruct",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-1.5B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-Coder-1.5B-Instruct",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "3",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-3B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-Coder-3B-Instruct",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-7B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-Coder-7B-Instruct",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-14B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-Coder-14B-Instruct",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-32B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-Coder-32B-Instruct",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-{quantization}",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-{quantization}",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF",
            "model_file_name_template": "qwen2.5-coder-1.5b-instruct-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF",
            "model_file_name_template": "qwen2.5-coder-1.5b-instruct-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-7B-Instruct-GGUF",
            "model_file_name_template": "qwen2.5-coder-7b-instruct-{quantization}.gguf",
            "model_file_name_split_template": "qwen2.5-coder-7b-instruct-{quantization}-{part}.gguf",
            "quantization_parts": {
              "q4_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q4_k_m": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q5_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q5_k_m": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q6_k": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q8_0": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ]
            }
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/Qwen2.5-Coder-7B-Instruct-GGUF",
            "model_file_name_template": "qwen2.5-coder-7b-instruct-{quantization}.gguf",
            "model_file_name_split_template": "qwen2.5-coder-7b-instruct-{quantization}-{part}.gguf",
            "quantization_parts": {
              "q4_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q4_k_m": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q5_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q5_k_m": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q6_k": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q8_0": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ]
            }
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "3",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "14",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "32",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "3",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-3B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-7B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "14",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-14B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "32",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-Coder-32B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 3,
        "model_src": {
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-{quantization}",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-{quantization}",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "model_src": {
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-{quantization}",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "model_src": {
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-{quantization}",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 3,
        "model_src": {
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2.5-Coder-3B-Instruct-AWQ",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "model_src": {
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2.5-Coder-7B-Instruct-AWQ",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "model_src": {
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2.5-Coder-14B-Instruct-AWQ",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "model_src": {
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2.5-Coder-32B-Instruct-AWQ",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "tool_parser": "qwen",
    "updated_at": 1769418566,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "qwen2.5-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-0.5B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-0.5B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-1.5B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-1.5B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-3B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-3B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-7B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-7B-Instruct"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-Research/Qwen2.5-7B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-14B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-14B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-32B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-32B-Instruct"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-Research/Qwen2.5-32B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-72B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "qwen/Qwen2.5-72B-Instruct"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2.5-0.5B-Instruct-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2.5-0.5B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2.5-1.5B-Instruct-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2.5-1.5B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2.5-3B-Instruct-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2.5-3B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2.5-7B-Instruct-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2.5-7B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2.5-14B-Instruct-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2.5-14B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2.5-32B-Instruct-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2.5-32B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "Qwen/Qwen2.5-72B-Instruct-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "qwen/Qwen2.5-72B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-0.5B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2-0.5B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-1.5B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2-1.5B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-3B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2.5-3B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-7B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2.5-7B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-14B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2.5-14B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-32B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2.5-32B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-72B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "qwen/Qwen2.5-72B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Qwen/Qwen2.5-0.5B-Instruct-GGUF",
            "model_file_name_template": "qwen2.5-0.5b-instruct-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/Qwen2.5-0.5B-Instruct-GGUF",
            "model_file_name_template": "qwen2.5-0.5b-instruct-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Qwen/Qwen2.5-1.5B-Instruct-GGUF",
            "model_file_name_template": "qwen2.5-1.5b-instruct-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/Qwen2.5-1.5B-Instruct-GGUF",
            "model_file_name_template": "qwen2.5-1.5b-instruct-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Qwen/Qwen2.5-3B-Instruct-GGUF",
            "model_file_name_template": "qwen2.5-3b-instruct-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/Qwen2.5-3B-Instruct-GGUF",
            "model_file_name_template": "qwen2.5-3b-instruct-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Qwen/Qwen2.5-7B-Instruct-GGUF",
            "model_file_name_template": "qwen2.5-7b-instruct-{quantization}.gguf",
            "model_file_name_split_template": "qwen2.5-7b-instruct-{quantization}-{part}.gguf",
            "quantization_parts": {
              "q4_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q4_k_m": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q5_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q5_k_m": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q6_k": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q8_0": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            }
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/Qwen2.5-7B-Instruct-GGUF",
            "model_file_name_template": "qwen2.5-7b-instruct-{quantization}.gguf",
            "model_file_name_split_template": "qwen2.5-7b-instruct-{quantization}-{part}.gguf",
            "quantization_parts": {
              "q4_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q4_k_m": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q5_0": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q5_k_m": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q6_k": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q8_0": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            }
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Qwen/Qwen2.5-14B-Instruct-GGUF",
            "model_file_name_template": "qwen2.5-14b-instruct-{quantization}.gguf",
            "model_file_name_split_template": "qwen2.5-14b-instruct-{quantization}-{part}.gguf",
            "quantization_parts": {
              "q2_k": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q3_k_m": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q4_0": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "q4_k_m": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "q5_0": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "q5_k_m": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "q6_k": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "q8_0": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ]
            }
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/Qwen2.5-14B-Instruct-GGUF",
            "model_file_name_template": "qwen2.5-14b-instruct-{quantization}.gguf",
            "model_file_name_split_template": "qwen2.5-14b-instruct-{quantization}-{part}.gguf",
            "quantization_parts": {
              "q2_k": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q3_k_m": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "q4_0": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "q4_k_m": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "q5_0": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "q5_k_m": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "q6_k": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "q8_0": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ]
            }
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "Qwen/Qwen2.5-32B-Instruct-GGUF",
            "model_file_name_template": "qwen2_5-32b-instruct-{quantization}.gguf",
            "model_file_name_split_template": "qwen2.5-32b-instruct-{quantization}-{part}.gguf",
            "quantization_parts": {
              "q2_k": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "q3_k_m": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "q4_0": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "q4_k_m": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "q5_0": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "q5_k_m": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "q6_k": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "q8_0": [
                "00001-of-00009",
                "00002-of-00009",
                "00003-of-00009",
                "00004-of-00009",
                "00005-of-00009",
                "00006-of-00009",
                "00007-of-00009",
                "00008-of-00009",
                "00009-of-00009"
              ]
            }
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/Qwen2.5-32B-Instruct-GGUF",
            "model_file_name_template": "qwen2_5-32b-instruct-{quantization}.gguf",
            "model_file_name_split_template": "qwen2.5-32b-instruct-{quantization}-{part}.gguf",
            "quantization_parts": {
              "q2_k": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "q3_k_m": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "q4_0": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "q4_k_m": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "q5_0": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "q5_k_m": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "q6_k": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "q8_0": [
                "00001-of-00009",
                "00002-of-00009",
                "00003-of-00009",
                "00004-of-00009",
                "00005-of-00009",
                "00006-of-00009",
                "00007-of-00009",
                "00008-of-00009",
                "00009-of-00009"
              ]
            }
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen2.5-3B-Instruct-4bit"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "okwinds/Qwen2.5-3B-Instruct-MLX-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen2.5-3B-Instruct-8bit"
          },
          "modelscope": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "okwinds/Qwen2.5-3B-Instruct-MLX-8bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen2.5-7B-Instruct-4bit"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "okwinds/Qwen2.5-7B-Instruct-MLX-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen2.5-7B-Instruct-8bit"
          },
          "modelscope": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "okwinds/Qwen2.5-7B-Instruct-MLX-8bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen2.5-14B-Instruct-4bit"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "okwinds/Qwen2.5-14B-Instruct-MLX-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen2.5-14B-Instruct-8bit"
          },
          "modelscope": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "okwinds/Qwen2.5-14B-Instruct-MLX-8bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen2.5-32B-Instruct-4bit"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "okwinds/Qwen2.5-32B-Instruct-MLX-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen2.5-32B-Instruct-8bit"
          },
          "modelscope": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "okwinds/Qwen2.5-32B-Instruct-MLX-8bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen2.5-72B-Instruct-4bit"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "okwinds/Qwen2.5-72B-Instruct-MLX-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen2.5-72B-Instruct-8bit"
          },
          "modelscope": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "okwinds/Qwen2.5-72B-Instruct-MLX-8bit"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0",
              "fp16"
            ],
            "model_id": "Qwen/Qwen2.5-72B-Instruct-GGUF",
            "model_file_name_template": "qwen2_5-72b-instruct-{quantization}.gguf",
            "model_file_name_split_template": "qwen2.5-72b-instruct-{quantization}-{part}.gguf",
            "quantization_parts": {
              "q2_k": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "q3_k_m": [
                "00001-of-00009",
                "00002-of-00009",
                "00003-of-00009",
                "00004-of-00009",
                "00005-of-00009",
                "00006-of-00009",
                "00007-of-00009",
                "00008-of-00009",
                "00009-of-00009"
              ],
              "q4_0": [
                "00001-of-00011",
                "00002-of-00011",
                "00003-of-00011",
                "00004-of-00011",
                "00005-of-00011",
                "00006-of-00011",
                "00007-of-00011",
                "00008-of-00011",
                "00009-of-00011",
                "00010-of-00011",
                "00011-of-00011"
              ],
              "q4_k_m": [
                "00001-of-00012",
                "00002-of-00012",
                "00003-of-00012",
                "00004-of-00012",
                "00005-of-00012",
                "00006-of-00012",
                "00007-of-00012",
                "00008-of-00012",
                "00009-of-00012",
                "00010-of-00012",
                "00011-of-00012",
                "00012-of-00012"
              ],
              "q5_0": [
                "00001-of-00013",
                "00002-of-00013",
                "00003-of-00013",
                "00004-of-00013",
                "00005-of-00013",
                "00006-of-00013",
                "00007-of-00013",
                "00008-of-00013",
                "00009-of-00013",
                "00010-of-00013",
                "00011-of-00013",
                "00012-of-00013",
                "00013-of-00013"
              ],
              "q5_k_m": [
                "00001-of-00014",
                "00002-of-00014",
                "00003-of-00014",
                "00004-of-00014",
                "00005-of-00014",
                "00006-of-00014",
                "00007-of-00014",
                "00008-of-00014",
                "00009-of-00014",
                "00010-of-00014",
                "00011-of-00014",
                "00012-of-00014",
                "00013-of-00014",
                "00014-of-00014"
              ],
              "q6_k": [
                "00001-of-00016",
                "00002-of-00016",
                "00003-of-00016",
                "00004-of-00016",
                "00005-of-00016",
                "00006-of-00016",
                "00007-of-00016",
                "00008-of-00016",
                "00009-of-00016",
                "00010-of-00016",
                "00011-of-00016",
                "00012-of-00016",
                "00013-of-00016",
                "00014-of-00016",
                "00015-of-00016",
                "00016-of-00016"
              ],
              "q8_0": [
                "00001-of-00021",
                "00002-of-00021",
                "00003-of-00021",
                "00004-of-00021",
                "00005-of-00021",
                "00006-of-00021",
                "00007-of-00021",
                "00008-of-00021",
                "00009-of-00021",
                "00010-of-00021",
                "00011-of-00021",
                "00012-of-00021",
                "00013-of-00021",
                "00014-of-00021",
                "00015-of-00021",
                "00016-of-00021",
                "00017-of-00021",
                "00018-of-00021",
                "00019-of-00021",
                "00020-of-00021",
                "00021-of-00021"
              ]
            }
          },
          "modelscope": {
            "quantizations": [
              "q2_k",
              "q3_k_m",
              "q4_0",
              "q4_k_m",
              "q5_0",
              "q5_k_m",
              "q6_k",
              "q8_0"
            ],
            "model_id": "qwen/Qwen2.5-72B-Instruct-GGUF",
            "model_file_name_template": "qwen2_5-72b-instruct-{quantization}.gguf",
            "model_file_name_split_template": "qwen2.5-72b-instruct-{quantization}-{part}.gguf",
            "quantization_parts": {
              "q2_k": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "q3_k_m": [
                "00001-of-00009",
                "00002-of-00009",
                "00003-of-00009",
                "00004-of-00009",
                "00005-of-00009",
                "00006-of-00009",
                "00007-of-00009",
                "00008-of-00009",
                "00009-of-00009"
              ],
              "q4_0": [
                "00001-of-00011",
                "00002-of-00011",
                "00003-of-00011",
                "00004-of-00011",
                "00005-of-00011",
                "00006-of-00011",
                "00007-of-00011",
                "00008-of-00011",
                "00009-of-00011",
                "00010-of-00011",
                "00011-of-00011"
              ],
              "q4_k_m": [
                "00001-of-00012",
                "00002-of-00012",
                "00003-of-00012",
                "00004-of-00012",
                "00005-of-00012",
                "00006-of-00012",
                "00007-of-00012",
                "00008-of-00012",
                "00009-of-00012",
                "00010-of-00012",
                "00011-of-00012",
                "00012-of-00012"
              ],
              "q5_0": [
                "00001-of-00013",
                "00002-of-00013",
                "00003-of-00013",
                "00004-of-00013",
                "00005-of-00013",
                "00006-of-00013",
                "00007-of-00013",
                "00008-of-00013",
                "00009-of-00013",
                "00010-of-00013",
                "00011-of-00013",
                "00012-of-00013",
                "00013-of-00013"
              ],
              "q5_k_m": [
                "00001-of-00014",
                "00002-of-00014",
                "00003-of-00014",
                "00004-of-00014",
                "00005-of-00014",
                "00006-of-00014",
                "00007-of-00014",
                "00008-of-00014",
                "00009-of-00014",
                "00010-of-00014",
                "00011-of-00014",
                "00012-of-00014",
                "00013-of-00014",
                "00014-of-00014"
              ],
              "q6_k": [
                "00001-of-00016",
                "00002-of-00016",
                "00003-of-00016",
                "00004-of-00016",
                "00005-of-00016",
                "00006-of-00016",
                "00007-of-00016",
                "00008-of-00016",
                "00009-of-00016",
                "00010-of-00016",
                "00011-of-00016",
                "00012-of-00016",
                "00013-of-00016",
                "00014-of-00016",
                "00015-of-00016",
                "00016-of-00016"
              ],
              "q8_0": [
                "00001-of-00021",
                "00002-of-00021",
                "00003-of-00021",
                "00004-of-00021",
                "00005-of-00021",
                "00006-of-00021",
                "00007-of-00021",
                "00008-of-00021",
                "00009-of-00021",
                "00010-of-00021",
                "00011-of-00021",
                "00012-of-00021",
                "00013-of-00021",
                "00014-of-00021",
                "00015-of-00021",
                "00016-of-00021",
                "00017-of-00021",
                "00018-of-00021",
                "00019-of-00021",
                "00020-of-00021",
                "00021-of-00021"
              ]
            }
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen2.5-0.5B-Instruct-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen2.5-0.5B-Instruct-8bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "0_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mlx-community/Qwen2.5-0.5B-Instruct-bf16"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen2.5-1.5B-Instruct-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen2.5-1.5B-Instruct-8bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mlx-community/Qwen2.5-1.5B-Instruct-bf16"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mlx-community/Qwen2.5-3B-Instruct-bf16"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mlx-community/Qwen2.5-7B-Instruct-bf16"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mlx-community/Qwen2.5-14B-Instruct-bf16"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mlx-community/Qwen2.5-32B-Instruct-bf16"
          },
          "modelscope": {
            "quantizations": [
              "2bit"
            ],
            "model_id": "okwinds/Qwen2.5-32B-Instruct-MLX-2bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "mlx-community/Qwen2.5-72B-Instruct-bf16"
          },
          "modelscope": {
            "quantizations": [
              "2bit"
            ],
            "model_id": "okwinds/Qwen2.5-32B-Instruct-MLX-2bit"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "tool_parser": "qwen",
    "updated_at": 1769418567,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 1010000,
    "model_name": "qwen2.5-instruct-1m",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Qwen2.5-1M is the long-context version of the Qwen2.5 series models, supporting a context length of up to 1M tokens.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-7B-Instruct-1M"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-7B-Instruct-1M"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-14B-Instruct-1M"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-14B-Instruct-1M"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ],
    "updated_at": 1769418568,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "qwen2.5-omni",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision",
      "audio",
      "omni"
    ],
    "model_description": "Qwen2.5-Omni: the new flagship end-to-end multimodal model in the Qwen series.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Omni-3B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Omni-3B"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Omni-7B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-Omni-7B"
          }
        }
      }
    ],
    "chat_template": "{% set audio_count = namespace(value=0) %}{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_bos|><|IMAGE|><|vision_eos|>{% elif content['type'] == 'audio' or 'audio' in content or 'audio_url' in content %}{% set audio_count.value = audio_count.value + 1 %}{% if add_audio_id %}Audio {{ audio_count.value }}: {% endif %}<|audio_bos|><|AUDIO|><|audio_eos|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_bos|><|VIDEO|><|vision_eos|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ],
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#system_numpy# ; #engine# == \"vllm\"",
        "qwen_omni_utils",
        "soundfile"
      ]
    },
    "updated_at": 1769418569,
    "featured": false,
    "architectures": [
      "Qwen2_5OmniModel"
    ],
    "model_type": "qwen2_5_omni"
  },
  {
    "version": 2,
    "context_length": 128000,
    "model_name": "qwen2.5-vl-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Qwen2.5-VL: Qwen2.5-VL is the latest version of the vision language models in the Qwen model familities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-VL-3B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-VL-3B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-VL-7B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-VL-7B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-VL-32B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-VL-32B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-VL-72B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen2.5-VL-72B-Instruct"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-VL-3B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-VL-3B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-VL-7B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-VL-7B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-VL-32B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-VL-32B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen2.5-VL-72B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen2.5-VL-3B-Instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen2.5-VL-3B-Instruct-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen2.5-VL-7B-Instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen2.5-VL-7B-Instruct-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen2.5-VL-32B-Instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen2.5-VL-32B-Instruct-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen2.5-VL-72B-Instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen2.5-VL-72B-Instruct-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ],
    "updated_at": 1769418570,
    "featured": false,
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "model_type": "qwen2_5_vl",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 40960,
    "model_name": "qwen3",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "hybrid",
      "tools"
    ],
    "model_description": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_6",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-0.6B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-0.6B"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": "0_6",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-0.6B-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-0.6B-FP8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_6",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "Qwen/Qwen3-0.6B-GPTQ-Int8"
          },
          "modelscope": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "Qwen/Qwen3-0.6B-GPTQ-Int8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_6",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "JunHowie/Qwen3-0.6B-GPTQ-Int4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "JunHowie/Qwen3-0.6B-GPTQ-Int4"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "0_6",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-0.6B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-0.6B-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_6",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "model_id": "unsloth/Qwen3-0.6B-GGUF",
            "model_file_name_template": "Qwen3-0.6B-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "model_id": "unsloth/Qwen3-0.6B-GGUF",
            "model_file_name_template": "Qwen3-0.6B-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-1.7B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-1.7B"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": "1_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-1.7B-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-1.7B-FP8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "Qwen/Qwen3-1.7B-GPTQ-Int8"
          },
          "modelscope": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "Qwen/Qwen3-1.7B-GPTQ-Int8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "JunHowie/Qwen3-1.7B-GPTQ-Int4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "JunHowie/Qwen3-1.7B-GPTQ-Int4"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "1_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-1.7B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-1.7B-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_7",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "model_id": "unsloth/Qwen3-1.7B-GGUF",
            "model_file_name_template": "Qwen3-1.7B-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "model_id": "unsloth/Qwen3-1.7B-GGUF",
            "model_file_name_template": "Qwen3-1.7B-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-4B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-4B"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-4B-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-4B-FP8"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen3-4B-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen3-4B-AWQ"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "JunHowie/Qwen3-4B-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "JunHowie/Qwen3-4B-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-4B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-4B-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "model_id": "unsloth/Qwen3-4B-GGUF",
            "model_file_name_template": "Qwen3-4B-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "model_id": "unsloth/Qwen3-4B-GGUF",
            "model_file_name_template": "Qwen3-4B-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-8B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-8B"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-8B-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-8B-FP8"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen3-8B-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen3-8B-AWQ"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "JunHowie/Qwen3-8B-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "JunHowie/Qwen3-8B-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-8B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-8B-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "model_id": "unsloth/Qwen3-8B-GGUF",
            "model_file_name_template": "Qwen3-8B-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "model_id": "unsloth/Qwen3-8B-GGUF",
            "model_file_name_template": "Qwen3-8B-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-14B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-14B"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-14B-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-14B-FP8"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen3-14B-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen3-14B-AWQ"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "JunHowie/Qwen3-14B-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "JunHowie/Qwen3-14B-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-14B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-14B-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "model_id": "unsloth/Qwen3-14B-GGUF",
            "model_file_name_template": "Qwen3-14B-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "model_id": "unsloth/Qwen3-14B-GGUF",
            "model_file_name_template": "Qwen3-14B-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-30B-A3B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-30B-A3B"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-30B-A3B-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-30B-A3B-FP8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "JunHowie/Qwen3-30B-A3B-GPTQ-Int8"
          },
          "modelscope": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "JunHowie/Qwen3-30B-A3B-GPTQ-Int8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen3-30B-A3B-GPTQ-Int4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen3-30B-A3B-GPTQ-Int4"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-30B-A3B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-30B-A3B-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "unsloth/Qwen3-30B-A3B-GGUF",
            "model_file_name_template": "Qwen3-30B-A3B-{quantization}.gguf",
            "model_file_name_split_template": "BF16/Qwen3-30B-A3B-{quantization}-{part}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "unsloth/Qwen3-30B-A3B-GGUF",
            "model_file_name_template": "Qwen3-30B-A3B-{quantization}.gguf",
            "model_file_name_split_template": "BF16/Qwen3-30B-A3B-{quantization}-{part}.gguf"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-32B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-32B"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-32B-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-32B-FP8"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen3-32B-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen3-32B-AWQ"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "JunHowie/Qwen3-32B-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "JunHowie/Qwen3-32B-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-32B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-32B-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "unsloth/Qwen3-32B-GGUF",
            "model_file_name_template": "Qwen3-32B-{quantization}.gguf",
            "model_file_name_split_template": "BF16/Qwen3-32B-{quantization}-{part}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "unsloth/Qwen3-32B-GGUF",
            "model_file_name_template": "Qwen3-32B-{quantization}.gguf",
            "model_file_name_split_template": "BF16/Qwen3-32B-{quantization}-{part}.gguf"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-235B-A22B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-235B-A22B"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-235B-A22B-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-235B-A22B-FP8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "QuantTrio/Qwen3-235B-A22B-GPTQ-Int8"
          },
          "modelscope": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "tclf90/Qwen3-235B-A22B-GPTQ-Int8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen3-235B-A22B-GPTQ-Int4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Qwen/Qwen3-235B-A22B-GPTQ-Int4"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen/Qwen3-235B-A22B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-235B-A22B-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00010",
                "00002-of-00010",
                "00003-of-00010",
                "00004-of-00010",
                "00005-of-00010",
                "00006-of-00010",
                "00007-of-00010",
                "00008-of-00010",
                "00009-of-00010",
                "00010-of-00010"
              ],
              "IQ4_XS": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q2_K": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q2_K_L": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q3_K_S": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_0": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_1": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q5_K_M": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q6_K": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q8_0": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "UD-Q2_K_XL": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "UD-Q3_K_XL": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ]
            },
            "model_id": "unsloth/Qwen3-235B-A22B-GGUF",
            "model_file_name_template": "Qwen3-235B-A22B-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Qwen3-235B-A22B-{quantization}-{part}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q5_K_M",
              "Q6_K",
              "Q8_0",
              "BF16",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "IQ4_NL",
              "IQ4_XS"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00010",
                "00002-of-00010",
                "00003-of-00010",
                "00004-of-00010",
                "00005-of-00010",
                "00006-of-00010",
                "00007-of-00010",
                "00008-of-00010",
                "00009-of-00010",
                "00010-of-00010"
              ],
              "IQ4_XS": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q2_K": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q2_K_L": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q3_K_S": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_0": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_1": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q5_K_M": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q6_K": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q8_0": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "UD-Q2_K_XL": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "UD-Q3_K_XL": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ]
            },
            "model_id": "unsloth/Qwen3-235B-A22B-GGUF",
            "model_file_name_template": "Qwen3-235B-A22B-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Qwen3-235B-A22B-{quantization}-{part}.gguf"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in message.content %}\n                {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n                {%- set reasoning_content = message.content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '<|im_start|>' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- '<think>\\n\\n</think>\\n\\n' }}\n    {%- endif %}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    },
    "tool_parser": "qwen",
    "updated_at": 1769418571,
    "featured": true,
    "architectures": [
      "Qwen3ForCausalLM"
    ],
    "model_type": "qwen3"
  },
  {
    "version": 2,
    "context_length": 262144,
    "model_name": "Qwen3-Instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "We introduce the updated version of the Qwen3-235B-A22B non-thinking mode, named Qwen3-235B-A22B-Instruct-2507",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-235B-A22B-Instruct-2507"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-235B-A22B-Instruct-2507"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-30B-A3B-Instruct-2507"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-30B-A3B-Instruct-2507"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-4B-Instruct-2507"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-4B-Instruct-2507"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-235B-A22B-Instruct-2507-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-235B-A22B-Instruct-2507-FP8"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-4B-Instruct-2507-FP8"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-4B-Instruct-2507-FP8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "QuantTrio/Qwen3-235B-A22B-Instruct-2507-GPTQ-Int4-Int8Mix"
          },
          "modelscope": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "tclf90/Qwen3-235B-A22B-Instruct-2507-GPTQ-Int4-Int8Mix"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "QuantTrio/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8"
          },
          "modelscope": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "tclf90/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "JunHowie/Qwen3-4B-Instruct-2507-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "JunHowie/Qwen3-4B-Instruct-2507-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/Qwen3-235B-A22B-Instruct-2507-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/Qwen3-235B-A22B-Instruct-2507-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "cpatonn/Qwen3-30B-A3B-Instruct-2507-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "cpatonn-mirror/Qwen3-30B-A3B-Instruct-2507-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Eslzzyl/Qwen3-4B-Instruct-2507-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Eslzzyl/Qwen3-4B-Instruct-2507-AWQ"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-235B-A22B-Instruct-2507-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-235B-A22B-Instruct-2507-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-30B-A3B-Instruct-2507-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-30B-A3B-Instruct-2507-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-4B-Instruct-2507-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-4B-Instruct-2507-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "BF16",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00010",
                "00002-of-00010",
                "00003-of-00010",
                "00004-of-00010",
                "00005-of-00010",
                "00006-of-00010",
                "00007-of-00010",
                "00008-of-00010",
                "00009-of-00010",
                "00010-of-00010"
              ],
              "IQ4_XS": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q2_K": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q2_K_L": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q3_K_M": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q3_K_S": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_0": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_1": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_K_M": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_K_S": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q5_K_M": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q5_K_S": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q6_K": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q8_0": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "UD-Q2_K_XL": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "UD-Q3_K_XL": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "UD-Q4_K_XL": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "UD-Q5_K_XL": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "UD-Q6_K_XL": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "UD-Q8_K_XL": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ]
            },
            "model_id": "unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF",
            "model_file_name_template": "Qwen3-235B-A22B-Instruct-2507-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Qwen3-235B-A22B-Instruct-2507-{quantization}-{part}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "BF16",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00010",
                "00002-of-00010",
                "00003-of-00010",
                "00004-of-00010",
                "00005-of-00010",
                "00006-of-00010",
                "00007-of-00010",
                "00008-of-00010",
                "00009-of-00010",
                "00010-of-00010"
              ],
              "IQ4_XS": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q2_K": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q2_K_L": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q3_K_M": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q3_K_S": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_0": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_1": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_K_M": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_K_S": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q5_K_M": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q5_K_S": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q6_K": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q8_0": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "UD-Q2_K_XL": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "UD-Q3_K_XL": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "UD-Q4_K_XL": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "UD-Q5_K_XL": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "UD-Q6_K_XL": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "UD-Q8_K_XL": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ]
            },
            "model_id": "unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF",
            "model_file_name_template": "Qwen3-235B-A22B-Instruct-2507-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Qwen3-235B-A22B-Instruct-2507-{quantization}-{part}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "UD-TQ1_0"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF",
            "model_file_name_template": "Qwen3-30B-A3B-Instruct-2507-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Qwen3-30B-A3B-Instruct-2507-{quantization}-{part}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "UD-TQ1_0"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF",
            "model_file_name_template": "Qwen3-30B-A3B-Instruct-2507-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Qwen3-30B-A3B-Instruct-2507-{quantization}-{part}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL"
            ],
            "model_id": "unsloth/Qwen3-4B-Instruct-2507-GGUF",
            "model_file_name_template": "Qwen3-4B-Instruct-2507-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL"
            ],
            "model_id": "unsloth/Qwen3-4B-Instruct-2507-GGUF",
            "model_file_name_template": "Qwen3-4B-Instruct-2507-{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in content %}\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '<|im_start|>' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "tool_parser": "qwen",
    "updated_at": 1769418573,
    "featured": false,
    "architectures": [
      "Qwen3MoeForCausalLM"
    ],
    "model_type": "qwen3_moe",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 262144,
    "model_name": "Qwen3-Thinking",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "tools"
    ],
    "model_description": "we have continued to scale the thinking capability of Qwen3-235B-A22B, improving both the quality and depth of reasoning",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-4B-Thinking-2507"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-4B-Thinking-2507"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507-FP8"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-4B-Thinking-2507-FP8"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-4B-Thinking-2507-FP8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "QuantTrio/Qwen3-235B-A22B-Thinking-2507-GPTQ-Int4-Int8Mix"
          },
          "modelscope": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "tclf90/Qwen3-235B-A22B-Thinking-2507-GPTQ-Int4-Int8Mix"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "QuantTrio/Qwen3-30B-A3B-Thinking-2507-GPTQ-Int8"
          },
          "modelscope": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "tclf90/Qwen3-30B-A3B-Thinking-2507-GPTQ-Int8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "JunHowie/Qwen3-4B-Thinking-2507-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "Int8"
            ],
            "model_id": "JunHowie/Qwen3-4B-Thinking-2507-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/Qwen3-235B-A22B-Thinking-2507-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/Qwen3-235B-A22B-Thinking-2507-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/Qwen3-30B-A3B-Thinking-2507-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/Qwen3-30B-A3B-Thinking-2507-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Eslzzyl/Qwen3-4B-Thinking-2507-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Eslzzyl/Qwen3-4B-Thinking-2507-AWQ"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "5bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-235B-A22B-Thinking-2507-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "5bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-235B-A22B-Thinking-2507-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-30B-A3B-Thinking-2507-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-30B-A3B-Thinking-2507-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-4B-Thinking-2507-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-4B-Thinking-2507-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "BF16",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00010",
                "00002-of-00010",
                "00003-of-00010",
                "00004-of-00010",
                "00005-of-00010",
                "00006-of-00010",
                "00007-of-00010",
                "00008-of-00010",
                "00009-of-00010",
                "00010-of-00010"
              ],
              "IQ4_XS": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q2_K": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q2_K_L": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q3_K_M": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q3_K_S": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_0": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_1": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_K_M": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_K_S": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q5_K_M": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q5_K_S": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q6_K": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q8_0": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "UD-Q2_K_XL": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "UD-Q3_K_XL": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "UD-Q4_K_XL": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "UD-Q5_K_XL": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "UD-Q6_K_XL": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "UD-Q8_K_XL": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ]
            },
            "model_id": "unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF",
            "model_file_name_template": "Qwen3-235B-A22B-Thinking-2507-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Qwen3-235B-A22B-Thinking-2507-{quantization}-{part}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "BF16",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00010",
                "00002-of-00010",
                "00003-of-00010",
                "00004-of-00010",
                "00005-of-00010",
                "00006-of-00010",
                "00007-of-00010",
                "00008-of-00010",
                "00009-of-00010",
                "00010-of-00010"
              ],
              "IQ4_XS": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q2_K": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q2_K_L": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "Q3_K_M": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q3_K_S": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_0": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_1": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_K_M": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_K_S": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q5_K_M": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q5_K_S": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q6_K": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q8_0": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "UD-Q2_K_XL": [
                "00001-of-00002",
                "00002-of-00002"
              ],
              "UD-Q3_K_XL": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "UD-Q4_K_XL": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "UD-Q5_K_XL": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "UD-Q6_K_XL": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "UD-Q8_K_XL": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ]
            },
            "model_id": "unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF",
            "model_file_name_template": "Qwen3-235B-A22B-Thinking-2507-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Qwen3-235B-A22B-Thinking-2507-{quantization}-{part}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "UD-TQ1_0"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF",
            "model_file_name_template": "Qwen3-30B-A3B-Thinking-2507-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Qwen3-30B-A3B-Thinking-2507-{quantization}-{part}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "UD-TQ1_0"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF",
            "model_file_name_template": "Qwen3-30B-A3B-Thinking-2507-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Qwen3-30B-A3B-Thinking-2507-{quantization}-{part}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL"
            ],
            "model_id": "unsloth/Qwen3-4B-Thinking-2507-GGUF",
            "model_file_name_template": "Qwen3-4B-Thinking-2507-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL"
            ],
            "model_id": "unsloth/Qwen3-4B-Thinking-2507-GGUF",
            "model_file_name_template": "Qwen3-4B-Thinking-2507-{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in content %}\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '<|im_start|>' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n<think>\\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "tool_parser": "qwen",
    "updated_at": 1769418574,
    "featured": false,
    "architectures": [
      "Qwen3MoeForCausalLM"
    ],
    "model_type": "qwen3_moe",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 262144,
    "model_name": "Qwen3-Coder",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "we're announcing Qwen3-Coder, our most agentic code model to date",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 480,
        "activated_size_in_billions": 35,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-Coder-480B-A35B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-Coder-480B-A35B-Instruct"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-Coder-30B-A3B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-Coder-30B-A3B-Instruct"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 480,
        "activated_size_in_billions": 35,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 480,
        "activated_size_in_billions": 35,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "QuantTrio/Qwen3-Coder-480B-A35B-Instruct-GPTQ-Int4-Int8Mix"
          },
          "modelscope": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "tclf90/Qwen3-Coder-480B-A35B-Instruct-GPTQ-Int4-Int8Mix"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-GPTQ-Int8"
          },
          "modelscope": {
            "quantizations": [
              "Int8"
            ],
            "model_id": "tclf90/Qwen3-Coder-30B-A3B-Instruct-GPTQ-Int8"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 480,
        "activated_size_in_billions": 35,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/Qwen3-Coder-480B-A35B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/Qwen3-Coder-480B-A35B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/Qwen3-Coder-30B-A3B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 480,
        "activated_size_in_billions": 35,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen3-Coder-480B-A35B-Instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen3-Coder-480B-A35B-Instruct-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-Coder-30B-A3B-Instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-Coder-30B-A3B-Instruct-{quantization}"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 480,
        "activated_size_in_billions": 35,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00021",
                "00002-of-00021",
                "00003-of-00021",
                "00004-of-00021",
                "00005-of-00021",
                "00006-of-00021",
                "00007-of-00021",
                "00008-of-00021",
                "00009-of-00021",
                "00010-of-00021",
                "00011-of-00021",
                "00012-of-00021",
                "00013-of-00021",
                "00014-of-00021",
                "00015-of-00021",
                "00016-of-00021",
                "00017-of-00021",
                "00018-of-00021",
                "00019-of-00021",
                "00020-of-00021",
                "00021-of-00021"
              ],
              "IQ4_NL": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "IQ4_XS": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "Q2_K": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q2_K_L": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q3_K_M": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q3_K_S": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q4_0": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "Q4_1": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "Q4_K_M": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "Q4_K_S": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "Q5_K_M": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "Q5_K_S": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "Q6_K": [
                "00001-of-00009",
                "00002-of-00009",
                "00003-of-00009",
                "00004-of-00009",
                "00005-of-00009",
                "00006-of-00009",
                "00007-of-00009",
                "00008-of-00009",
                "00009-of-00009"
              ],
              "Q8_0": [
                "00001-of-00011",
                "00002-of-00011",
                "00003-of-00011",
                "00004-of-00011",
                "00005-of-00011",
                "00006-of-00011",
                "00007-of-00011",
                "00008-of-00011",
                "00009-of-00011",
                "00010-of-00011",
                "00011-of-00011"
              ],
              "UD-IQ3_XXS": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "UD-Q2_K_XL": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "UD-Q3_K_XL": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "UD-Q4_K_XL": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "UD-Q5_K_XL": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "UD-Q6_K_XL": [
                "00001-of-00009",
                "00002-of-00009",
                "00003-of-00009",
                "00004-of-00009",
                "00005-of-00009",
                "00006-of-00009",
                "00007-of-00009",
                "00008-of-00009",
                "00009-of-00009"
              ],
              "UD-Q8_K_XL": [
                "00001-of-00012",
                "00002-of-00012",
                "00003-of-00012",
                "00004-of-00012",
                "00005-of-00012",
                "00006-of-00012",
                "00007-of-00012",
                "00008-of-00012",
                "00009-of-00012",
                "00010-of-00012",
                "00011-of-00012",
                "00012-of-00012"
              ]
            },
            "model_id": "unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF",
            "model_file_name_template": "Qwen3-Coder-480B-A35B-Instruct-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Qwen3-Coder-480B-A35B-Instruct-{quantization}-{part}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00021",
                "00002-of-00021",
                "00003-of-00021",
                "00004-of-00021",
                "00005-of-00021",
                "00006-of-00021",
                "00007-of-00021",
                "00008-of-00021",
                "00009-of-00021",
                "00010-of-00021",
                "00011-of-00021",
                "00012-of-00021",
                "00013-of-00021",
                "00014-of-00021",
                "00015-of-00021",
                "00016-of-00021",
                "00017-of-00021",
                "00018-of-00021",
                "00019-of-00021",
                "00020-of-00021",
                "00021-of-00021"
              ],
              "IQ4_NL": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "IQ4_XS": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "Q2_K": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q2_K_L": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q3_K_M": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q3_K_S": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q4_0": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "Q4_1": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "Q4_K_M": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "Q4_K_S": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "Q5_K_M": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "Q5_K_S": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "Q6_K": [
                "00001-of-00009",
                "00002-of-00009",
                "00003-of-00009",
                "00004-of-00009",
                "00005-of-00009",
                "00006-of-00009",
                "00007-of-00009",
                "00008-of-00009",
                "00009-of-00009"
              ],
              "Q8_0": [
                "00001-of-00011",
                "00002-of-00011",
                "00003-of-00011",
                "00004-of-00011",
                "00005-of-00011",
                "00006-of-00011",
                "00007-of-00011",
                "00008-of-00011",
                "00009-of-00011",
                "00010-of-00011",
                "00011-of-00011"
              ],
              "UD-IQ3_XXS": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "UD-Q2_K_XL": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "UD-Q3_K_XL": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "UD-Q4_K_XL": [
                "00001-of-00006",
                "00002-of-00006",
                "00003-of-00006",
                "00004-of-00006",
                "00005-of-00006",
                "00006-of-00006"
              ],
              "UD-Q5_K_XL": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ],
              "UD-Q6_K_XL": [
                "00001-of-00009",
                "00002-of-00009",
                "00003-of-00009",
                "00004-of-00009",
                "00005-of-00009",
                "00006-of-00009",
                "00007-of-00009",
                "00008-of-00009",
                "00009-of-00009"
              ],
              "UD-Q8_K_XL": [
                "00001-of-00012",
                "00002-of-00012",
                "00003-of-00012",
                "00004-of-00012",
                "00005-of-00012",
                "00006-of-00012",
                "00007-of-00012",
                "00008-of-00012",
                "00009-of-00012",
                "00010-of-00012",
                "00011-of-00012",
                "00012-of-00012"
              ]
            },
            "model_id": "unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF",
            "model_file_name_template": "Qwen3-Coder-480B-A35B-Instruct-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Qwen3-Coder-480B-A35B-Instruct-{quantization}-{part}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "UD-TQ1_0"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF",
            "model_file_name_template": "Qwen3-Coder-30B-A3B-Instruct-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Qwen3-Coder-30B-A3B-Instruct-{quantization}-{part}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL",
              "UD-TQ1_0"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF",
            "model_file_name_template": "Qwen3-Coder-30B-A3B-Instruct-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Qwen3-Coder-30B-A3B-Instruct-{quantization}-{part}.gguf"
          }
        }
      }
    ],
    "chat_template": "{% macro render_extra_keys(json_dict, handled_keys) %}\n    {%- if json_dict is mapping %}\n        {%- for json_key in json_dict if json_key not in handled_keys %}\n            {%- if json_dict[json_key] is mapping %}\n                {{- '\\n<' ~ json_key ~ '>' ~ (json_dict[json_key] | tojson | safe) ~ '</' ~ json_key ~ '>' }}\n            {%- else %}\n                {{-'\\n<' ~ json_key ~ '>' ~ (json_dict[json_key] | string) ~ '</' ~ json_key ~ '>' }}\n            {%- endif %}\n        {%- endfor %}\n    {%- endif %}\n{% endmacro %}\n\n{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{%- if not tools is defined %}\n    {%- set tools = [] %}\n{%- endif %}\n\n{%- if system_message is defined %}\n    {{- \"<|im_start|>system\\n\" + system_message }}\n{%- else %}\n    {%- if tools is iterable and tools | length > 0 %}\n        {{- \"<|im_start|>system\\nYou are Qwen, a helpful AI assistant that can interact with a computer to solve tasks.\" }}\n    {%- endif %}\n{%- endif %}\n{%- if tools is iterable and tools | length > 0 %}\n    {{- \"\\n\\nYou have access to the following functions:\\n\\n\" }}\n    {{- \"<tools>\" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- \"\\n<function>\\n<name>\" ~ tool.name ~ \"</name>\" }}\n        {%- if tool.description is defined %}\n            {{- '\\n<description>' ~ (tool.description | trim) ~ '</description>' }}\n        {%- endif %}\n        {{- '\\n<parameters>' }}\n        {%- if tool.parameters is defined and tool.parameters is mapping and tool.parameters.properties is defined and tool.parameters.properties is mapping %}\n            {%- for param_name, param_fields in tool.parameters.properties|items %}\n                {{- '\\n<parameter>' }}\n                {{- '\\n<name>' ~ param_name ~ '</name>' }}\n                {%- if param_fields.type is defined %}\n                    {{- '\\n<type>' ~ (param_fields.type | string) ~ '</type>' }}\n                {%- endif %}\n                {%- if param_fields.description is defined %}\n                    {{- '\\n<description>' ~ (param_fields.description | trim) ~ '</description>' }}\n                {%- endif %}\n                {%- set handled_keys = ['name', 'type', 'description'] %}\n                {{- render_extra_keys(param_fields, handled_keys) }}\n                {{- '\\n</parameter>' }}\n            {%- endfor %}\n        {%- endif %}\n        {% set handled_keys = ['type', 'properties'] %}\n        {{- render_extra_keys(tool.parameters, handled_keys) }}\n        {{- '\\n</parameters>' }}\n        {%- set handled_keys = ['type', 'name', 'description', 'parameters'] %}\n        {{- render_extra_keys(tool, handled_keys) }}\n        {{- '\\n</function>' }}\n    {%- endfor %}\n    {{- \"\\n</tools>\" }}\n    {{- '\\n\\nIf you choose to call a function ONLY reply in the following format with NO suffix:\\n\\n<tool_call>\\n<function=example_function_name>\\n<parameter=example_parameter_1>\\nvalue_1\\n</parameter>\\n<parameter=example_parameter_2>\\nThis is the value for the second parameter\\nthat can span\\nmultiple lines\\n</parameter>\\n</function>\\n</tool_call>\\n\\n<IMPORTANT>\\nReminder:\\n- Function calls MUST follow the specified format: an inner <function=...></function> block must be nested within <tool_call></tool_call> XML tags\\n- Required parameters MUST be specified\\n- You may provide optional reasoning for your function call in natural language BEFORE the function call, but NOT after\\n- If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls\\n</IMPORTANT>' }}\n{%- endif %}\n{%- if system_message is defined %}\n    {{- '<|im_end|>\\n' }}\n{%- else %}\n    {%- if tools is iterable and tools | length > 0 %}\n        {{- '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if message.role == \"assistant\" and message.tool_calls is defined and message.tool_calls is iterable and message.tool_calls | length > 0 %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content is defined and message.content is string and message.content | trim | length > 0 %}\n            {{- '\\n' + message.content | trim + '\\n' }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n<function=' + tool_call.name + '>\\n' }}\n            {%- if tool_call.arguments is defined %}\n                {%- for args_name, args_value in tool_call.arguments|items %}\n                    {{- '<parameter=' + args_name + '>\\n' }}\n                    {%- set args_value = args_value | tojson | safe if args_value is mapping else args_value | string %}\n                    {{- args_value }}\n                    {{- '\\n</parameter>\\n' }}\n                {%- endfor %}\n            {%- endif %}\n            {{- '</function>\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"user\" or message.role == \"system\" or message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.previtem and loop.previtem.role != \"tool\" %}\n            {{- '<|im_start|>user\\n' }}\n        {%- endif %}\n        {{- '<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>\\n' }}\n        {%- if not loop.last and loop.nextitem.role != \"tool\" %}\n            {{- '<|im_end|>\\n' }}\n        {%- elif loop.last %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- else %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_end|>"
    ],
    "tool_parser": "qwen",
    "updated_at": 1769418575,
    "featured": false,
    "architectures": [
      "Qwen3MoeForCausalLM"
    ],
    "model_type": "qwen3_moe",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "seallms-v3",
    "model_lang": [
      "en",
      "zh",
      "id",
      "vi",
      "th",
      "ph",
      "ms",
      "mm",
      "kh",
      "la",
      "in"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "SeaLLMs - Large Language Models for Southeast Asia",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "SeaLLMs/SeaLLMs-v3-1.5B-Chat"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "SeaLLMs/SeaLLMs-v3-1.5B-Chat"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "SeaLLMs/SeaLLMs-v3-7B-Chat"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "SeaLLMs/SeaLLMs-v3-7B-Chat"
          }
        }
      }
    ],
    "chat_template": "{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\n' + system_message + '<|im_end|>\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\n' + content + '<|im_end|>\n<|im_start|>assistant\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\n' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "updated_at": 1769418576,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "skywork-or1",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "We release the final version of Skywork-OR1 (Open Reasoner 1) series of models, including",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Skywork/Skywork-OR1-32B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Skywork/Skywork-OR1-32B"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int8",
              "Int4"
            ],
            "model_id": "JunHowie/Skywork-OR1-32B-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int8",
              "Int4"
            ],
            "model_id": "JunHowie/Skywork-OR1-32B-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Skywork/Skywork-OR1-7B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Skywork/Skywork-OR1-7B"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int8",
              "Int4"
            ],
            "model_id": "JunHowie/Skywork-OR1-7B-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int8",
              "Int4"
            ],
            "model_id": "JunHowie/Skywork-OR1-7B-GPTQ-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}",
    "stop_token_ids": [
      151643
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "updated_at": 1769418577,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "skywork-or1-preview",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Skywork-OR1 (Open Reasoner 1) model series consists of powerful math and code reasoning models trained using large-scale rule-based reinforcement learning with carefully designed datasets and training recipes.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Skywork/Skywork-OR1-32B-Preview"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Skywork/Skywork-OR1-32B-Preview"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4",
              "int8"
            ],
            "model_id": "JunHowie/Skywork-OR1-32B-Preview-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int4",
              "int8"
            ],
            "model_id": "JunHowie/Skywork-OR1-32B-Preview-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Skywork/Skywork-OR1-7B-Preview"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Skywork/Skywork-OR1-7B-Preview"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "IQ2_M",
              "IQ2_S",
              "IQ2_XS",
              "IQ3_M",
              "IQ3_XS",
              "IQ3_XXS",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q3_K_XL",
              "Q4_0",
              "Q4_1",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0"
            ],
            "model_id": "bartowski/Skywork_Skywork-OR1-32B-Preview-GGUF",
            "model_file_name_template": "Skywork_Skywork-OR1-32B-Preview-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "IQ2_M",
              "IQ2_S",
              "IQ2_XS",
              "IQ3_M",
              "IQ3_XS",
              "IQ3_XXS",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q3_K_XL",
              "Q4_0",
              "Q4_1",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0"
            ],
            "model_id": "bartowski/Skywork_Skywork-OR1-32B-Preview-GGUF",
            "model_file_name_template": "Skywork_Skywork-OR1-32B-Preview-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "IQ2_M",
              "IQ2_S",
              "IQ2_XS",
              "IQ3_M",
              "IQ3_XS",
              "IQ3_XXS",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q3_K_XL",
              "Q4_0",
              "Q4_1",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0"
            ],
            "model_id": "bartowski/Skywork_Skywork-OR1-7B-Preview-GGUF",
            "model_file_name_template": "Skywork_Skywork-OR1-7B-Preview-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "IQ2_M",
              "IQ2_S",
              "IQ2_XS",
              "IQ3_M",
              "IQ3_XS",
              "IQ3_XXS",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q3_K_XL",
              "Q4_0",
              "Q4_1",
              "Q4_K_L",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_L",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q6_K_L",
              "Q8_0"
            ],
            "model_id": "bartowski/Skywork_Skywork-OR1-7B-Preview-GGUF",
            "model_file_name_template": "Skywork_Skywork-OR1-7B-Preview-{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "updated_at": 1769418577,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 8192,
    "model_name": "telechat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The TeleChat is a large language model developed and trained by China Telecom Artificial Intelligence Technology Co., LTD. The 7B model base is trained with 1.5 trillion Tokens and 3 trillion Tokens and Chinese high-quality corpus.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Tele-AI/telechat-7B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "TeleAI/telechat-7B",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "TeleAI/TeleChat-7B-pt"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "int4",
              "int8"
            ],
            "model_id": "Tele-AI/telechat-7B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "int4",
              "int8"
            ],
            "model_id": "TeleAI/telechat-7B-{quantization}",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Tele-AI/TeleChat-12B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "TeleAI/TeleChat-12B",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "TeleAI/TeleChat-12B-pt"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "int4",
              "int8"
            ],
            "model_id": "Tele-AI/TeleChat-12B-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "int4",
              "int8"
            ],
            "model_id": "TeleAI/TeleChat-12B-{quantization}",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 52,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Tele-AI/TeleChat-52B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "TeleAI/TeleChat-52B",
            "model_revision": "master"
          },
          "openmind_hub": {
            "quantizations": [
              "none"
            ],
            "model_id": "TeleAI/TeleChat-52B-pt"
          }
        }
      }
    ],
    "chat_template": "{{ (messages|selectattr('role', 'equalto', 'system')|list|last).content|trim if (messages|selectattr('role', 'equalto', 'system')|list) else '' }}{%- for message in messages -%}{%- if message['role'] == 'user' -%}{{- '<_user>' + message['content'] +'<_bot>' -}}{%- elif message['role'] == 'assistant' -%}{{- message['content'] + '<_end>' -}}{%- endif -%}{%- endfor -%}",
    "stop": [
      "<_end>",
      "<_start>"
    ],
    "stop_token_ids": [
      160133,
      160132
    ],
    "updated_at": 1769418578,
    "featured": false,
    "architectures": [
      "TelechatForCausalLM"
    ],
    "model_type": "telechat",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 2048,
    "model_name": "tiny-llama",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 1,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF",
            "model_file_name_template": "tinyllama-1.1b-chat-v0.3.{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K"
            ],
            "model_id": "Xorbits/TinyLlama-1.1B-step-50K-105b-GGUF",
            "model_revision": "v0.0.1",
            "model_file_name_template": "ggml-model-{quantization}.gguf"
          }
        }
      }
    ],
    "updated_at": 1769418579,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "tinyllama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 100000,
    "model_name": "wizardcoder-python-v1.0",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "WizardLMTeam/WizardCoder-Python-13B-V1.0",
            "model_revision": "5ac6748b1f5a4c282107ddc7d3b69fdc4a686d75"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-ModelScope/WizardCoder-Python-13B-V1.0",
            "model_revision": "v1.0.0"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "WizardLMTeam/WizardCoder-Python-34B-V1.0",
            "model_revision": "897fc6d9e12136c68c441b2350d015902c144b20"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "AI-ModelScope/WizardCoder-Python-34B-V1.0",
            "model_revision": "v1.0.0"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/WizardCoder-Python-7B-V1.0-GGUF",
            "model_file_name_template": "wizardcoder-python-7b-v1.0.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/WizardCoder-Python-13B-V1.0-GGUF",
            "model_file_name_template": "wizardcoder-python-13b-v1.0.{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q3_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_0",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0"
            ],
            "model_id": "TheBloke/WizardCoder-Python-34B-V1.0-GGUF",
            "model_file_name_template": "wizardcoder-python-34b-v1.0.{quantization}.gguf"
          }
        }
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ item['content'] + '\n\n### ' }}{% elif loop.first %}{{ 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### ' }}{% endif %}{% if item['role'] == 'user' %}{{ 'Instruction: ' + item['content'] + '\n\n### ' }}{% elif item['role'] == 'assistant' %}{{ 'Response: ' + item['content'] + '\n\n### ' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Response: Let\\'s think step by step.' }}{% endif %}",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ],
    "updated_at": 1769418580,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 2048,
    "model_name": "wizardmath-v1.0",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "WizardMath is an open-source LLM trained by fine-tuning Llama2 with Evol-Instruct, specializing in math.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "WizardLMTeam/WizardMath-7B-V1.0",
            "model_revision": "825a586f260d6c583b8aa9ceab6cdfaa3d9a4ddc"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Xorbits/WizardMath-7B-V1.0",
            "model_revision": "v1.0.0"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "WizardLMTeam/WizardMath-70B-V1.0",
            "model_revision": "4dd9f3fcd8c056561d67ec59ae011f7c146aebd2"
          }
        }
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ item['content'] + '\n\n### ' }}{% elif loop.first %}{{ 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### ' }}{% endif %}{% if item['role'] == 'user' %}{{ 'Instruction: ' + item['content'] + '\n\n### ' }}{% elif item['role'] == 'assistant' %}{{ 'Response: ' + item['content'] + '\n\n### ' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Response: Let\\'s think step by step.' }}{% endif %}",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ],
    "updated_at": 1769418581,
    "featured": false,
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 2048,
    "model_name": "xverse",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "XVERSE is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "xverse/XVERSE-7B",
            "model_revision": "3778b254def675586e9218ccb15b78d6ef66a3a7"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "xverse/XVERSE-7B",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "xverse/XVERSE-13B",
            "model_revision": "11ac840dda17af81046614229fdd0c658afff747"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "xverse/XVERSE-13B",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 65,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "xverse/XVERSE-65B",
            "model_revision": "7f1b7394f74c630f50612a19ba90bd021c373989"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "xverse/XVERSE-65B",
            "model_revision": "master"
          }
        }
      }
    ],
    "updated_at": 1769418582,
    "featured": false,
    "architectures": [
      "XverseForCausalLM"
    ],
    "model_type": "xverse",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 2048,
    "model_name": "xverse-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "XVERSEB-Chat is the aligned version of model XVERSE.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "xverse/XVERSE-7B-Chat",
            "model_revision": "60acc8c453c067b54df88be98bfdf60585ab5441"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "xverse/XVERSE-7B-Chat",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "xverse/XVERSE-13B-Chat",
            "model_revision": "1e4944aaa1d8c8d0cdca28bb8e3a003303d0781b"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "xverse/XVERSE-13B-Chat",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ '<|system|> \n' + item['content'] }}{% endif %}{% if item['role'] == 'user' %}{{ '<|user|> \n' + item['content'] }}{% elif item['role'] == 'assistant' %}{{ '<|assistant|> \n' + item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% endif %}",
    "stop_token_ids": [
      3
    ],
    "stop": [
      "<|endoftext|>"
    ],
    "updated_at": 1769418583,
    "featured": false,
    "architectures": [
      "XverseForCausalLM"
    ],
    "model_type": "xverse",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "qwenLong-l1",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Tongyi-Zhiwen/QwenLong-L1-32B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "iic/QwenLong-L1-32B"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "Tongyi-Zhiwen/QwenLong-L1-32B-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "iic/QwenLong-L1-32B-AWQ"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}",
    "stop_token_ids": [
      151643
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "updated_at": 1769418584,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "Ernie4.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "ERNIE 4.5, a new family of large-scale multimodal models comprising 10 distinct variants.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_3",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "baidu/ERNIE-4.5-0.3B-PT"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "PaddlePaddle/ERNIE-4.5-0.3B-PT"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_3",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "F16"
            ],
            "model_id": "unsloth/ERNIE-4.5-0.3B-PT-GGUF",
            "model_file_name_template": "ERNIE-4.5-0.3B-PT-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "F16"
            ],
            "model_id": "unsloth/ERNIE-4.5-0.3B-PT-GGUF",
            "model_file_name_template": "ERNIE-4.5-0.3B-PT-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "0_3",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "bf16"
            ],
            "model_id": "mlx-community/ERNIE-4.5-0.3B-PT-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "bf16"
            ],
            "model_id": "mlx-community/ERNIE-4.5-0.3B-PT-{quantization}"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 21,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "baidu/ERNIE-4.5-21B-A3B-Base-PT"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "PaddlePaddle/ERNIE-4.5-21B-A3B-Base-PT"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 21,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "BF16"
            ],
            "model_id": "unsloth/ERNIE-4.5-21B-A3B-PT-GGUF",
            "model_file_name_template": "ERNIE-4.5-21B-A3B-PT-{quantization}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "BF16"
            ],
            "model_id": "unsloth/ERNIE-4.5-21B-A3B-PT-GGUF",
            "model_file_name_template": "ERNIE-4.5-21B-A3B-PT-{quantization}.gguf"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 21,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "5bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/ERNIE-4.5-21B-A3B-PT-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "5bit",
              "6bit",
              "8bit",
              "bf16"
            ],
            "model_id": "mlx-community/ERNIE-4.5-21B-A3B-PT-{quantization}"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 300,
        "activated_size_in_billions": 47,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "baidu/ERNIE-4.5-300B-A47B-PT"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "PaddlePaddle/ERNIE-4.5-300B-A47B-PT"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 300,
        "activated_size_in_billions": 47,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Q2_K",
              "Q4_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "quantization_parts": {
              "Q2_K": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_K_M": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q6_K": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q8_0": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ]
            },
            "model_id": "unsloth/ERNIE-4.5-300B-A47B-PT-GGUF",
            "model_file_name_template": "ERNIE-4.5-0.3B-PT-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/ERNIE-4.5-300B-A47B-PT-{quantization}-{part}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "Q2_K",
              "Q4_K_M",
              "Q6_K",
              "Q8_0"
            ],
            "quantization_parts": {
              "Q2_K": [
                "00001-of-00003",
                "00002-of-00003",
                "00003-of-00003"
              ],
              "Q4_K_M": [
                "00001-of-00004",
                "00002-of-00004",
                "00003-of-00004",
                "00004-of-00004"
              ],
              "Q6_K": [
                "00001-of-00005",
                "00002-of-00005",
                "00003-of-00005",
                "00004-of-00005",
                "00005-of-00005"
              ],
              "Q8_0": [
                "00001-of-00007",
                "00002-of-00007",
                "00003-of-00007",
                "00004-of-00007",
                "00005-of-00007",
                "00006-of-00007",
                "00007-of-00007"
              ]
            },
            "model_id": "unsloth/ERNIE-4.5-300B-A47B-PT-GGUF",
            "model_file_name_template": "ERNIE-4.5-0.3B-PT-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/ERNIE-4.5-300B-A47B-PT-{quantization}-{part}.gguf"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 300,
        "activated_size_in_billions": 47,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/ERNIE-4.5-300B-47B-PT-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/ERNIE-4.5-300B-47B-PT-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{%- if not add_generation_prompt is defined -%}\n    {%- set add_generation_prompt = true -%}\n{%- endif -%}\n{%- if not cls_token is defined -%}\n    {%- set cls_token = \"<|begin_of_sentence|>\" -%}\n{%- endif -%}\n{%- if not sep_token is defined -%}\n    {%- set sep_token = \"<|end_of_sentence|>\" -%}\n{%- endif -%}\n{{- cls_token -}}\n{%- for message in messages -%}\n    {%- if message[\"role\"] == \"user\" -%}\n        {{- \"User: \" + message[\"content\"] + \"\n\" -}}\n    {%- elif message[\"role\"] == \"assistant\" -%}\n        {{- \"Assistant: \" + message[\"content\"] + sep_token -}}\n    {%- elif message[\"role\"] == \"system\" -%}\n        {{- message[\"content\"] + \"\n\" -}}\n    {%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{- \"Assistant: \" -}}\n{%- endif -%}",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ],
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "mlx-lm>=0.25.2 ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy#"
      ]
    },
    "updated_at": 1769418585,
    "featured": true,
    "architectures": [
      "Ernie4_5ForCausalLM"
    ],
    "model_type": "ernie4_5"
  },
  {
    "version": 2,
    "context_length": 65536,
    "model_name": "glm-4.1v-thinking",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision",
      "reasoning",
      "tools"
    ],
    "model_description": "GLM-4.1V-9B-Thinking, designed to explore the upper limits of reasoning in vision-language models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/GLM-4.1V-9B-Thinking",
            "model_revision": "b627c82cd8fc9175ff2b82b33fb439eba260055f"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/GLM-4.1V-9B-Thinking",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/GLM-4.1V-9B-Thinking-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/GLM-4.1V-9B-Thinking-AWQ",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 9,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "QuantTrio/GLM-4.1V-9B-Thinking-GPTQ-Int4-Int8Mix"
          },
          "modelscope": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "tclf90/GLM-4.1V-9B-Thinking-GPTQ-Int4-Int8Mix",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "[gMASK]<sop> {%- for msg in messages %} {%- if msg.role == 'system' %} <|system|> {{ msg.content }} {%- elif msg.role == 'user' %} <|user|>{{ '\n' }} {%- if msg.content is string %} {{ msg.content }} {%- else %} {%- for item in msg.content %} {%- if item.type == 'video' or 'video' in item %} <|begin_of_video|><|video|><|end_of_video|> {%- elif item.type == 'image' or 'image' in item %} <|begin_of_image|><|image|><|end_of_image|> {%- elif item.type == 'text' %} {{ item.text }} {%- endif %} {%- endfor %} {%- endif %} {%- elif msg.role == 'assistant' %} {%- if msg.metadata %} <|assistant|>{{ msg.metadata }} {{ msg.content }} {%- else %} <|assistant|> {{ msg.content }} {%- endif %} {%- endif %} {%- endfor %} {% if add_generation_prompt %}<|assistant|> {% endif %}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "virtualenv": {
      "packages": [
        "transformers>=4.53.2 ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#system_numpy# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"Transformers\""
      ]
    },
    "featured": false,
    "architectures": [
      "Glm4vForConditionalGeneration"
    ],
    "updated_at": 1769418605,
    "model_type": "glm4v"
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "glm-4.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "hybrid",
      "tools"
    ],
    "model_description": "The GLM-4.5 series models are foundation models designed for intelligent agents. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 355,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/GLM-4.5"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/GLM-4.5"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 355,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "FP8"
            ],
            "model_id": "zai-org/GLM-4.5-FP8"
          },
          "modelscope": {
            "quantizations": [
              "FP8"
            ],
            "model_id": "ZhipuAI/GLM-4.5-FP8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 355,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "QuantTrio/GLM-4.5-GPTQ-Int4-Int8Mix"
          },
          "modelscope": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "tclf90/GLM-4.5-GPTQ-Int4-Int8Mix"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 355,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/GLM-4.5-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/GLM-4.5-AWQ"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 355,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/GLM-4.5-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/GLM-4.5-{quantization}"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 106,
        "activated_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/GLM-4.5-Air"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/GLM-4.5-Air"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 106,
        "activated_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "FP8"
            ],
            "model_id": "zai-org/GLM-4.5-Air-FP8"
          },
          "modelscope": {
            "quantizations": [
              "FP8"
            ],
            "model_id": "ZhipuAI/GLM-4.5-Air-FP8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 106,
        "activated_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "QuantTrio/GLM-4.5-Air-GPTQ-Int4-Int8Mix"
          },
          "modelscope": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "tclf90/GLM-4.5-Air-GPTQ-Int4-Int8Mix"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 106,
        "activated_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "AWQ-FP16Mix"
            ],
            "model_id": "QuantTrio/GLM-4.5-Air-AWQ-FP16Mix"
          },
          "modelscope": {
            "quantizations": [
              "AWQ-FP16Mix"
            ],
            "model_id": "tclf90/GLM-4.5-Air-AWQ-FP16Mix"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 106,
        "activated_size_in_billions": 12,
        "model_src": {
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "cpatonn-mirror/GLM-4.5-Air-AWQ-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 106,
        "activated_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "2bit",
              "3bit",
              "4bit",
              "5bit",
              "8bit"
            ],
            "model_id": "mlx-community/GLM-4.5-Air-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "2bit",
              "3bit",
              "4bit",
              "5bit",
              "8bit"
            ],
            "model_id": "mlx-community/GLM-4.5-Air-{quantization}"
          }
        }
      }
    ],
    "chat_template": "[gMASK]<sop>{%- if tools -%}<|system|># Tools You may call one or more functions to assist with the user query. You are provided with function signatures within <tools></tools> XML tags:<tools>{% for tool in tools %}{{ tool | tojson(ensure_ascii=False) }}{% endfor %}</tools>For each function call, output the function name and arguments within the following XML format:<tool_call>{function-name}<arg_key>{arg-key-1}</arg_key><arg_value>{arg-value-1}</arg_value><arg_key>{arg-key-2}</arg_key><arg_value>{arg-value-2}</arg_value>...</tool_call>{%- endif -%}{%- macro visible_text(content) -%}{%- if content is string -%}{{- content }}{%- elif content is iterable and content is not mapping -%}{%- for item in content -%}{%- if item is mapping and item.type == 'text' -%}{{- item.text }}{%- elif item is string -%}{{- item }}{%- endif -%}{%- endfor -%}{%- else -%}{{- content }}{%- endif -%}{%- endmacro -%}{%- set ns = namespace(last_user_index=-1) %}{% for m in messages %}{%- if m.role == 'user' %}{% set ns.last_user_index = loop.index0 -%}{%- endif %}{%- endfor %}{% for m in messages %}{%- if m.role == 'user' -%}<|user|>{{ visible_text(m.content) }}{{- '/nothink' if (enable_thinking is defined and not enable_thinking and not visible_text(m.content).endswith(\"/nothink\")) else '' -}}{%- elif m.role == 'assistant' -%}<|assistant|>{%- set reasoning_content = '' %}{%- set content = visible_text(m.content) %}{%- if m.reasoning_content is string %}{%- set reasoning_content = m.reasoning_content %}{%- else %}{%- if '</think>' in content %}{%- set reasoning_content = content.split('</think>')[0].rstrip('\n').split('<think>')[-1].lstrip('\n') %}{%- set content = content.split('</think>')[-1].lstrip('\n') %}{%- endif %}{%- endif %}{%- if loop.index0 > ns.last_user_index and reasoning_content -%}{{ '\n<think>' + reasoning_content.strip() +  '</think>'}}{%- else -%}{{ '\n<think></think>' }}{%- endif %}{{ '\n' + content.strip() if content.strip() }}{% if m.tool_calls %}{% for tc in m.tool_calls %}{%- if tc.function %}{%- set tc = tc.function %}{%- endif %}{{ '\n<tool_call>' + tc.name }}{% set _args = tc.arguments %}{% for k, v in _args.items() %}<arg_key>{{ k }}</arg_key><arg_value>{{ v | tojson(ensure_ascii=False) if v is not string else v }}</arg_value>{% endfor %}</tool_call>{% endfor %}{% endif %}{%- elif m.role == 'tool' -%}{%- if m.content is string -%}{%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}{{- '<|observation|>' }}{%- endif %}{{- '\n<tool_response>\n' }}{{- m.content }}{{- '\n</tool_response>' }}{%- else -%}<|observation|>{% for tr in m.content %}<tool_response>{{ tr.output if tr.output is defined else tr }}</tool_response>{% endfor -%}{% endif -%}{%- elif m.role == 'system' -%}<|system|>{{ visible_text(m.content) }}{%- endif -%}{%- endfor %}{%- if add_generation_prompt -%}<|assistant|>{{- '\n<think></think>' if (enable_thinking is defined and not enable_thinking) else '' -}}{%- endif -%}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ],
    "tool_parser": "glm4",
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    },
    "featured": true,
    "architectures": [
      "Glm4MoeForCausalLM"
    ],
    "updated_at": 1769418586,
    "model_type": "glm4_moe"
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "glm-4.5v",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision",
      "reasoning",
      "tools"
    ],
    "model_description": "GLM-4.5V is based on ZhipuAI’s next-generation flagship text foundation model GLM-4.5-Air (106B parameters, 12B active). It continues the technical approach of GLM-4.1V-Thinking, achieving SOTA performance among models of the same scale on 42 public vision-language benchmarks.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 106,
        "activated_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/GLM-4.5V"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/GLM-4.5V"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 106,
        "activated_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "FP8"
            ],
            "model_id": "zai-org/GLM-4.5V-FP8"
          },
          "modelscope": {
            "quantizations": [
              "FP8"
            ],
            "model_id": "ZhipuAI/GLM-4.5V-FP8"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 106,
        "activated_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/GLM-4.5V-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/GLM-4.5V-AWQ"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 106,
        "activated_size_in_billions": 12,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit",
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/GLM-4.5V-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "3bit",
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/GLM-4.5V-{quantization}"
          }
        }
      }
    ],
    "chat_template": "[gMASK]<sop>\n{%- if tools -%}\n<|system|>\n# Tools\nYou may call one or more functions to assist with the user query.\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>\n{% for tool in tools %}\n{{ tool | tojson(ensure_ascii=False) }}\n{% endfor %}\n</tools>\nFor each function call, output the function name and arguments within the following XML format:\n<tool_call>{function-name}\n<arg_key>{arg-key-1}</arg_key>\n<arg_value>{arg-value-1}</arg_value>\n<arg_key>{arg-key-2}</arg_key>\n<arg_value>{arg-value-2}</arg_value>\n...\n</tool_call>{%- endif -%}\n{%- macro visible_text(content) -%}\n    {%- if content is string -%}\n        {{- content }}\n    {%- elif content is iterable and content is not mapping -%}\n        {%- for item in content -%}\n            {%- if item is mapping and item.type == 'text' -%}\n                {{- item.text }}\n            {%- elif item is mapping and (item.type == 'image' or 'image' in item) -%}\n                <|begin_of_image|><|image|><|end_of_image|>\n            {%- elif item is mapping and (item.type == 'video' or 'video' in item) -%}\n                <|begin_of_video|><|video|><|end_of_video|>\n            {%- elif item is string -%}\n                {{- item }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{- content }}\n    {%- endif -%}\n{%- endmacro -%}\n{%- set ns = namespace(last_user_index=-1) %}\n{%- for m in messages %}\n    {%- if m.role == 'user' %}\n        {% set ns.last_user_index = loop.index0 -%}\n    {%- endif %}\n{%- endfor %}\n{% for m in messages %}\n{%- if m.role == 'user' -%}<|user|>\n{% if m.content is string %}\n{{ m.content }}\n{%- else %}\n{%- for item in m.content %}\n{% if item.type == 'video' or 'video' in item %}\n<|begin_of_video|><|video|><|end_of_video|>{% elif item.type == 'image' or 'image' in item %}\n<|begin_of_image|><|image|><|end_of_image|>{% elif item.type == 'text' %}\n{{ item.text }}\n{%- endif %}\n{%- endfor %}\n{%- endif %}\n{{- '/nothink' if (enable_thinking is defined and not enable_thinking and not visible_text(m.content).endswith(\"/nothink\")) else '' -}}\n{%- elif m.role == 'assistant' -%}\n<|assistant|>\n{%- set reasoning_content = '' %}\n{%- set content = visible_text(m.content) %}\n{%- if m.reasoning_content is string %}\n    {%- set reasoning_content = m.reasoning_content %}\n{%- else %}\n    {%- if '</think>' in content %}\n        {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n        {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n    {%- endif %}\n{%- endif %}\n{%- if loop.index0 > ns.last_user_index and reasoning_content -%}\n{{ '\\n<think>' + reasoning_content.strip() +  '</think>'}}\n{%- else -%}\n{{ '\\n<think></think>' }}\n{%- endif -%}\n{%- if content.strip() -%}\n{{ '\\n' + content.strip() }}\n{%- endif -%}\n{% if m.tool_calls %}\n{% for tc in m.tool_calls %}\n{%- if tc.function %}\n    {%- set tc = tc.function %}\n{%- endif %}\n{{ '\\n<tool_call>' + tc.name }}\n{% set _args = tc.arguments %}\n{% for k, v in _args.items() %}\n<arg_key>{{ k }}</arg_key>\n<arg_value>{{ v | tojson(ensure_ascii=False) if v is not string else v }}</arg_value>\n{% endfor %}\n</tool_call>{% endfor %}\n{% endif %}\n{%- elif m.role == 'tool' -%}\n{%- if m.content is string -%}\n{%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n    {{- '<|observation|>' }}\n{%- endif %}\n{{- '\\n<tool_response>\\n' }}\n{{- m.content }}\n{{- '\\n</tool_response>' }}\n{%- else -%}\n<|observation|>{% for tr in m.content %}\n<tool_response>\n{{ tr.output if tr.output is defined else tr }}\n</tool_response>{% endfor -%}\n{% endif -%}\n{%- elif m.role == 'system' -%}\n<|system|>\n{{ visible_text(m.content) }}\n{%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n<|assistant|>\n{{'<think></think>\\n' if (enable_thinking is defined and not enable_thinking) else ''}}\n{%- endif -%}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ],
    "tool_parser": "glm4",
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    },
    "featured": false,
    "architectures": [
      "Glm4vMoeForConditionalGeneration"
    ],
    "updated_at": 1769418587,
    "model_type": "glm4v_moe"
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "gpt-oss",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat",
      "reasoning"
    ],
    "model_description": "gpt-oss series, OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "activated_size_in_billions": "3_6",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "openai/gpt-oss-20b"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "openai-mirror/gpt-oss-20b"
          }
        }
      },
      {
        "model_format": "bnb",
        "model_size_in_billions": 20,
        "activated_size_in_billions": "3_6",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4-bit"
            ],
            "model_id": "unsloth/gpt-oss-20b-bnb-4bit"
          },
          "modelscope": {
            "quantizations": [
              "4-bit"
            ],
            "model_id": "unsloth/gpt-oss-20b-bnb-4bit"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 120,
        "activated_size_in_billions": "5_1",
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "openai/gpt-oss-120b"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "openai-mirror/gpt-oss-120b"
          }
        }
      }
    ],
    "chat_template": "{#-\n  In addition to the normal inputs of `messages` and `tools`, this template also accepts the\n  following kwargs:\n  - \"builtin_tools\": A list, can contain \"browser\" and/or \"python\".\n  - \"model_identity\": A string that optionally describes the model identity.\n  - \"reasoning_effort\": A string that describes the reasoning effort, defaults to \"medium\".\n #}\n\n{#- Tool Definition Rendering ============================================== #}\n{%- macro render_typescript_type(param_spec, required_params, is_nullable=false) -%}\n    {%- if param_spec.type == \"array\" -%}\n        {%- if param_spec['items'] -%}\n            {%- if param_spec['items']['type'] == \"string\" -%}\n                {{- \"string[]\" }}\n            {%- elif param_spec['items']['type'] == \"number\" -%}\n                {{- \"number[]\" }}\n            {%- elif param_spec['items']['type'] == \"integer\" -%}\n                {{- \"number[]\" }}\n            {%- elif param_spec['items']['type'] == \"boolean\" -%}\n                {{- \"boolean[]\" }}\n            {%- else -%}\n                {%- set inner_type = render_typescript_type(param_spec['items'], required_params) -%}\n                {%- if inner_type == \"object | object\" or inner_type|length > 50 -%}\n                    {{- \"any[]\" }}\n                {%- else -%}\n                    {{- inner_type + \"[]\" }}\n                {%- endif -%}\n            {%- endif -%}\n            {%- if param_spec.nullable -%}\n                {{- \" | null\" }}\n            {%- endif -%}\n        {%- else -%}\n            {{- \"any[]\" }}\n            {%- if param_spec.nullable -%}\n                {{- \" | null\" }}\n            {%- endif -%}\n        {%- endif -%}\n    {%- elif param_spec.type is defined and param_spec.type is iterable and param_spec.type is not string and param_spec.type is not mapping and param_spec.type[0] is defined -%}\n        {#- Handle array of types like [\"object\", \"object\"] from Union[dict, list] #}\n        {%- if param_spec.type | length > 1 -%}\n            {{- param_spec.type | join(\" | \") }}\n        {%- else -%}\n            {{- param_spec.type[0] }}\n        {%- endif -%}\n    {%- elif param_spec.oneOf -%}\n        {#- Handle oneOf schemas - check for complex unions and fallback to any #}\n        {%- set has_object_variants = false -%}\n        {%- for variant in param_spec.oneOf -%}\n            {%- if variant.type == \"object\" -%}\n                {%- set has_object_variants = true -%}\n            {%- endif -%}\n        {%- endfor -%}\n        {%- if has_object_variants and param_spec.oneOf|length > 1 -%}\n            {{- \"any\" }}\n        {%- else -%}\n            {%- for variant in param_spec.oneOf -%}\n                {{- render_typescript_type(variant, required_params) -}}\n                {%- if variant.description %}\n                    {{- \"// \" + variant.description }}\n                {%- endif -%}\n                {%- if variant.default is defined %}\n                    {{ \"// default: \" + variant.default|tojson }}\n                {%- endif -%}\n                {%- if not loop.last %}\n                    {{- \" | \" }}\n                {% endif -%}\n            {%- endfor -%}\n        {%- endif -%}\n    {%- elif param_spec.type == \"string\" -%}\n        {%- if param_spec.enum -%}\n            {{- '\"' + param_spec.enum|join('\" | \"') + '\"' -}}\n        {%- else -%}\n            {{- \"string\" }}\n            {%- if param_spec.nullable %}\n                {{- \" | null\" }}\n            {%- endif -%}\n        {%- endif -%}\n    {%- elif param_spec.type == \"number\" -%}\n        {{- \"number\" }}\n    {%- elif param_spec.type == \"integer\" -%}\n        {{- \"number\" }}\n    {%- elif param_spec.type == \"boolean\" -%}\n        {{- \"boolean\" }}\n\n    {%- elif param_spec.type == \"object\" -%}\n        {%- if param_spec.properties -%}\n            {{- \"{\n\" }}\n            {%- for prop_name, prop_spec in param_spec.properties.items() -%}\n                {{- prop_name -}}\n                {%- if prop_name not in (param_spec.required or []) -%}\n                    {{- \"?\" }}\n                {%- endif -%}\n                {{- \": \" }}\n                {{ render_typescript_type(prop_spec, param_spec.required or []) }}\n                {%- if not loop.last -%}\n                    {{-\", \" }}\n                {%- endif -%}\n            {%- endfor -%}\n            {{- \"}\" }}\n        {%- else -%}\n            {{- \"object\" }}\n        {%- endif -%}\n    {%- else -%}\n        {{- \"any\" }}\n    {%- endif -%}\n{%- endmacro -%}\n\n{%- macro render_tool_namespace(namespace_name, tools) -%}\n    {{- \"## \" + namespace_name + \"\n\n\" }}\n    {{- \"namespace \" + namespace_name + \" {\n\n\" }}\n    {%- for tool in tools %}\n        {%- set tool = tool.function %}\n        {{- \"// \" + tool.description + \"\n\" }}\n        {{- \"type \"+ tool.name + \" = \" }}\n        {%- if tool.parameters and tool.parameters.properties %}\n            {{- \"(_: {\n\" }}\n            {%- for param_name, param_spec in tool.parameters.properties.items() %}\n                {%- if param_spec.description %}\n                    {{- \"// \" + param_spec.description + \"\n\" }}\n                {%- endif %}\n                {{- param_name }}\n                {%- if param_name not in (tool.parameters.required or []) -%}\n                    {{- \"?\" }}\n                {%- endif -%}\n                {{- \": \" }}\n                {{- render_typescript_type(param_spec, tool.parameters.required or []) }}\n                {%- if param_spec.default is defined -%}\n                    {%- if param_spec.enum %}\n                        {{- \", // default: \" + param_spec.default }}\n                    {%- elif param_spec.oneOf %}\n                        {{- \"// default: \" + param_spec.default }}\n                    {%- else %}\n                        {{- \", // default: \" + param_spec.default|tojson }}\n                    {%- endif -%}\n                {%- endif -%}\n                {%- if not loop.last %}\n                    {{- \",\n\" }}\n                {%- else %}\n                    {{- \"\n\" }}\n                {%- endif -%}\n            {%- endfor %}\n            {{- \"}) => any;\n\n\" }}\n        {%- else -%}\n            {{- \"() => any;\n\n\" }}\n        {%- endif -%}\n    {%- endfor %}\n    {{- \"} // namespace \" + namespace_name }}\n{%- endmacro -%}\n\n{%- macro render_builtin_tools(browser_tool, python_tool) -%}\n    {%- if browser_tool %}\n        {{- \"## browser\n\n\" }}\n        {{- \"// Tool for browsing.\n\" }}\n        {{- \"// The `cursor` appears in brackets before each browsing display: `[{cursor}]`.\n\" }}\n        {{- \"// Cite information from the tool using the following format:\n\" }}\n        {{- \"// `【{cursor}†L{line_start}(-L{line_end})?】`, for example: `【6†L9-L11】` or `【8†L3】`.\n\" }}\n        {{- \"// Do not quote more than 10 words directly from the tool output.\n\" }}\n        {{- \"// sources=web (default: web)\n\" }}\n        {{- \"namespace browser {\n\n\" }}\n        {{- \"// Searches for information related to `query` and displays `topn` results.\n\" }}\n        {{- \"type search = (_: {\n\" }}\n        {{- \"query: string,\n\" }}\n        {{- \"topn?: number, // default: 10\n\" }}\n        {{- \"source?: string,\n\" }}\n        {{- \"}) => any;\n\n\" }}\n        {{- \"// Opens the link `id` from the page indicated by `cursor` starting at line number `loc`, showing `num_lines` lines.\n\" }}\n        {{- \"// Valid link ids are displayed with the formatting: `【{id}†.*】`.\n\" }}\n        {{- \"// If `cursor` is not provided, the most recent page is implied.\n\" }}\n        {{- \"// If `id` is a string, it is treated as a fully qualified URL associated with `source`.\n\" }}\n        {{- \"// If `loc` is not provided, the viewport will be positioned at the beginning of the document or centered on the most relevant passage, if available.\n\" }}\n        {{- \"// Use this function without `id` to scroll to a new location of an opened page.\n\" }}\n        {{- \"type open = (_: {\n\" }}\n        {{- \"id?: number | string, // default: -1\n\" }}\n        {{- \"cursor?: number, // default: -1\n\" }}\n        {{- \"loc?: number, // default: -1\n\" }}\n        {{- \"num_lines?: number, // default: -1\n\" }}\n        {{- \"view_source?: boolean, // default: false\n\" }}\n        {{- \"source?: string,\n\" }}\n        {{- \"}) => any;\n\n\" }}\n        {{- \"// Finds exact matches of `pattern` in the current page, or the page given by `cursor`.\n\" }}\n        {{- \"type find = (_: {\n\" }}\n        {{- \"pattern: string,\n\" }}\n        {{- \"cursor?: number, // default: -1\n\" }}\n        {{- \"}) => any;\n\n\" }}\n        {{- \"} // namespace browser\n\n\" }}\n    {%- endif -%}\n\n    {%- if python_tool %}\n        {{- \"## python\n\n\" }}\n        {{- \"Use this tool to execute Python code in your chain of thought. The code will not be shown to the user. This tool should be used for internal reasoning, but not for code that is intended to be visible to the user (e.g. when creating plots, tables, or files).\n\n\" }}\n        {{- \"When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 120.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is UNKNOWN. Depends on the cluster.\n\n\" }}\n    {%- endif -%}\n{%- endmacro -%}\n\n{#- System Message Construction ============================================ #}\n{%- macro build_system_message() -%}\n    {%- if model_identity is not defined %}\n        {%- set model_identity = \"You are ChatGPT, a large language model trained by OpenAI.\" %}\n    {%- endif %}\n    {{- model_identity + \"\n\" }}\n    {{- \"Knowledge cutoff: 2024-06\n\" }}\n    {{- \"Current date: \" + strftime_now(\"%Y-%m-%d\") + \"\n\n\" }}\n    {%- if reasoning_effort is not defined %}\n        {%- set reasoning_effort = \"medium\" %}\n    {%- endif %}\n    {{- \"Reasoning: \" + reasoning_effort + \"\n\n\" }}\n    {%- if builtin_tools %}\n        {{- \"# Tools\n\n\" }}\n        {%- set available_builtin_tools = namespace(browser=false, python=false) %}\n        {%- for tool in builtin_tools %}\n            {%- if tool == \"browser\" %}\n                {%- set available_builtin_tools.browser = true %}\n            {%- elif tool == \"python\" %}\n                {%- set available_builtin_tools.python = true %}\n            {%- endif %}\n        {%- endfor %}\n        {{- render_builtin_tools(available_builtin_tools.browser, available_builtin_tools.python) }}\n    {%- endif -%}\n    {{- \"# Valid channels: analysis, commentary, final. Channel must be included for every message.\" }}\n    {%- if tools -%}\n        {{- \"\nCalls to these tools must go to the commentary channel: 'functions'.\" }}\n    {%- endif -%}\n{%- endmacro -%}\n\n{#- Main Template Logic ================================================= #}\n{#- Set defaults #}\n\n{#- Render system message #}\n{{- \"<|start|>system<|message|>\" }}\n{{- build_system_message() }}\n{{- \"<|end|>\" }}\n\n{#- Extract developer message #}\n{%- if messages[0].role == \"developer\" or messages[0].role == \"system\" %}\n    {%- set developer_message = messages[0].content %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set developer_message = \"\" %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{#- Render developer message #}\n{%- if developer_message or tools %}\n    {{- \"<|start|>developer<|message|>\" }}\n    {%- if developer_message %}\n        {{- \"# Instructions\n\n\" }}\n        {{- developer_message }}\n    {%- endif %}\n    {%- if tools -%}\n        {{- \"\n\n\" }}\n        {{- \"# Tools\n\n\" }}\n        {{- render_tool_namespace(\"functions\", tools) }}\n    {%- endif -%}\n    {{- \"<|end|>\" }}\n{%- endif %}\n\n{#- Render messages #}\n{%- set last_tool_call = namespace(name=none) %}\n{%- for message in loop_messages -%}\n    {#- At this point only assistant/user/tool messages should remain #}\n    {%- if message.role == 'assistant' -%}\n        {#- Checks to ensure the messages are being passed in the format we expect #}\n        {%- if \"content\" in message %}\n            {%- if \"<|channel|>analysis<|message|>\" in message.content or \"<|channel|>final<|message|>\" in message.content %}\n                {{- raise_exception(\"You have passed a message containing <|channel|> tags in the content field. Instead of doing this, you should pass analysis messages (the string between '<|message|>' and '<|end|>') in the 'thinking' field, and final messages (the string between '<|message|>' and '<|end|>') in the 'content' field.\") }}\n            {%- endif %}\n        {%- endif %}\n        {%- if \"thinking\" in message %}\n            {%- if \"<|channel|>analysis<|message|>\" in message.thinking or \"<|channel|>final<|message|>\" in message.thinking %}\n                {{- raise_exception(\"You have passed a message containing <|channel|> tags in the thinking field. Instead of doing this, you should pass analysis messages (the string between '<|message|>' and '<|end|>') in the 'thinking' field, and final messages (the string between '<|message|>' and '<|end|>') in the 'content' field.\") }}\n            {%- endif %}\n        {%- endif %}\n        {%- if \"tool_calls\" in message %}\n            {#- We assume max 1 tool call per message, and so we infer the tool call name #}\n            {#- in \"tool\" messages from the most recent assistant tool call name #}\n            {%- set tool_call = message.tool_calls[0] %}\n            {%- if tool_call.function %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {%- if message.content and message.thinking %}\n                {{- raise_exception(\"Cannot pass both content and thinking in an assistant message with tool calls! Put the analysis message in one or the other, but not both.\") }}\n            {%- elif message.content %}\n                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.content + \"<|end|>\" }}\n            {%- elif message.thinking %}\n                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.thinking + \"<|end|>\" }}\n            {%- endif %}\n            {{- \"<|start|>assistant to=\" }}\n            {{- \"functions.\" + tool_call.name + \"<|channel|>commentary \" }}\n            {{- (tool_call.content_type if tool_call.content_type is defined else \"json\") + \"<|message|>\" }}\n            {{- tool_call.arguments|tojson }}\n            {{- \"<|call|>\" }}\n            {%- set last_tool_call.name = tool_call.name %}\n        {%- elif loop.last and not add_generation_prompt %}\n            {#- Only render the CoT if the final turn is an assistant turn and add_generation_prompt is false #}\n            {#- This is a situation that should only occur in training, never in inference. #}\n            {%- if \"thinking\" in message %}\n                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.thinking + \"<|end|>\" }}\n            {%- endif %}\n            {#- <|return|> indicates the end of generation, but <|end|> does not #}\n            {#- <|return|> should never be an input to the model, but we include it as the final token #}\n            {#- when training, so the model learns to emit it. #}\n            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|return|>\" }}\n        {%- else %}\n            {#- CoT is dropped during all previous turns, so we never render it for inference #}\n            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|end|>\" }}\n            {%- set last_tool_call.name = none %}\n        {%- endif %}\n    {%- elif message.role == 'tool' -%}\n        {%- if last_tool_call.name is none %}\n            {{- raise_exception(\"Message has tool role, but there was no previous assistant message with a tool call!\") }}\n        {%- endif %}\n        {{- \"<|start|>functions.\" + last_tool_call.name }}\n        {{- \" to=assistant<|channel|>commentary<|message|>\" + message.content|tojson + \"<|end|>\" }}\n    {%- elif message.role == 'user' -%}\n        {{- \"<|start|>user<|message|>\" + message.content + \"<|end|>\" }}\n    {%- endif -%}\n{%- endfor -%}\n\n{#- Generation prompt #}\n{%- if add_generation_prompt -%}\n<|start|>assistant\n{%- endif -%}",
    "stop_token_ids": [
      200002,
      199999
    ],
    "stop": [
      "<|endoftext|>",
      "<|return|>"
    ],
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    },
    "updated_at": 1769418588,
    "featured": true,
    "architectures": [
      "GptOssForCausalLM"
    ],
    "tool_parser": "qwen",
    "reasoning_start_tag": "analysis",
    "reasoning_end_tag": "assistantfinal",
    "cache_config": {
      "ignore_patterns": [
        "metal/**",
        "original/**"
      ],
      "ignore_file_pattern": [
        "metal/*",
        "original/*"
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "KAT-V1",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Kwaipilot-AutoThink ranks first among all open-source models on LiveCodeBench Pro, a challenging benchmark explicitly designed to prevent data leakage, and even surpasses strong proprietary systems such as Seed and o3-mini.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 40,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Kwaipilot/KAT-V1-40B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Kwaipilot/KAT-V1-40B"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 40,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "QuantTrio/KAT-V1-40B-GPTQ-Int4-Int8Mix"
          },
          "modelscope": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "tclf90/KAT-V1-40B-GPTQ-Int4-Int8Mix"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 40,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/KAT-V1-40B-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/KAT-V1-40B-AWQ"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- '' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" and not message.tool_calls %}\n        {%- set content = message.content %}\n        {%- if not loop.last %}\n            {%- set answer_blocks = message.content.split('<answer>\\n') %}\n            {%- if answer_blocks|length > 1 %}\n                {%- set last_answer_block = answer_blocks[-1] %}\n                {%- if '\\n</answer>' in last_answer_block %}\n                    {%- set content = last_answer_block.split('\\n</answer>')[0] %}\n                {%- else %}\n                    {%- set content = message.content.split('<think_off>')[-1].lstrip('\\n') %}\n                    {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n                {%- endif %}\n            {%- else %}\n                {%- set content = message.content.split('<think_off>')[-1].lstrip('\\n') %}\n                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content %}\n        {%- if not loop.last %}\n            {%- set answer_blocks = message.content.split('<answer>\\n') %}\n            {%- if answer_blocks|length > 1 %}\n                {%- set last_answer_block = answer_blocks[-1] %}\n                {%- if '\\n</answer>' in last_answer_block %}\n                    {%- set content = last_answer_block.split('\\n</answer>')[0] %}\n                {%- else %}\n                    {%- set content = message.content.split('<think_off>')[-1].lstrip('\\n') %}\n                    {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n                {%- endif %}\n            {%- else %}\n                {%- set content = message.content.split('<think_off>')[-1].lstrip('\\n') %}\n                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\\\"name\\\": \\\"' }}\n            {{- tool_call.name }}\n            {{- '\\\", \\\"arguments\\\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n<judge>\\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_end|>"
    ],
    "updated_at": 1769418589,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "Deepseek-V3.1",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "hybrid",
      "tools"
    ],
    "model_description": "DeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-V3.1"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "deepseek-ai/DeepSeek-V3.1"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "cpatonn/DeepSeek-V3.1-GPTQ-4bit"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "cpatonn/DeepSeek-V3.1-GPTQ-4bit"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/DeepSeek-V3.1-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/DeepSeek-V3.1-AWQ"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit",
              "4bit"
            ],
            "model_id": "mlx-community/DeepSeek-V3.1-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "8bit",
              "4bit"
            ],
            "model_id": "mlx-community/DeepSeek-V3.1-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% if not thinking is defined %}{% set thinking = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, system_prompt='', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\n\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}{%- if ns.is_last_user %}{{'<｜Assistant｜></think>'}}{%- endif %}{%- set ns.is_last_user = false -%}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>'+ tool['function']['name'] + '<｜tool▁sep｜>' + tool['function']['arguments'] + '<｜tool▁call▁end｜>'}}{%- else %}{{message['content'] + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['function']['name'] + '<｜tool▁sep｜>' + tool['function']['arguments'] + '<｜tool▁call▁end｜>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'<｜tool▁call▁begin｜>'+ tool['function']['name'] + '<｜tool▁sep｜>' + tool['function']['arguments'] + '<｜tool▁call▁end｜>'}}{%- endif %}{%- endfor %}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none) %}{%- if ns.is_last_user %}{{'<｜Assistant｜>'}}{%- if message['prefix'] is defined and message['prefix'] and thinking %}{{'<think>'}}  {%- else %}{{'</think>'}}{%- endif %}{%- endif %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{%- set content = message['content'] -%}{%- if '</think>' in content %}{%- set content = content.split('</think>', 1)[1] -%}{%- endif %}{{content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{{'<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endfor -%}{%- if add_generation_prompt and ns.is_last_user and not ns.is_tool %}{{'<｜Assistant｜>'}}{%- if not thinking %}{{'</think>'}}{%- else %}{{'<think>'}}{%- endif %}{% endif %}",
    "stop_token_ids": [
      1
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    },
    "updated_at": 1769418590,
    "featured": false,
    "architectures": [
      "DeepseekV3ForCausalLM"
    ],
    "model_type": "deepseek_v3"
  },
  {
    "version": 2,
    "context_length": 524288,
    "model_name": "seed-oss",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "tools"
    ],
    "model_description": "Seed-OSS is a series of open-source large language models developed by ByteDance's Seed Team, designed for powerful long-context, reasoning, agent and general capabilities, and versatile developer-friendly features. Although trained with only 12T tokens, Seed-OSS achieves excellent performance on several popular open benchmarks.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 36,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "ByteDance-Seed/Seed-OSS-36B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ByteDance-Seed/Seed-OSS-36B-Instruct"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 36,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int8",
              "Int4",
              "Int3"
            ],
            "model_id": "QuantTrio/Seed-OSS-36B-Instruct-GPTQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "Int8",
              "Int4",
              "Int3"
            ],
            "model_id": "tclf90/Seed-OSS-36B-Instruct-GPTQ-{quantization}"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 36,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/Seed-OSS-36B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/Seed-OSS-36B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 36,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Seed-OSS-36B-Instruct-4bit"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Seed-OSS-36B-Instruct-4bit"
          }
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 36,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "unsloth/Seed-OSS-36B-Instruct-GGUF",
            "model_file_name_template": "Seed-OSS-36B-Instruct-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Seed-OSS-36B-Instruct-{quantization}-{part}.gguf"
          },
          "modelscope": {
            "quantizations": [
              "BF16",
              "IQ4_NL",
              "IQ4_XS",
              "Q2_K",
              "Q2_K_L",
              "Q3_K_M",
              "Q3_K_S",
              "Q4_0",
              "Q4_1",
              "Q4_K_M",
              "Q4_K_S",
              "Q5_K_M",
              "Q5_K_S",
              "Q6_K",
              "Q8_0",
              "UD-IQ1_M",
              "UD-IQ1_S",
              "UD-IQ2_M",
              "UD-IQ2_XXS",
              "UD-IQ3_XXS",
              "UD-Q2_K_XL",
              "UD-Q3_K_XL",
              "UD-Q4_K_XL",
              "UD-Q5_K_XL",
              "UD-Q6_K_XL",
              "UD-Q8_K_XL"
            ],
            "quantization_parts": {
              "BF16": [
                "00001-of-00002",
                "00002-of-00002"
              ]
            },
            "model_id": "unsloth/Seed-OSS-36B-Instruct-GGUF",
            "model_file_name_template": "Seed-OSS-36B-Instruct-{quantization}.gguf",
            "model_file_name_split_template": "{quantization}/Seed-OSS-36B-Instruct-{quantization}-{part}.gguf"
          }
        }
      }
    ],
    "chat_template": "{# ------------- special token variables ------------- #}{%- set bos_token              = '<seed:bos>'               -%}{%- set eos_token              = '<seed:eos>'               -%}{%- set pad_token              = '<seed:pad>'               -%}{%- set toolcall_begin_token   = '<seed:tool_call>'         -%}{%- set toolcall_end_token     = '</seed:tool_call>'        -%}{%- set think_begin_token      = '<seed:think>'             -%}{%- set think_end_token        = '</seed:think>'            -%}{%- set budget_begin_token     = '<seed:cot_budget_reflect>'-%}{%- set budget_end_token       = '</seed:cot_budget_reflect>'-%}{# -------------- reflection-interval lookup -------------- #}{%- if not thinking_budget is defined %}{%- set thinking_budget = -1 -%}{%- endif -%}{%- set budget_reflections_v05 = {     0:      0,     512:    128,     1024:   256,     2048:   512,     4096:   512,     8192:   1024,     16384:  1024} -%}{%- set ns = namespace(interval = None) -%}{%- for k, v in budget_reflections_v05 | dictsort -%}    {%- if ns.interval is none and thinking_budget <= k -%}        {%- set ns.interval = v -%}    {%- endif -%}{%- endfor -%}{%- if ns.interval is none -%}    {%- set ns.interval = budget_reflections_v05[16384] -%}{%- endif -%}{%- if messages[0][\"role\"] == \"system\" %}{%- set system_message = messages[0][\"content\"] %}{%- set loop_messages = messages[1:] %}{%- else %}{%- set loop_messages = messages %}{%- endif %}{%- if not tools is defined or tools is none %}{%- set tools = [] %}{%- endif %}{%- macro py_type(t) -%}    {%- if t == \"string\" -%}str    {%- elif t in (\"number\", \"integer\") -%}int    {%- elif t == \"boolean\" -%}bool    {%- elif t == \"array\" -%}list    {%- else -%}Any{%- endif -%}{%- endmacro -%}{%- if system_message is defined %}{{ bos_token + \"system\\n\" + system_message }}{%- else %}{%- if tools is iterable and tools | length > 0 %}{{ bos_token + \"system\\nYou are Doubao, a helpful AI assistant. You may call one or more functions to assist with the user query.\" }}{%- endif %}{%- endif %}{%- if use_json_tooldef is defined and use_json_tooldef %}{{\"Tool List:\\nYou are authorized to use the following tools (described in JSON Schema format). Before performing any task, you must decide how to call them based on the descriptions and parameters of these tools.\"}}{{ tools | tojson(ensure_ascii=False) }}{%- else %}{%- for item in tools if item.type == \"function\" %}Function:def {{ item.function.name }}({%- for name, spec in item.function.parameters.properties.items() %}        {{- name }}: {{ py_type(spec.type) }}{% if not loop.last %},{% endif %}{%- endfor %}):    \"\"\"    {{ item.function.description | trim }}    {%- if item.function.parameters.properties %}    Args:    {%- for name, spec in item.function.parameters.properties.items() %}    - {{ name }} ({{ py_type(spec.type) }})      {%- if name in item.function.parameters.required %} [必填]{% else %} [选填]{% endif %}:      {{- \" \" ~ (spec.description or \"\") }}    {%- endfor %}    {%- endif %}    {%- if item.function.returns is defined           and item.function.returns.properties is defined           and item.function.returns.properties %}    Returns:    {%- for name, spec in item.function.returns.properties.items() %}    - {{ name }} ({{ py_type(spec.type) }}):      {{- \" \" ~ (spec.description or \"\") }}    {%- endfor %}    {%- endif %}    \"\"\"{%- endfor %}{%- endif %}{%- if tools is iterable and tools | length > 0 %}{{\"工具调用请遵循如下格式:\\n<seed:tool_call>\\n<function=example_function_name>\\n<parameter=example_parameter_1>value_1</parameter>\\n<parameter=example_parameter_2>This is the value for the second parameter\\nthat can span\\nmultiple lines</parameter>\\n</function>\\n</seed:tool_call>\\n\"}}{%- endif %}{%- if system_message is defined or tools is iterable and tools | length > 0 %}{{ eos_token }}{%- endif %}{%- if thinking_budget is defined %}{%- if thinking_budget == 0 %}{{ bos_token+\"system\" }}{{ \"You are an intelligent assistant that can answer questions in one step without the need for reasoning and thinking, that is, your thinking budget is 0. Next, please skip the thinking process and directly start answering the user's questions.\" }}{{ eos_token }}{%- elif not thinking_budget == -1 %}{{ bos_token+\"system\" }}{{ \"You are an intelligent assistant with reflective ability. In the process of thinking and reasoning, you need to strictly follow the thinking budget, which is \"}}{{thinking_budget}}{{\". That is, you need to complete your thinking within \"}}{{thinking_budget}}{{\" tokens and start answering the user's questions. You will reflect on your thinking process every \"}}{{ns.interval}}{{\" tokens, stating how many tokens have been used and how many are left.\"}}{{ eos_token }}{%- endif %}{%- endif %}{%- for message in loop_messages %}{%- if message.role == \"assistant\"   and message.tool_calls is defined   and message.tool_calls is iterable   and message.tool_calls | length > 0 %}{{ bos_token + message.role }}{%- if message.reasoning_content is defined and message.reasoning_content is string and message.reasoning_content | trim | length > 0 %}{{ \"\\n\" + think_begin_token + message.reasoning_content | trim + think_end_token }}{%- endif %}{%- if message.content is defined and message.content is string and message.content | trim | length > 0 %}{{ \"\\n\" + message.content | trim + \"\\n\" }}{%- endif %}{%- for tool_call in message.tool_calls %}{%- if tool_call.function is defined %}{% set tool_call = tool_call.function %}{% endif %}{{ \"\\n\" + toolcall_begin_token + \"\\n<function=\" + tool_call.name + \">\\n\" }}{%- if tool_call.arguments is defined %}{%- for arg_name, arg_value in tool_call.arguments | items %}{{ \"<parameter=\" + arg_name + \">\" }}{%- set arg_value = arg_value if arg_value is string else arg_value | string %}{{ arg_value+\"</parameter>\\n\" }}{%- endfor %}{%- endif %}{{ \"</function>\\n\" + toolcall_end_token }}{%- endfor %}{{ eos_token }}{%- elif message.role in [\"user\", \"system\"] %}{{ bos_token + message.role + \"\\n\" + message.content + eos_token }}{%- elif message.role == \"assistant\" %}{{ bos_token + message.role }}{%- if message.reasoning_content is defined and message.reasoning_content is string and message.reasoning_content | trim | length > 0 %}{{ \"\\n\" + think_begin_token + message.reasoning_content | trim + think_end_token }}{%- endif %}{%- if message.content is defined and message.content is string and message.content | trim | length > 0 %}{{ \"\\n\" + message.content | trim + eos_token }}{%- endif %}{%- else %}{{ bos_token + message.role + \"\\n\" + message.content + eos_token }}{%- endif %}{%- endfor %}{%- if add_generation_prompt %}{{ bos_token+\"assistant\\n\" }}{%- if thinking_budget == 0 %}{{ think_begin_token + \"\\n\" + budget_begin_token + \"The current thinking budget is 0, so I will directly start answering the question.\" + budget_end_token + \"\\n\" + think_end_token }}{%- endif %}{%- endif %}",
    "stop_token_ids": [
      0,
      1,
      2
    ],
    "stop": [
      "<seed:bos>",
      "<seed:pad>",
      "<seed:eos>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "updated_at": 1769418591,
    "featured": false,
    "architectures": [
      "SeedOssForCausalLM"
    ],
    "model_type": "seed_oss",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#llama_cpp_dependencies# ; #engine# == \"llama.cpp\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 131072,
    "model_name": "Baichuan-M2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "hybrid",
      "tools"
    ],
    "model_description": "Baichuan-M2-32B is Baichuan AI's medical-enhanced reasoning model, the second medical model released by Baichuan. Designed for real-world medical reasoning tasks, this model builds upon Qwen2.5-32B with an innovative Large Verifier System. Through domain-specific fine-tuning on real-world medical questions, it achieves breakthrough medical performance while maintaining strong general capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "baichuan-inc/Baichuan-M2-32B"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "baichuan-inc/Baichuan-M2-32B"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "baichuan-inc/Baichuan-M2-32B-GPTQ-Int4"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "baichuan-inc/Baichuan-M2-32B-GPTQ-Int4"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in message.content %}\n                {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n                {%- set reasoning_content = message.content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '<|im_start|>' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n    {%- if thinking_mode is defined %}\n        {%- if thinking_mode == \"on\" %}\n            {{- '<think>\\n' }}\n        {%- elif thinking_mode == \"off\" %}\n            {{- '<think>\\n\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "updated_at": 1769418592,
    "featured": false,
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "model_type": "qwen2",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 262144,
    "model_name": "Qwen3-VL-Instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision",
      "tools"
    ],
    "model_description": "Meet Qwen3-VL — the most powerful vision-language model in the Qwen series to date.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-235B-A22B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-235B-A22B-Instruct"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-235B-A22B-Instruct-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-235B-A22B-Instruct-FP8"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/Qwen3-VL-235B-A22B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-30B-A3B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-30B-A3B-Instruct"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn/Qwen3-VL-30B-A3B-Instruct-AWQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn-mirror/Qwen3-VL-30B-A3B-Instruct-AWQ-{quantization}"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-32B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-32B-Instruct"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-32B-Instruct-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-32B-Instruct-FP8"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/Qwen3-VL-32B-Instruct-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/Qwen3-VL-32B-Instruct-AWQ"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-8B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-8B-Instruct"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-8B-Instruct-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-8B-Instruct-FP8"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn/Qwen3-VL-8B-Instruct-AWQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn-mirror/Qwen3-VL-8B-Instruct-AWQ-{quantization}"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-4B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-4B-Instruct"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-4B-Instruct-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-4B-Instruct-FP8"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn/Qwen3-VL-4B-Instruct-AWQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn-mirror/Qwen3-VL-4B-Instruct-AWQ-{quantization}"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-2B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-2B-Instruct"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-2B-Instruct-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-2B-Instruct-FP8"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-235B-A22B-Instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-235B-A22B-Instruct-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-30B-A3B-Instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-30B-A3B-Instruct-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-32B-Instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-32B-Instruct-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Instruct-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-2B-Instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-2B-Instruct-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 2,
        "activated_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-2B-Instruct-bf16"
          },
          "modelscope": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-2B-Instruct-bf16"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 4,
        "activated_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Instruct-3bit"
          },
          "modelscope": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Instruct-3bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 4,
        "activated_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Instruct-4bit"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Instruct-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 4,
        "activated_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Instruct-8bit"
          },
          "modelscope": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Instruct-8bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 4,
        "activated_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Instruct-bf16"
          },
          "modelscope": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Instruct-bf16"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "activated_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Instruct-3bit"
          },
          "modelscope": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Instruct-3bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "activated_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "5bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Instruct-5bit"
          },
          "modelscope": {
            "quantizations": [
              "5bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Instruct-5bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "activated_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Instruct-bf16"
          },
          "modelscope": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Instruct-bf16"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 30,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-30B-A3B-Instruct-3bit"
          },
          "modelscope": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-30B-A3B-Instruct-3bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 30,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-30B-A3B-Instruct-bf16"
          },
          "modelscope": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-30B-A3B-Instruct-bf16"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-32B-Instruct-3bit"
          },
          "modelscope": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-32B-Instruct-3bit"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {%- if messages[0].content is string %}\n            {{- messages[0].content }}\n        {%- else %}\n            {%- for content in messages[0].content %}\n                {%- if 'text' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' }}\n        {%- if messages[0].content is string %}\n            {{- messages[0].content }}\n        {%- else %}\n            {%- for content in messages[0].content %}\n                {%- if 'text' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set image_count = namespace(value=0) %}\n{%- set video_count = namespace(value=0) %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" %}\n        {{- '<|im_start|>' + message.role + '\\n' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- else %}\n            {%- for content in message.content %}\n                {%- if content.type == 'image' or 'image' in content or 'image_url' in content %}\n                    {%- set image_count.value = image_count.value + 1 %}\n                    {%- if add_vision_id %}Picture {{ image_count.value }}: {% endif -%}\n                    <|vision_start|><|image_pad|><|vision_end|>\n                {%- elif content.type == 'video' or 'video' in content %}\n                    {%- set video_count.value = video_count.value + 1 %}\n                    {%- if add_vision_id %}Video {{ video_count.value }}: {% endif -%}\n                    <|vision_start|><|video_pad|><|vision_end|>\n                {%- elif 'text' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role + '\\n' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- else %}\n            {%- for content_item in message.content %}\n                {%- if 'text' in content_item %}\n                    {{- content_item.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and message.content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- else %}\n            {%- for content in message.content %}\n                {%- if content.type == 'image' or 'image' in content or 'image_url' in content %}\n                    {%- set image_count.value = image_count.value + 1 %}\n                    {%- if add_vision_id %}Picture {{ image_count.value }}: {% endif -%}\n                    <|vision_start|><|image_pad|><|vision_end|>\n                {%- elif content.type == 'video' or 'video' in content %}\n                    {%- set video_count.value = video_count.value + 1 %}\n                    {%- if add_vision_id %}Video {{ video_count.value }}: {% endif -%}\n                    <|vision_start|><|video_pad|><|vision_end|>\n                {%- elif 'text' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_end|>"
    ],
    "tool_parser": "qwen",
    "updated_at": 1769418594,
    "featured": true,
    "architectures": [
      "Qwen3VLMoeForConditionalGeneration"
    ],
    "model_type": "qwen3_vl_moe",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 262144,
    "model_name": "Qwen3-VL-Thinking",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision",
      "reasoning",
      "tools"
    ],
    "model_description": "Meet Qwen3-VL — the most powerful vision-language model in the Qwen series to date.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-235B-A22B-Thinking"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-235B-A22B-Thinking"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-235B-A22B-Thinking-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-235B-A22B-Thinking-FP8"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/Qwen3-VL-235B-A22B-Thinking-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/Qwen3-VL-235B-A22B-Thinking-AWQ"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-30B-A3B-Thinking"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-VL-30B-A3B-Thinking"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-30B-A3B-Thinking-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-VL-30B-A3B-Thinking-FP8"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn/Qwen3-VL-30B-A3B-Thinking-AWQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn-mirror/Qwen3-VL-30B-A3B-Thinking-AWQ-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-235B-A22B-Thinking-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-235B-A22B-Thinking-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-30B-A3B-Thinking-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-30B-A3B-Thinking-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 2,
        "activated_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-2B-Thinking-4bit"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-2B-Thinking-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 2,
        "activated_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-2B-Thinking-8bit"
          },
          "modelscope": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-2B-Thinking-8bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 2,
        "activated_size_in_billions": 2,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-2B-Thinking-bf16"
          },
          "modelscope": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-2B-Thinking-bf16"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 4,
        "activated_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Thinking-3bit"
          },
          "modelscope": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Thinking-3bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 4,
        "activated_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Thinking-4bit"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Thinking-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 4,
        "activated_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Thinking-8bit"
          },
          "modelscope": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Thinking-8bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 4,
        "activated_size_in_billions": 4,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Thinking-bf16"
          },
          "modelscope": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-4B-Thinking-bf16"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "activated_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Thinking-3bit"
          },
          "modelscope": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Thinking-3bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "activated_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Thinking-4bit"
          },
          "modelscope": {
            "quantizations": [
              "4bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Thinking-4bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "activated_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "5bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Thinking-5bit"
          },
          "modelscope": {
            "quantizations": [
              "5bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Thinking-5bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "activated_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Thinking-8bit"
          },
          "modelscope": {
            "quantizations": [
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Thinking-8bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "activated_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Thinking-bf16"
          },
          "modelscope": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-8B-Thinking-bf16"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 30,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-30B-A3B-Thinking-3bit"
          },
          "modelscope": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-30B-A3B-Thinking-3bit"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 30,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-30B-A3B-Thinking-bf16"
          },
          "modelscope": {
            "quantizations": [
              "bf16"
            ],
            "model_id": "mlx-community/Qwen3-VL-30B-A3B-Thinking-bf16"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 235,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-235B-A22B-Thinking-3bit"
          },
          "modelscope": {
            "quantizations": [
              "3bit"
            ],
            "model_id": "mlx-community/Qwen3-VL-235B-A22B-Thinking-3bit"
          }
        }
      }
    ],
    "chat_template": "{%- set image_count = namespace(value=0) %}\n{%- set video_count = namespace(value=0) %}\n{%- macro render_content(content, do_vision_count) %}\n    {%- if content is string %}\n        {{- content }}\n    {%- else %}\n        {%- for item in content %}\n            {%- if 'image' in item or 'image_url' in item or item.type == 'image' %}\n                {%- if do_vision_count %}\n                    {%- set image_count.value = image_count.value + 1 %}\n                {%- endif %}\n                {%- if add_vision_id %}Picture {{ image_count.value }}: {% endif -%}\n                <|vision_start|><|image_pad|><|vision_end|>\n            {%- elif 'video' in item or item.type == 'video' %}\n                {%- if do_vision_count %}\n                    {%- set video_count.value = video_count.value + 1 %}\n                {%- endif %}\n                {%- if add_vision_id %}Video {{ video_count.value }}: {% endif -%}\n                <|vision_start|><|video_pad|><|vision_end|>\n            {%- elif 'text' in item %}\n                {{- item.text }}\n            {%- endif %}\n        {%- endfor %}\n    {%- endif %}\n{%- endmacro %}\n{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- render_content(messages[0].content, false) + '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + render_content(messages[0].content, false) + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" %}\n        {%- set content = render_content(message.content, false) %}\n        {%- if not(content.startswith('<tool_response>') and content.endswith('</tool_response>')) %}\n            {%- set ns.multi_step_tool = false %}\n            {%- set ns.last_query_index = index %}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- set content = render_content(message.content, True) %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in content %}\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '<|im_start|>' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n<think>\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_end|>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "tool_parser": "qwen",
    "updated_at": 1769418596,
    "featured": false,
    "architectures": [
      "Qwen3VLMoeForConditionalGeneration"
    ],
    "model_type": "qwen3_vl_moe",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 262144,
    "model_name": "Qwen3-Next-Instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen3-Next-80B-A3B is the first installment in the Qwen3-Next series",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 80,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-Next-80B-A3B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-Next-80B-A3B-Instruct"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 80,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-Next-80B-A3B-Instruct-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-Next-80B-A3B-Instruct-FP8"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 80,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn-mirror/Qwen3-Next-80B-A3B-Instruct-AWQ-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 80,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-Next-80B-A3B-Instruct-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-Next-80B-A3B-Instruct-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_end|>"
    ],
    "tool_parser": "qwen",
    "updated_at": 1769418597,
    "featured": false,
    "architectures": [
      "Qwen3NextForCausalLM"
    ],
    "model_type": "qwen3_next",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 262144,
    "model_name": "Qwen3-Next-Thinking",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "tools"
    ],
    "model_description": "Qwen3-Next-80B-A3B is the first installment in the Qwen3-Next series",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 80,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-Next-80B-A3B-Thinking"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-Next-80B-A3B-Thinking"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 80,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-Next-80B-A3B-Thinking-FP8"
          },
          "modelscope": {
            "quantizations": [
              "fp8"
            ],
            "model_id": "Qwen/Qwen3-Next-80B-A3B-Thinking-FP8"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 80,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn/Qwen3-Next-80B-A3B-Thinking-AWQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn-mirror/Qwen3-Next-80B-A3B-Thinking-AWQ-{quantization}"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 80,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-Next-80B-A3B-Thinking-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/Qwen3-Next-80B-A3B-Thinking-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in content %}\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '<|im_start|>' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n<think>\\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_end|>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "tool_parser": "qwen",
    "updated_at": 1769418598,
    "featured": false,
    "architectures": [
      "Qwen3NextForCausalLM"
    ],
    "model_type": "qwen3_next",
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    }
  },
  {
    "version": 2,
    "context_length": 32768,
    "model_name": "MiniCPM-V-4.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "MiniCPM-V 4.5 is an improved version in the MiniCPM-V series with enhanced multimodal capabilities and better performance.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "openbmb/MiniCPM-V-4_5",
            "model_revision": "main"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenBMB/MiniCPM-V-4_5",
            "model_revision": "master"
          }
        }
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "openbmb/MiniCPM-V-4_5-int4",
            "model_revision": "main"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "OpenBMB/MiniCPM-V-4_5-int4",
            "model_revision": "master"
          }
        }
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>"
    ],
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    },
    "updated_at": 1769418599,
    "featured": false,
    "architectures": [
      "MiniCPMV"
    ],
    "model_type": "minicpmv"
  },
  {
    "version": 2,
    "context_length": 262144,
    "model_name": "Qwen3-Omni-Thinking",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision",
      "audio",
      "omni",
      "reasoning",
      "tools"
    ],
    "model_description": "Qwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-Omni-30B-A3B-Thinking"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-Omni-30B-A3B-Thinking"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn/Qwen3-Omni-30B-A3B-Thinking-AWQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn-mirror/Qwen3-Omni-30B-A3B-Thinking-AWQ-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}{{- messages[0].content + '\\n\\n' }}{%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {%- if messages[0].content is string %}\n            {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n        {%- else %}\n            {%- for content in messages[0].content %}\n                {%- if content.type == 'image' or 'image' in content or 'image_url' in content %}\n                    {{- '<|im_start|>system\\n' +\"<|vision_start|><|image_pad|><|vision_end|>\"+ '<|im_end|>\\n' }}\n                {%- elif content.type == 'audio' or 'audio' in content or 'audio_url' in content %}\n                    {{- '<|im_start|>system\\n' +\"<|audio_start|><|audio_pad|><|audio_end|>\"+ '<|im_end|>\\n' }}\n                {%- elif content.type == 'video' or 'video' in content %}\n                    {{- '<|im_start|>system\\n' +\"<|vision_start|><|video_pad|><|vision_end|>\"+ '<|im_end|>\\n' }}\n                {%- elif content.type == 'text' %}\n                    {{- '<|im_start|>system\\n' +content.text+ '<|im_end|>\\n' }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = namespace(text=\"\") %}\n        {%- for mcontent in message.content %}\n            {%- if mcontent.type == 'image' or 'image' in mcontent or 'image_url' in mcontent %}\n                {%- set content.text = content.text~\"<|vision_start|><|image_pad|><|vision_end|>\" %}\n            {%- elif mcontent.type == 'audio' or 'audio' in mcontent or 'audio_url' in mcontent %}\n                {%- set content.text = content.text~\"<|audio_start|><|audio_pad|><|audio_end|>\" %}\n            {%- elif mcontent.type == 'video' or 'video' in mcontent %}\n                {%- set content.text = content.text~\"<|vision_start|><|video_pad|><|vision_end|>\" %}\n            {%- elif mcontent.type == 'text' %}\n                {%- set content.text = content.text~mcontent.text %}\n            {%- endif %}\n        {%- endfor %}\n        {%- set content = content.text %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = \"\" %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in content %}\n            {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n            {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n        {%- endif %}\n    {%- endif %}\n    {%- if loop.index0 > ns.last_query_index %}\n        {%- if loop.last or (not loop.last and reasoning_content) %}\n            {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip(\"\\n\") + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n    {%- else %}\n        {{- '<|im_start|>' + message.role + '\\n' + content }}\n    {%- endif %}\n    {%- if message.tool_calls %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if (loop.first and content) or (not loop.first) %}{{- '\\n' }}{%- endif %}\n            {%- if tool_call.function %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {%- if tool_call.arguments is string %}\n                {{- tool_call.arguments }}\n            {%- else %}\n                {{- tool_call.arguments | tojson }}\n            {%- endif %}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n    {%- endif %}\n    {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}{{- '<|im_start|>user' }}{%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}{{- '<|im_end|>\\n' }}{%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}{{- '<think>\\n\\n</think>\\n\\n' }}{%- endif %}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_end|>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "tool_parser": "qwen",
    "virtualenv": {
      "packages": [
        "transformers==4.57.1 ; #engine# == \"Transformers\"",
        "accelerate>=0.28.0 ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#system_numpy# ; #engine# == \"vllm\"",
        "qwen_omni_utils",
        "soundfile"
      ]
    },
    "updated_at": 1769418600,
    "featured": false,
    "architectures": [
      "Qwen3OmniMoeForConditionalGeneration"
    ],
    "model_type": "qwen3_omni_moe"
  },
  {
    "version": 2,
    "context_length": 262144,
    "model_name": "Qwen3-Omni-Instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision",
      "audio",
      "omni",
      "tools"
    ],
    "model_description": "Qwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-Omni-30B-A3B-Instruct"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "Qwen/Qwen3-Omni-30B-A3B-Instruct"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn/Qwen3-Omni-30B-A3B-Instruct-AWQ-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "8bit"
            ],
            "model_id": "cpatonn-mirror/Qwen3-Omni-30B-A3B-Instruct-AWQ-{quantization}"
          }
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {%- if messages[0].content is string %}\n            {{- messages[0].content }}\n        {%- else %}\n            {%- for content in messages[0].content %}\n                {%- if content.type == 'image' or 'image' in content or 'image_url' in content %}\n                    {{- \"<|vision_start|><|image_pad|><|vision_end|>\" }}\n                {%- elif content.type == 'audio' or 'audio' in content or 'audio_url' in content %}\n                    {{- \"<|audio_start|><|audio_pad|><|audio_end|>\" }}\n                {%- elif content.type == 'video' or 'video' in content %}\n                    {{- \"<|vision_start|><|video_pad|><|vision_end|>\" }}\n                {%- elif content.type == 'text' %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n    {%- endif %}\n    {{- '\\n\\n' }}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {%- if messages[0].content is string %}\n            {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n        {%- else %}\n            {%- for content in messages[0].content %}\n                {%- if content.type == 'image' or 'image' in content or 'image_url' in content %}\n                    {{- '<|im_start|>system\\n' +\"<|vision_start|><|image_pad|><|vision_end|>\"+ '<|im_end|>\\n' }}\n                {%- elif content.type == 'audio' or 'audio' in content or 'audio_url' in content %}\n                    {{- '<|im_start|>system\\n' +\"<|audio_start|><|audio_pad|><|audio_end|>\"+ '<|im_end|>\\n' }}\n                {%- elif content.type == 'video' or 'video' in content %}\n                    {{- '<|im_start|>system\\n' +\"<|vision_start|><|video_pad|><|vision_end|>\"+ '<|im_end|>\\n' }}\n                {%- elif content.type == 'text' %}\n                    {{- '<|im_start|>system\\n' +content.text+ '<|im_end|>\\n' }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = namespace(text=\"\") %}\n        {%- for mcontent in message.content %}\n            {%- if mcontent.type == 'image' or 'image' in mcontent or 'image_url' in mcontent %}\n                {%- set content.text = content.text~\"<|vision_start|><|image_pad|><|vision_end|>\" %}\n            {%- elif mcontent.type == 'audio' or 'audio' in mcontent or 'audio_url' in mcontent %}\n                {%- set content.text = content.text~\"<|audio_start|><|audio_pad|><|audio_end|>\" %}\n            {%- elif mcontent.type == 'video' or 'video' in mcontent %}\n                {%- set content.text = content.text~\"<|vision_start|><|video_pad|><|vision_end|>\" %}\n            {%- elif mcontent.type == 'text' %}\n                {%- set content.text = content.text~mcontent.text %}\n            {%- endif %}\n        {%- endfor %}\n        {%- set content = content.text %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = \"\" %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in content %}\n            {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n            {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n        {%- endif %}\n    {%- endif %}\n    {%- if loop.index0 > ns.last_query_index %}\n        {%- if loop.last or (not loop.last and reasoning_content) %}\n            {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip(\"\\n\") + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n    {%- else %}\n        {{- '<|im_start|>' + message.role + '\\n' + content }}\n    {%- endif %}\n    {%- if message.tool_calls %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if (loop.first and content) or (not loop.first) %}{{- '\\n' }}{%- endif %}\n            {%- if tool_call.function %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {%- if tool_call.arguments is string %}\n                {{- tool_call.arguments }}\n            {%- else %}\n                {{- tool_call.arguments | tojson }}\n            {%- endif %}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n    {%- endif %}\n    {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}{{- '<|im_start|>user' }}{%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}{{- '<|im_end|>\\n' }}{%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}{{- '<think>\\n\\n</think>\\n\\n' }}{%- endif %}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_end|>"
    ],
    "tool_parser": "qwen",
    "virtualenv": {
      "packages": [
        "transformers==4.57.1 ; #engine# == \"Transformers\"",
        "accelerate>=0.28.0 ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#system_numpy# ; #engine# == \"vllm\"",
        "qwen_omni_utils",
        "soundfile"
      ]
    },
    "updated_at": 1769418602,
    "featured": false,
    "architectures": [
      "Qwen3OmniMoeForConditionalGeneration"
    ],
    "model_type": "qwen3_omni_moe"
  },
  {
    "model_name": "MiniMax-M2",
    "model_description": "MiniMax-M2, a Mini model built for Max coding & agentic workflows.",
    "context_length": 196608,
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools",
      "reasoning"
    ],
    "model_specs": [
      {
        "model_size_in_billions": 230,
        "activated_size_in_billions": 10,
        "model_format": "pytorch",
        "model_src": {
          "huggingface": {
            "model_id": "MiniMaxAI/MiniMax-M2",
            "quantizations": [
              "none"
            ]
          },
          "modelscope": {
            "model_id": "MiniMax/MiniMax-M2",
            "quantizations": [
              "none"
            ]
          }
        }
      },
      {
        "model_size_in_billions": 230,
        "activated_size_in_billions": 10,
        "model_format": "awq",
        "model_src": {
          "huggingface": {
            "model_id": "QuantTrio/MiniMax-M2-AWQ",
            "quantizations": [
              "Int4"
            ]
          },
          "modelscope": {
            "model_id": "tclf90/MiniMax-M2-AWQ",
            "quantizations": [
              "Int4"
            ]
          }
        }
      },
      {
        "model_size_in_billions": 230,
        "activated_size_in_billions": 10,
        "model_format": "mlx",
        "model_src": {
          "huggingface": {
            "model_id": "mlx-community/MiniMax-M2-{quantization}",
            "quantizations": [
              "3bit",
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ]
          },
          "modelscope": {
            "model_id": "mlx-community/MiniMax-M2-{quantization}",
            "quantizations": [
              "3bit",
              "4bit",
              "5bit",
              "6bit",
              "8bit"
            ]
          }
        }
      }
    ],
    "chat_template": "{# ----------‑‑‑ special token variables ‑‑‑---------- #}\n{%- set toolcall_begin_token   = '<minimax:tool_call>'         -%}\n{%- set toolcall_end_token     = '</minimax:tool_call>'        -%}\n{#- Tool Rendering Functions ============================================== -#}\n{%- macro render_tool_namespace(namespace_name, tool_list) -%}\n{%- for tool in tool_list -%}\n<tool>{{ tool.function | tojson(ensure_ascii=False) }}</tool>\n{% endfor -%}\n{%- endmacro -%}\n{%- macro visible_text(content) -%}\n    {%- if content is string -%}\n        {{ content }}\n    {%- elif content is iterable and content is not mapping -%}\n        {%- for item in content -%}\n            {%- if item is mapping and item.type == 'text' -%}\n                {{- item.text }}\n            {%- elif item is string -%}\n                {{- item }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{- content }}\n    {%- endif -%}\n{%- endmacro -%}\n{#- System Message Construction ============================================ -#}\n{%- macro build_system_message(system_message) -%}\n    {%- if system_message and system_message.content -%}\n        {{- visible_text(system_message.content) }}\n    {%- else -%}\n        {%- if model_identity is not defined -%}\n            {%- set model_identity = \"You are a helpful assistant.\" -%}\n        {%- endif -%}\n        {{- model_identity }}\n    {%- endif -%}\n    \n    {#- Handle current_date -#}\n    {%- if system_message and system_message.current_date -%}\n        {{- '\\n' ~ 'Current date: ' + system_message.current_date }}\n    {%- endif -%}\n    {#- Handle current_location -#}\n    {%- if system_message and system_message.current_location -%}\n        {{- '\\n' ~ 'Current location: ' + system_message.current_location }}\n    {%- endif -%}\n{%- endmacro -%}\n{#- Main Template Logic ================================================= -#}\n{#- Extract system message (only first message if it's system) -#}\n{%- set system_message = none -%}\n{%- set conversation_messages = messages -%}\n{%- if messages and messages[0].role == \"system\" -%}\n    {%- set system_message = messages[0] -%}\n    {%- set conversation_messages = messages[1:] -%}\n{%- endif -%}\n{#- Get the last user message turn, for interleved thinking -#}\n{%- set ns = namespace(last_user_index=-1) %}\n{% for m in conversation_messages %}\n    {%- if m.role == 'user' %}\n        {% set ns.last_user_index = loop.index0 -%}\n    {%- endif %}\n{%- endfor %}\n{#- Render system message -#}\n{{- ']~!b[' ~ ']~b]system' ~ '\\n' }}\n{{- build_system_message(system_message) }}\n{#- Render tools if available -#}\n{%- if tools -%}\n    {{- '\\n\\n' ~ '# Tools' ~ '\\n' ~ 'You may call one or more tools to assist with the user query.\\nHere are the tools available in JSONSchema format:' ~ '\\n' }}\n    {{- '\\n' ~ '<tools>' ~ '\\n' }}\n    {{- render_tool_namespace(\"functions\", tools) }}\n    {{- '</tools>' ~ '\\n\\n' }}\n{{- 'When making tool calls, use XML format to invoke tools and pass parameters:' ~ '\\n' }}\n{{- '\\n' ~ toolcall_begin_token }}\n<invoke name=\"tool-name-1\">\n<parameter name=\"param-key-1\">param-value-1</parameter>\n<parameter name=\"param-key-2\">param-value-2</parameter>\n...\n</invoke>\n{{- '\\n' ~ toolcall_end_token }}\n{%- endif -%}\n{{- '[e~[\\n' }}\n\n{#- Render messages -#}\n{%- set last_tool_call = namespace(name=none) -%}\n{%- for message in conversation_messages -%}\n    {%- if message.role == 'assistant' -%}\n        {#- Only render reasoning_content if no user message follows -#}\n        {{- ']~b]ai' ~ '\\n' }}\n\n        {%- set reasoning_content = '' %}\n        {%- set content = visible_text(message.content) %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in content %}\n                {%- set reasoning_content = content.split('</think>')[0].strip('\\n').split('<think>')[-1].strip('\\n') %}\n                {%- set content = content.split('</think>')[-1].strip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if reasoning_content and loop.index0 > ns.last_user_index -%}\n            {{- '<think>' ~ '\\n' ~ reasoning_content ~ '\\n' ~ '</think>' ~ '\\n\\n' }}\n        {%- endif -%}\n        {%- if content -%}\n            {{- content }}\n        {%- endif -%}\n        {%- if message.tool_calls -%}\n            {{- '\\n' ~ toolcall_begin_token ~ '\\n' }}\n\n            {%- for tool_call in message.tool_calls -%}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<invoke name=\"' + tool_call.name + '\">' }}\n                {% set _args = tool_call.arguments %}\n                {%- for k, v in _args.items() %}\n                {{- '<parameter name=\"' + k + '\">' }}\n                {{- v | tojson(ensure_ascii=False) if v is not string else v }}\n                {{- '</parameter>' }}\n                {% endfor %}\n                {{- '</invoke>' ~ '\\n' }}\n            {%- endfor -%}\n            \n            {{- toolcall_end_token}}\n            {%- set last_tool_call.name = message.tool_calls[-1].name -%}\n        {%- else -%}\n            {%- set last_tool_call.name = none -%}\n        {%- endif -%}\n        {{- '[e~[' ~ '\\n' }}\n        \n    {%- elif message.role == 'tool' -%}\n    {%- if last_tool_call.name is none -%}\n        {{- raise_exception(\"Message has tool role, but there was no previous assistant message with a tool call!\") }}\n    {%- endif -%}\n    {%- if loop.first or (conversation_messages[loop.index0 - 1].role != 'tool') -%}\n        {{- ']~b]tool' }}\n    {%- endif -%}\n    {%- if message.content is string -%}\n        {{- '\\n<response>' }}\n        {{- message.content }}\n        {{- '</response>' }}\n    {%- else -%}\n        {%- for tr in message.content -%}\n            {{- '\\n<response>' }}\n            {{- tr.output if tr.output is defined else (tr.text if tr.type == 'text' and tr.text is defined else tr) }}\n            {{- '\\n</response>' }}\n        {%- endfor -%}\n    {%- endif -%}\n    {%- if loop.last or (conversation_messages[loop.index0 + 1].role != 'tool') -%}\n        {{- '[e~[\\n' -}}\n    {%- endif -%}\n        \n    {%- elif message.role == 'user' -%}\n        {{- ']~b]user' ~ '\\n' }}\n        {{- visible_text(message.content) }}\n        {{- '[e~[' ~ '\\n' }}\n    {%- endif -%}\n{%- endfor -%}\n\n{#- Generation prompt -#}\n{%- if add_generation_prompt -%}\n{{- ']~b]ai' ~ '\\n' ~ '<think>' ~ '\\n' }}\n{%- endif -%}",
    "stop_token_ids": [
      200020
    ],
    "stop": [
      "[e~["
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "tool_parser": "minimax",
    "version": 2,
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#mlx_dependencies# ; #engine# == \"MLX\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#sglang_dependencies# ; #engine# == \"sglang\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    },
    "updated_at": 1769418603,
    "featured": false,
    "architectures": [
      "MiniMaxM2ForCausalLM"
    ],
    "model_type": "minimax_m2"
  },
  {
    "model_name": "DeepSeek-V3.2",
    "model_description": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance",
    "context_length": 163840,
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "hybrid",
      "tools"
    ],
    "model_specs": [
      {
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_format": "pytorch",
        "model_src": {
          "huggingface": {
            "model_id": "deepseek-ai/DeepSeek-V3.2",
            "quantizations": [
              "none"
            ]
          },
          "modelscope": {
            "model_id": "deepseek-ai/DeepSeek-V3.2",
            "quantizations": [
              "none"
            ]
          }
        }
      },
      {
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_format": "awq",
        "model_src": {
          "huggingface": {
            "model_id": "QuantTrio/DeepSeek-V3.2-AWQ",
            "quantizations": [
              "Int4"
            ]
          },
          "modelscope": {
            "model_id": "tclf90/DeepSeek-V3.2-AWQ",
            "quantizations": [
              "Int4"
            ]
          }
        }
      }
    ],
    "architectures": [
      "DeepseekV32ForCausalLM"
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}\n  {% set add_generation_prompt = false %}\n{% endif %}\n{% if not thinking is defined %}\n  {% set thinking = false %}\n{% endif %}\n{% set ns = namespace(is_first=false, is_tool=false, system_prompt='', is_first_sp=true, is_last_user=false, is_only_sys=false, is_prefix=false) %}\n{%- for message in messages %}\n  {%- if message['role'] == 'system' %}\n    {%- if ns.is_first_sp %}\n      {% set ns.system_prompt = ns.system_prompt + message['content'] %}\n      {% set ns.is_first_sp = false %}\n    {%- else %}\n      {% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}\n    {%- endif %}\n    {% set ns.is_only_sys = true %}\n  {%- endif %}\n{%- endfor %}\n{{ bos_token }}{{ ns.system_prompt }}\n{%- for message in messages %}\n  {%- if message['role'] == 'user' %}\n    {%- set ns.is_tool = false -%}\n    {%- set ns.is_first = false -%}\n    {%- set ns.is_last_user = true -%}\n    {{'<｜User｜>' + message['content']}}\n  {%- endif %}\n  {%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}\n    {%- if ns.is_last_user or ns.is_only_sys %}\n      {{'<｜Assistant｜></think>'}}\n    {%- endif %}\n    {%- set ns.is_last_user = false -%}\n    {%- set ns.is_first = false %}\n    {%- set ns.is_tool = false -%}\n    {%- for tool in message['tool_calls'] %}\n      {%- if not ns.is_first %}\n        {%- if message['content'] is none %}\n          {{'<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>'+ tool['function']['name'] + '<｜tool▁sep｜>' + tool['function']['arguments'] + '<｜tool▁call▁end｜>'}}\n        {%- else %}\n          {{message['content'] + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['function']['name'] + '<｜tool▁sep｜>' + tool['function']['arguments'] + '<｜tool▁call▁end｜>'}}\n        {%- endif %}\n        {%- set ns.is_first = true -%}\n      {%- else %}\n        {{'<｜tool▁call▁begin｜>'+ tool['function']['name'] + '<｜tool▁sep｜>' + tool['function']['arguments'] + '<｜tool▁call▁end｜>'}}\n      {%- endif %}\n    {%- endfor %}\n    {{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}\n  {%- endif %}\n  {%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none) %}\n    {%- if ns.is_last_user %}\n      {{'<｜Assistant｜>'}}\n      {%- if message['prefix'] is defined and message['prefix'] and thinking %}\n        {{'<think>'}}\n      {%- else %}\n        {{'</think>'}}\n      {%- endif %}\n    {%- endif %}\n    {%- if message['prefix'] is defined and message['prefix'] %}\n      {%- set ns.is_prefix = true -%}\n    {%- endif %}\n    {%- set ns.is_last_user = false -%}\n    {%- if ns.is_tool %}\n      {{message['content'] + '<｜end▁of▁sentence｜>'}}\n      {%- set ns.is_tool = false -%}\n    {%- else %}\n      {%- set content = message['content'] -%}\n      {%- if '</think>' in content %}\n        {%- set content = content.split('</think>', 1)[1] -%}\n      {%- endif %}\n      {{content + '<｜end▁of▁sentence｜>'}}\n    {%- endif %}\n  {%- endif %}\n  {%- if message['role'] == 'tool' %}\n    {%- set ns.is_last_user = false -%}\n    {%- set ns.is_tool = true -%}\n    {{'<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}\n  {%- endif %}\n  {%- if message['role'] != 'system' %}\n    {% set ns.is_only_sys = false %}\n  {%- endif %}\n{%- endfor -%}\n{% if add_generation_prompt and not ns.is_tool%}\n  {% if ns.is_last_user or ns.is_only_sys or not ns.is_prefix %}\n    {{'<｜Assistant｜>'}}\n    {%- if not thinking %}\n      {{'</think>'}}\n    {%- else %}\n      {{'<think>'}}\n    {%- endif %}\n  {% endif %}\n{% endif %}",
    "stop_token_ids": [
      1
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "version": 2,
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    },
    "tool_parser": "deepseek-r1",
    "featured": true,
    "updated_at": 1770087071
  },
  {
    "model_name": "DeepSeek-V3.2-Exp",
    "model_description": "We are excited to announce the official release of DeepSeek-V3.2-Exp, an experimental version of our model. As an intermediate step toward our next-generation architecture, V3.2-Exp builds upon V3.1-Terminus by introducing DeepSeek Sparse Attention—a sparse attention mechanism designed to explore and validate optimizations for training and inference efficiency in long-context scenarios.",
    "context_length": 163840,
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "hybrid",
      "tools"
    ],
    "model_specs": [
      {
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_format": "pytorch",
        "model_src": {
          "huggingface": {
            "model_id": "deepseek-ai/DeepSeek-V3.2-Exp",
            "quantizations": [
              "none"
            ]
          },
          "modelscope": {
            "model_id": "deepseek-ai/DeepSeek-V3.2-Exp",
            "quantizations": [
              "none"
            ]
          }
        }
      },
      {
        "model_size_in_billions": 671,
        "activated_size_in_billions": 37,
        "model_format": "awq",
        "model_src": {
          "huggingface": {
            "model_id": "QuantTrio/DeepSeek-V3.2-Exp-{quantization}",
            "quantizations": [
              "AWQ",
              "AWQ-Lite"
            ]
          },
          "modelscope": {
            "model_id": "tclf90/DeepSeek-V3.2-Exp-{quantization}",
            "quantizations": [
              "AWQ",
              "AWQ-Lite"
            ]
          }
        }
      }
    ],
    "architectures": [
      "DeepseekV32ForCausalLM"
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}\n  {% set add_generation_prompt = false %}\n{% endif %}\n{% if not thinking is defined %}\n  {% set thinking = false %}\n{% endif %}\n{% set ns = namespace(is_first=false, is_tool=false, system_prompt='', is_first_sp=true, is_last_user=false, is_only_sys=false, is_prefix=false) %}\n{%- for message in messages %}\n  {%- if message['role'] == 'system' %}\n    {%- if ns.is_first_sp %}\n      {% set ns.system_prompt = ns.system_prompt + message['content'] %}\n      {% set ns.is_first_sp = false %}\n    {%- else %}\n      {% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}\n    {%- endif %}\n    {% set ns.is_only_sys = true %}\n  {%- endif %}\n{%- endfor %}\n{{ bos_token }}{{ ns.system_prompt }}\n{%- for message in messages %}\n  {%- if message['role'] == 'user' %}\n    {%- set ns.is_tool = false -%}\n    {%- set ns.is_first = false -%}\n    {%- set ns.is_last_user = true -%}\n    {{'<｜User｜>' + message['content']}}\n  {%- endif %}\n  {%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}\n    {%- if ns.is_last_user or ns.is_only_sys %}\n      {{'<｜Assistant｜></think>'}}\n    {%- endif %}\n    {%- set ns.is_last_user = false -%}\n    {%- set ns.is_first = false %}\n    {%- set ns.is_tool = false -%}\n    {%- for tool in message['tool_calls'] %}\n      {%- if not ns.is_first %}\n        {%- if message['content'] is none %}\n          {{'<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>'+ tool['function']['name'] + '<｜tool▁sep｜>' + tool['function']['arguments'] + '<｜tool▁call▁end｜>'}}\n        {%- else %}\n          {{message['content'] + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['function']['name'] + '<｜tool▁sep｜>' + tool['function']['arguments'] + '<｜tool▁call▁end｜>'}}\n        {%- endif %}\n        {%- set ns.is_first = true -%}\n      {%- else %}\n        {{'<｜tool▁call▁begin｜>'+ tool['function']['name'] + '<｜tool▁sep｜>' + tool['function']['arguments'] + '<｜tool▁call▁end｜>'}}\n      {%- endif %}\n    {%- endfor %}\n    {{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}\n  {%- endif %}\n  {%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none) %}\n    {%- if ns.is_last_user %}\n      {{'<｜Assistant｜>'}}\n      {%- if message['prefix'] is defined and message['prefix'] and thinking %}\n        {{'<think>'}}\n      {%- else %}\n        {{'</think>'}}\n      {%- endif %}\n    {%- endif %}\n    {%- if message['prefix'] is defined and message['prefix'] %}\n      {%- set ns.is_prefix = true -%}\n    {%- endif %}\n    {%- set ns.is_last_user = false -%}\n    {%- if ns.is_tool %}\n      {{message['content'] + '<｜end▁of▁sentence｜>'}}\n      {%- set ns.is_tool = false -%}\n    {%- else %}\n      {%- set content = message['content'] -%}\n      {%- if '</think>' in content %}\n        {%- set content = content.split('</think>', 1)[1] -%}\n      {%- endif %}\n      {{content + '<｜end▁of▁sentence｜>'}}\n    {%- endif %}\n  {%- endif %}\n  {%- if message['role'] == 'tool' %}\n    {%- set ns.is_last_user = false -%}\n    {%- set ns.is_tool = true -%}\n    {{'<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}\n  {%- endif %}\n  {%- if message['role'] != 'system' %}\n    {% set ns.is_only_sys = false %}\n  {%- endif %}\n{%- endfor -%}\n{% if add_generation_prompt and not ns.is_tool%}\n  {% if ns.is_last_user or ns.is_only_sys or not ns.is_prefix %}\n    {{'<｜Assistant｜>'}}\n    {%- if not thinking %}\n      {{'</think>'}}\n    {%- else %}\n      {{'<think>'}}\n    {%- endif %}\n  {% endif %}\n{% endif %}",
    "stop_token_ids": [
      1
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "version": 2,
    "virtualenv": {
      "packages": [
        "#transformers_dependencies# ; #engine# == \"Transformers\"",
        "#vllm_dependencies# ; #engine# == \"vllm\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    },
    "tool_parser": "deepseek-r1",
    "featured": true,
    "updated_at": 1770100312
  },
  {
    "version": 2,
    "context_length": 202752,
    "model_name": "GLM-4.6",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "hybrid",
      "tools"
    ],
    "model_description": "GLM-4.6 significantly enhances context length (up to 200K tokens), code generation, reasoning with tool use, agent capabilities, and human-aligned writing compared to GLM-4.5.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 355,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/GLM-4.6"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/GLM-4.6"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 355,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "FP8"
            ],
            "model_id": "zai-org/GLM-4.6-FP8"
          },
          "modelscope": {
            "quantizations": [
              "FP8"
            ],
            "model_id": "ZhipuAI/GLM-4.6-FP8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 355,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "QuantTrio/GLM-4.6-GPTQ-Int4-Int8Mix"
          },
          "modelscope": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "tclf90/GLM-4.6-GPTQ-Int4-Int8Mix"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 355,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/GLM-4.6-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/GLM-4.6-AWQ"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 355,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "5bit"
            ],
            "model_id": "mlx-community/GLM-4.6-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "5bit"
            ],
            "model_id": "mlx-community/GLM-4.6-{quantization}"
          }
        }
      }
    ],
    "chat_template": "[gMASK]<sop>{%- if tools -%}<|system|># Tools\\nYou may call one or more functions to assist with the user query.\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>{% for tool in tools %}{{ tool | tojson(ensure_ascii=False) }}{% endfor %}</tools>\\nFor each function call, output the function name and arguments within the following XML format:\\n<tool_call>{function-name}\\n<arg_key>{arg-key-1}</arg_key>\\n<arg_value>{arg-value-1}</arg_value>\\n<arg_key>{arg-key-2}</arg_key>\\n<arg_value>{arg-value-2}</arg_value>\\n...<tool_call>{%- endif -%}{%- macro visible_text(content) -%}{%- if content is string -%}{{- content }}{%- elif content is iterable and content is not mapping -%}{%- for item in content -%}{%- if item is mapping and item.type == 'text' -%}{{- item.text }}{%- elif item is string -%}{{- item }}{%- endif -%}{%- endfor -%}{%- else -%}{{- content }}{%- endif -%}{%- endmacro -%}{%- set ns = namespace(last_user_index=-1) %}{%- for m in messages %}{%- if m.role == 'user' %}{% set ns.last_user_index = loop.index0 -%}{%- endif %}{%- endfor %}{% for m in messages %}{%- if m.role == 'user' -%}<|user|>{{ visible_text(m.content) }}{{- '/nothink' if (enable_thinking is defined and not enable_thinking and not visible_text(m.content).endswith(\"/nothink\")) else '' }}{%- elif m.role == 'assistant' -%}<|assistant|>{%- set reasoning_content = '' %}{%- set content = visible_text(m.content) %}{%- if m.reasoning_content is string %}{%- set reasoning_content = m.reasoning_content %}{%- else %}{%- if '</think>' in content %}{%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}{%- set content = content.split('</think>')[-1].lstrip('\\n') %}{%- endif %}{%- endif %}{%- if loop.index0 > ns.last_user_index and reasoning_content -%}{{ '\\n<think>\\' + reasoning_content.strip() + '</think>'}}{%- else -%}{{ '\\n<think></think>' }}{%- endif -%}{%- if content.strip() -%}{{ '\\n' + content.strip() }}{%- endif -%}{% if m.tool_calls %}{% for tc in m.tool_calls %}{%- if tc.function %}{%- set tc = tc.function %}{%- endif %}{{ '\\n<tool_call>' + tc.name }}{% set _args = tc.arguments %}{% for k, v in _args.items() %}<arg_key>{{ k }}</arg_key><arg_value>{{ v | tojson(ensure_ascii=False) if v is not string else v }}</arg_value>{% endfor %}<tool_call>{% endfor %}{% endif %}{%- elif m.role == 'tool' -%}{%- if m.content is string -%}{%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}{{- '<|observation|>' }}{%- endif %}{{- '\\n<tool_call>\\n' }}{{- m.content }}{{- '\\n<tool_call>' }}{%- else -%}<|observation|>{% for tr in m.content %}<tool_call>{{ tr.output if tr.output is defined else tr }}<tool_call>{% endfor -%}{% endif -%}{%- elif m.role == 'system' -%}<|system|>{{ visible_text(m.content) }}{%- endif -%}{%- endfor -%}{%- if add_generation_prompt -%}<|assistant|>{{- '\\n<think></think>' if (enable_thinking is defined and not enable_thinking) else '' -}}{%- endif -%}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ],
    "tool_parser": "glm4",
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "virtualenv": {
      "packages": [
        "transformers_dependencies ; #engine# == \"Transformers\"",
        "mlx_dependencies ; #engine# == \"MLX\"",
        "vllm_dependencies ; #engine# == \"vllm\"",
        "sglang_dependencies ; #engine# == \"sglang\"",
        "https://github.com/sgl-project/whl/releases/download/v0.3.20/sgl_kernel-0.3.20+cu130-cp310-abi3-manylinux2014_x86_64.whl ; #engine# == \"sglang\" and cuda_version == \"13.0\" and platform_machine == \"x86_64\"",
        "https://github.com/sgl-project/whl/releases/download/v0.3.20/sgl_kernel-0.3.20+cu130-cp310-abi3-manylinux2014_aarch64.whl ; #engine# == \"sglang\" and cuda_version == \"13.0\" and platform_machine == \"aarch64\"",
        "sgl_kernel ; #engine# == \"sglang\" and cuda_version < \"13.0\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ],
      "extra_index_url": [
        "https://wheels.vllm.ai/0.14.0/cu130",
        "https://download.pytorch.org/whl/cu130"
      ],
      "index_strategy": "unsafe-best-match"
    },
    "featured": true,
    "architectures": [
      "Glm4MoeForCausalLM"
    ],
    "updated_at": 1769162354
  },
  {
    "version": 2,
    "context_length": 202752,
    "model_name": "GLM-4.7",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "hybrid",
      "tools"
    ],
    "model_description": "GLM-4.7 significantly advances core and multilingual agentic coding, UI/vibe coding, tool use, and complex reasoning—outperforming GLM-4.6 across benchmarks like SWE-bench, Terminal Bench 2.0, τ²-Bench, and HLE—while also improving chat, creative writing, and role-play.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 355,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/GLM-4.7"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/GLM-4.7"
          }
        }
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 355,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "FP8"
            ],
            "model_id": "zai-org/GLM-4.7-FP8"
          },
          "modelscope": {
            "quantizations": [
              "FP8"
            ],
            "model_id": "ZhipuAI/GLM-4.7-FP8"
          }
        }
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 355,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "QuantTrio/GLM-4.7-GPTQ-Int4-Int8Mix"
          },
          "modelscope": {
            "quantizations": [
              "Int4-Int8Mix"
            ],
            "model_id": "tclf90/GLM-4.7-GPTQ-Int4-Int8Mix"
          }
        }
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 355,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "QuantTrio/GLM-4.7-AWQ"
          },
          "modelscope": {
            "quantizations": [
              "Int4"
            ],
            "model_id": "tclf90/GLM-4.7-AWQ"
          }
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 355,
        "activated_size_in_billions": 32,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/GLM-4.7-{quantization}"
          },
          "modelscope": {
            "quantizations": [
              "4bit",
              "6bit",
              "8bit"
            ],
            "model_id": "mlx-community/GLM-4.7-{quantization}"
          }
        }
      }
    ],
    "chat_template": "[gMASK]<sop>{%- if tools -%}<|system|># Tools\\nYou may call one or more functions to assist with the user query.\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>{% for tool in tools %}{{ tool | tojson(ensure_ascii=False) }}{% endfor %}</tools>\\nFor each function call, output the function name and arguments within the following XML format:\\n<tool_call>{function-name}<arg_key>{arg-key-1}</arg_key><arg_value>{arg-value-1}</arg_value><arg_key>{arg-key-2}</arg_key><arg_value>{arg-value-2}</arg_value>...<tool_call>{%- endif -%}{%- macro visible_text(content) -%}{%- if content is string -%}{{- content }}{%- elif content is iterable and content is not mapping -%}{%- for item in content -%}{%- if item is mapping and item.type == 'text' -%}{{- item.text }}{%- elif item is string -%}{{- item }}{%- endif -%}{%- endfor -%}{%- else -%}{{- content }}{%- endif -%}{%- endmacro -%}{%- set ns = namespace(last_user_index=-1) %}{%- for m in messages %}{%- if m.role == 'user' %}{% set ns.last_user_index = loop.index0 -%}{%- endif %}{%- endfor %}{% for m in messages %}{%- if m.role == 'user' -%}<|user|>{{ visible_text(m.content) }}{%- elif m.role == 'assistant' -%}<|assistant|>{%- set reasoning_content = '' %}{%- set content = visible_text(m.content) %}{%- if m.reasoning_content is string %}{%- set reasoning_content = m.reasoning_content %}{%- else %}{%- if '</think>' in content %}{%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}{%- set content = content.split('</think>')[-1].lstrip('\\n') %}{%- endif %}{%- endif %}{%- if ((clear_thinking is defined and not clear_thinking) or loop.index0 > ns.last_user_index) and reasoning_content -%}{{ '<think>' + reasoning_content.strip() + '</think>'}}{%- else -%}{{ '</think>' }}{%- endif -%}{%- if content.strip() -%}{{ content.strip() }}{%- endif -%}{% if m.tool_calls %}{% for tc in m.tool_calls %}{%- if tc.function %}{%- set tc = tc.function %}{%- endif %}{{- '<tool_call>' + tc.name -}}{% set _args = tc.arguments %}{% for k, v in _args.items() %}<arg_key>{{ k }}</arg_key><arg_value>{{ v | tojson(ensure_ascii=False) if v is not string else v }}</arg_value>{% endfor %}<tool_call>{% endfor %}{% endif %}{%- elif m.role == 'tool' -%}{%- if m.content is string -%}{%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}{{- '<|observation|>' }}{%- endif %}{{- '<tool_call>' }}{{- m.content }}{{- '<tool_call>' }}{%- else -%}<|observation|>{% for tr in m.content %}<tool_call>{{ tr.output if tr.output is defined else tr }}<tool_call>{% endfor -%}{% endif -%}{%- elif m.role == 'system' -%}<|system|>{{ visible_text(m.content) }}{%- endif -%}{%- endfor -%}{%- if add_generation_prompt -%}<|assistant|>{{- '</think>' if (enable_thinking is defined and not enable_thinking) else '<think>' -}}{%- endif -%}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ],
    "tool_parser": "glm4",
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "virtualenv": {
      "packages": [
        "transformers_dependencies ; #engine# == \"Transformers\"",
        "mlx_dependencies ; #engine# == \"MLX\"",
        "vllm_dependencies ; #engine# == \"vllm\"",
        "sglang_dependencies ; #engine# == \"sglang\"",
        "https://github.com/sgl-project/whl/releases/download/v0.3.20/sgl_kernel-0.3.20+cu130-cp310-abi3-manylinux2014_x86_64.whl ; #engine# == \"sglang\" and cuda_version == \"13.0\" and platform_machine == \"x86_64\"",
        "https://github.com/sgl-project/whl/releases/download/v0.3.20/sgl_kernel-0.3.20+cu130-cp310-abi3-manylinux2014_aarch64.whl ; #engine# == \"sglang\" and cuda_version == \"13.0\" and platform_machine == \"aarch64\"",
        "sgl_kernel ; #engine# == \"sglang\" and cuda_version < \"13.0\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ],
      "extra_index_url": [
        "https://wheels.vllm.ai/0.14.0/cu130",
        "https://download.pytorch.org/whl/cu130"
      ],
      "index_strategy": "unsafe-best-match"
    },
    "architectures": [
      "Glm4MoeForCausalLM"
    ],
    "updated_at": 1770097727
  },
  {
    "version": 2,
    "context_length": 202752,
    "model_name": "GLM-4.7-Flash",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "hybrid",
      "tools"
    ],
    "model_description": "GLM-4.7-Flash is a 30B-A3B MoE model. As the strongest model in the 30B class, it offers a lightweight deployment option that balances performance and efficiency.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "model_src": {
          "huggingface": {
            "quantizations": [
              "none"
            ],
            "model_id": "zai-org/GLM-4.7-Flash"
          },
          "modelscope": {
            "quantizations": [
              "none"
            ],
            "model_id": "ZhipuAI/GLM-4.7-Flash"
          }
        }
      }
    ],
    "chat_template": "[gMASK]<sop>{%- if tools -%}<|system|># Tools\\nYou may call one or more functions to assist with the user query.\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>{% for tool in tools %}{{ tool | tojson(ensure_ascii=False) }}{% endfor %}</tools>\\nFor each function call, output the function name and arguments within the following XML format:\\n<tool_call>{function-name}<arg_key>{arg-key-1}</arg_key><arg_value>{arg-value-1}</arg_value><arg_key>{arg-key-2}</arg_key><arg_value>{arg-value-2}</arg_value>...<tool_call>{%- endif -%}{%- macro visible_text(content) -%}{%- if content is string -%}{{- content }}{%- elif content is iterable and content is not mapping -%}{%- for item in content -%}{%- if item is mapping and item.type == 'text' -%}{{- item.text }}{%- elif item is string -%}{{- item }}{%- endif -%}{%- endfor -%}{%- else -%}{{- content }}{%- endif -%}{%- endmacro -%}{%- set ns = namespace(last_user_index=-1) %}{%- for m in messages %}{%- if m.role == 'user' %}{% set ns.last_user_index = loop.index0 -%}{%- endif %}{%- endfor %}{% for m in messages %}{%- if m.role == 'user' -%}<|user|>{{ visible_text(m.content) }}{%- elif m.role == 'assistant' -%}<|assistant|>{%- set reasoning_content = '' %}{%- set content = visible_text(m.content) %}{%- if m.reasoning_content is string %}{%- set reasoning_content = m.reasoning_content %}{%- else %}{%- if '</think>' in content %}{%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}{%- set content = content.split('</think>')[-1].lstrip('\\n') %}{%- endif %}{%- endif %}{%- if ((clear_thinking is defined and not clear_thinking) or loop.index0 > ns.last_user_index) and reasoning_content -%}{{ '<think>' + reasoning_content.strip() + '</think>'}}{%- else -%}{{ '</think>' }}{%- endif -%}{%- if content.strip() -%}{{ content.strip() }}{%- endif -%}{% if m.tool_calls %}{% for tc in m.tool_calls %}{%- if tc.function %}{%- set tc = tc.function %}{%- endif %}{{- '<tool_call>' + tc.name -}}{% set _args = tc.arguments %}{% for k, v in _args.items() %}<arg_key>{{ k }}</arg_key><arg_value>{{ v | tojson(ensure_ascii=False) if v is not string else v }}</arg_value>{% endfor %}<tool_call>{% endfor %}{% endif %}{%- elif m.role == 'tool' -%}{%- if m.content is string -%}{%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}{{- '<|observation|>' }}{%- endif %}{{- '<tool_call>' }}{{- m.content }}{{- '<tool_call>' }}{%- else -%}<|observation|>{% for tr in m.content %}<tool_call>{{ tr.output if tr.output is defined else tr }}<tool_call>{% endfor -%}{% endif -%}{%- elif m.role == 'system' -%}<|system|>{{ visible_text(m.content) }}{%- endif -%}{%- endfor -%}{%- if add_generation_prompt -%}<|assistant|>{{- '</think>' if (enable_thinking is defined and not enable_thinking) else '<think>' -}}{%- endif -%}",
    "stop_token_ids": [
      154820,
      154827,
      154829
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ],
    "tool_parser": "glm4",
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "virtualenv": {
      "packages": [
        "transformers_dependencies ; #engine# == \"Transformers\"",
        "mlx_dependencies ; #engine# == \"MLX\"",
        "vllm_dependencies ; #engine# == \"vllm\"",
        "sglang_dependencies ; #engine# == \"sglang\"",
        "#system_numpy# ; #engine# == \"vllm\""
      ]
    },
    "architectures": [
      "Glm4MoeLiteForCausalLM"
    ],
    "featured": false,
    "updated_at": 1770196377
  },
  {
    "model_name": "MinerU2.5-2509-1.2B",
    "model_description": "MinerU2.5-2509-1.2B is a vision language model for document understanding.",
    "context_length": 32768,
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_2",
        "model_src": {
          "huggingface": {
            "model_id": "opendatalab/MinerU2.5-2509-1.2B",
            "model_revision": "main",
            "quantizations": [
              "none"
            ]
          },
          "modelscope": {
            "model_id": "opendatalab/MinerU2.5-2509-1.2B",
            "model_revision": "master",
            "quantizations": [
              "none"
            ]
          }
        }
      }
    ],
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "chat_template": "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ],
    "version": 2,
    "virtualenv": {
      "packages": [
        "transformers>=4.45.0 ; #engine# == \"Transformers\"",
        "mineru-vl-utils[transformers] ; #engine# == \"Transformers\"",
        "vllm_dependencies ; #engine# == \"vllm\"",
        "qwen-vl-utils",
        "#system_torch#",
        "#system_numpy#",
        "qwen_omni_utils"
      ]
    },
    "featured": false,
    "updated_at": 1770103567
  }
]
