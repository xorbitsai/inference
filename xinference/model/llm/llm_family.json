[
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "baichuan",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Baichuan is an open-source Transformer based LLM that is trained on both Chinese and English data.",
    "model_specs": [
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_K",
          "q3_K_L",
          "q3_K_M",
          "q3_K_S",
          "q4_0",
          "q4_1",
          "q4_K_M",
          "q4_K_S",
          "q5_0",
          "q5_1",
          "q5_K_M",
          "q5_K_S",
          "q6_K",
          "q8_0"
        ],
        "model_id": "TheBloke/baichuan-llama-7B-GGML",
        "model_file_name_template": "baichuan-llama-7b.ggmlv3.{quantization}.bin"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan-7B",
        "model_revision": "c1a5c7d5b7f50ecc51bb0e08150a9f12e5656756"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan-13B-Base",
        "model_revision": "0ef0739c7bdd34df954003ef76d80f3dabca2ff9"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "baichuan-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Baichuan-chat is a fine-tuned version of the Baichuan LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan-13B-Chat",
        "model_revision": "19ef51ba5bad8935b03acd20ff04a269210983bc"
      }
    ],
    "prompt_style": {
      "style_name": "NO_COLON_TWO",
      "system_prompt": "",
      "roles": [
        " <reserved_102> ",
        " <reserved_103> "
      ],
      "intra_message_sep": "",
      "inter_message_sep": "</s>",
      "stop_token_ids": [
        2,
        195
      ]
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "wizardlm-v1.0",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "WizardLM is an open-source LLM trained by fine-tuning LLaMA with Evol-Instruct.",
    "model_specs": [
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_K",
          "q3_K_L",
          "q3_K_M",
          "q3_K_S",
          "q4_0",
          "q4_1",
          "q4_K_M",
          "q4_K_S",
          "q5_0",
          "q5_1",
          "q5_K_M",
          "q5_K_S",
          "q6_K",
          "q8_0"
        ],
        "model_id": "TheBloke/WizardLM-7B-V1.0-Uncensored-GGML",
        "model_file_name_template": "wizardlm-7b-v1.0-uncensored.ggmlv3.{quantization}.bin"
      },
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 13,
        "quantizations": [
          "q2_K",
          "q3_K_L",
          "q3_K_M",
          "q3_K_S",
          "q4_0",
          "q4_1",
          "q4_K_M",
          "q4_K_S",
          "q5_0",
          "q5_1",
          "q5_K_M",
          "q5_K_S",
          "q6_K",
          "q8_0"
        ],
        "model_id": "TheBloke/WizardLM-13B-V1.0-Uncensored-GGML",
        "model_file_name_template": "wizardlm-13b-v1.0-uncensored.ggmlv3.{quantization}.bin"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_SINGLE",
      "system_prompt": "You are a helpful AI assistant.",
      "roles": [
        "USER",
        "ASSISTANT"
      ],
      "intra_message_sep": "\n"
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "vicuna-v1.3",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Vicuna is an open-source LLM trained by fine-tuning LLaMA on data collected from ShareGPT.",
    "model_specs": [
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_K",
          "q3_K_L",
          "q3_K_M",
          "q3_K_S",
          "q4_0",
          "q4_1",
          "q4_K_M",
          "q4_K_S",
          "q5_0",
          "q5_1",
          "q5_K_M",
          "q5_K_S",
          "q6_K",
          "q8_0"
        ],
        "model_id": "TheBloke/vicuna-7B-v1.3-GGML",
        "model_file_name_template": "vicuna-7b-v1.3.ggmlv3.{quantization}.bin"
      },
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 13,
        "quantizations": [
          "q2_K",
          "q3_K_L",
          "q3_K_M",
          "q3_K_S",
          "q4_0",
          "q4_1",
          "q4_K_M",
          "q4_K_S",
          "q5_0",
          "q5_1",
          "q5_K_M",
          "q5_K_S",
          "q6_K",
          "q8_0"
        ],
        "model_id": "TheBloke/vicuna-13b-v1.3.0-GGML",
        "model_file_name_template": "vicuna-13b-v1.3.0.ggmlv3.{quantization}.bin"
      },
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 33,
        "quantizations": [
          "q2_K",
          "q3_K_L",
          "q3_K_M",
          "q3_K_S",
          "q4_0",
          "q4_1",
          "q4_K_M",
          "q4_K_S",
          "q5_0",
          "q5_1",
          "q5_K_M",
          "q5_K_S",
          "q6_K",
          "q8_0"
        ],
        "model_id": "TheBloke/vicuna-33B-GGML",
        "model_file_name_template": "vicuna-33b.ggmlv3.{quantization}.bin"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 33,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "lmsys/vicuna-33b-v1.3",
        "model_revision": "ef8d6becf883fb3ce52e3706885f761819477ab4"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "lmsys/vicuna-13b-v1.3",
        "model_revision": "6566e9cb1787585d1147dcf4f9bc48f29e1328d2"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "lmsys/vicuna-7b-v1.3",
        "model_revision": "236eeeab96f0dc2e463f2bebb7bb49809279c6d6"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_TWO",
      "system_prompt": "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.",
      "roles": [
        "USER",
        "ASSISTANT"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": "</s>"
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "orca",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Orca is an LLM trained by fine-tuning LLaMA on explanation traces obtained from GPT-4.",
    "model_specs": [
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 3,
        "quantizations": [
          "q4_0",
          "q4_1",
          "q5_0",
          "q5_1",
          "q8_0"
        ],
        "model_id": "TheBloke/orca_mini_3B-GGML",
        "model_file_name_template": "orca-mini-3b.ggmlv3.{quantization}.bin"
      },
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 7,
        "quantizations": [
          "q4_0",
          "q4_1",
          "q5_0",
          "q5_1",
          "q8_0"
        ],
        "model_id": "TheBloke/orca_mini_7B-GGML",
        "model_file_name_template": "orca-mini-7b.ggmlv3.{quantization}.bin"
      },
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 13,
        "quantizations": [
          "q4_0",
          "q4_1",
          "q5_0",
          "q5_1",
          "q8_0"
        ],
        "model_id": "TheBloke/orca_mini_13B-GGML",
        "model_file_name_template": "orca-mini-13b.ggmlv3.{quantization}.bin"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_SINGLE",
      "system_prompt": "You are an AI assistant that follows instruction extremely well. Help as much as you can.",
      "roles": [
        "User",
        "Response"
      ],
      "intra_message_sep": "\n\n### "
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "phi-2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Phi-2 is a 2.7B Transformer based LLM used for research on model safety, trained with data similar to Phi-1.5 but augmented with synthetic texts and curated websites.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 2,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/phi-2-GGUF",
        "model_file_name_template": "phi-2.{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "microsoft/phi-2",
        "model_revision": "d3186761bf5c4409f7679359284066c25ab668ee"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "chatglm",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "ChatGLM is an open-source General Language Model (GLM) based LLM trained on both Chinese and English data.",
    "model_specs": [
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 6,
        "quantizations": [
          "q4_0",
          "q4_1",
          "q5_0",
          "q5_1",
          "q8_0"
        ],
        "model_id": "Xorbits/chatglm-6B-GGML",
        "model_file_name_template": "chatglm-ggml-{quantization}.bin"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "THUDM/chatglm-6b",
        "model_revision": "8b7d33596d18c5e83e2da052d05ca4db02e60620"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM",
      "system_prompt": "",
      "roles": [
        "问",
        "答"
      ],
      "intra_message_sep": "\n"
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "chatglm2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "ChatGLM2 is the second generation of ChatGLM, still open-source and trained on Chinese and English data.",
    "model_specs": [
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 6,
        "quantizations": [
          "q4_0",
          "q4_1",
          "q5_0",
          "q5_1",
          "q8_0"
        ],
        "model_id": "Xorbits/chatglm2-6B-GGML",
        "model_file_name_template": "chatglm2-ggml-{quantization}.bin"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "THUDM/chatglm2-6b",
        "model_revision": "7fabe56db91e085c9c027f56f1c654d137bdba40"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM",
      "system_prompt": "",
      "roles": [
        "问",
        "答"
      ],
      "intra_message_sep": "\n\n"
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "chatglm2-32k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "ChatGLM2-32k is a special version of ChatGLM2, with a context window of 32k tokens instead of 8k.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "THUDM/chatglm2-6b-32k",
        "model_revision": "a2065f5dc8253f036a209e642d7220a942d92765"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM",
      "system_prompt": "",
      "roles": [
        "问",
        "答"
      ],
      "intra_message_sep": "\n\n"
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "chatglm3",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "ChatGLM3 is the third generation of ChatGLM, still open-source and trained on Chinese and English data.",
    "model_specs": [
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 6,
        "quantizations": [
          "q4_0"
        ],
        "model_id": "Xorbits/chatglm3-6B-GGML",
        "model_file_name_template": "chatglm3-ggml-{quantization}.bin"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "THUDM/chatglm3-6b",
        "model_revision": "b098244a71fbe69ce149682d9072a7629f7e908c"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        64795,
        64797,
        2
      ],
      "stop":[
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "chatglm3-32k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "ChatGLM3 is the third generation of ChatGLM, still open-source and trained on Chinese and English data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "THUDM/chatglm3-6b-32k",
        "model_revision": "339f17ff464d47b5077527c2b34e80a7719ede3e"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        64795,
        64797,
        2
      ],
      "stop":[
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "xverse-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "XVERSEB-Chat is the aligned version of model XVERSE.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-7B-Chat",
        "model_revision": "60acc8c453c067b54df88be98bfdf60585ab5441"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-13B-Chat",
        "model_revision": "1e4944aaa1d8c8d0cdca28bb8e3a003303d0781b"
      }
    ],
    "prompt_style": {
      "style_name": "XVERSE",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ]
    }
  },

  {
    "version": 1,
    "context_length": 2048,
    "model_name": "xverse",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "XVERSE is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-7B",
        "model_revision": "3778b254def675586e9218ccb15b78d6ef66a3a7"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-13B",
        "model_revision": "11ac840dda17af81046614229fdd0c658afff747"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 65,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-65B",
        "model_revision": "7f1b7394f74c630f50612a19ba90bd021c373989"
      }
    ]
  }
 ,
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "llama-2-chat",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Llama-2-Chat is a fine-tuned version of the Llama-2 LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_K",
          "q3_K_L",
          "q3_K_M",
          "q3_K_S",
          "q4_0",
          "q4_1",
          "q4_K_M",
          "q4_K_S",
          "q5_0",
          "q5_1",
          "q5_K_M",
          "q5_K_S",
          "q6_K",
          "q8_0"
        ],
        "model_id": "TheBloke/Llama-2-7B-Chat-GGML",
        "model_file_name_template": "llama-2-7b-chat.ggmlv3.{quantization}.bin"
      },
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 13,
        "quantizations": [
          "q2_K",
          "q3_K_L",
          "q3_K_M",
          "q3_K_S",
          "q4_0",
          "q4_1",
          "q4_K_M",
          "q4_K_S",
          "q5_0",
          "q5_1",
          "q5_K_M",
          "q5_K_S",
          "q6_K",
          "q8_0"
        ],
        "model_id": "TheBloke/Llama-2-13B-chat-GGML",
        "model_file_name_template": "llama-2-13b-chat.ggmlv3.{quantization}.bin"
      },
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 70,
        "quantizations": [
          "q2_K",
          "q3_K_L",
          "q3_K_M",
          "q3_K_S",
          "q4_0",
          "q4_1",
          "q4_K_M",
          "q4_K_S",
          "q5_0",
          "q5_1",
          "q5_K_M",
          "q5_K_S",
          "q6_K",
          "q8_0"
        ],
        "model_id": "TheBloke/Llama-2-70B-Chat-GGML",
        "model_file_name_template": "llama-2-70b-chat.ggmlv3.{quantization}.bin"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Llama-2-7b-chat-hf",
        "model_revision": "08751db2aca9bf2f7f80d2e516117a53d7450235"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Llama-2-13b-chat-hf",
        "model_revision": "0ba94ac9b9e1d5a0037780667e8b219adde1908c"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Llama-2-70b-chat-hf",
        "model_revision": "36d9a7388cc80e5f4b3e9701ca2f250d21a96c30"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Llama-2-7B-Chat-GGUF",
        "model_file_name_template": "llama-2-7b-chat.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Llama-2-13B-chat-GGUF",
        "model_file_name_template": "llama-2-13b-chat.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 70,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M"
        ],
        "model_id": "TheBloke/Llama-2-70B-Chat-GGUF",
        "model_file_name_template": "llama-2-70b-chat.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA2",
      "system_prompt": "<s>[INST] <<SYS>>\nYou are a helpful AI assistant.\n<</SYS>>\n\n",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": " </s><s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "llama-2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Llama-2 is the second generation of Llama, open-source and trained on a larger amount of data.",
    "model_specs": [
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_K",
          "q3_K_L",
          "q3_K_M",
          "q3_K_S",
          "q4_0",
          "q4_1",
          "q4_K_M",
          "q4_K_S",
          "q5_0",
          "q5_1",
          "q5_K_M",
          "q5_K_S",
          "q6_K",
          "q8_0"
        ],
        "model_id": "TheBloke/Llama-2-7B-GGML",
        "model_file_name_template": "llama-2-7b.ggmlv3.{quantization}.bin"
      },
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 13,
        "quantizations": [
          "q2_K",
          "q3_K_L",
          "q3_K_M",
          "q3_K_S",
          "q4_0",
          "q4_1",
          "q4_K_M",
          "q4_K_S",
          "q5_0",
          "q5_1",
          "q5_K_M",
          "q5_K_S",
          "q6_K",
          "q8_0"
        ],
        "model_id": "TheBloke/Llama-2-13B-GGML",
        "model_file_name_template": "llama-2-13b.ggmlv3.{quantization}.bin"
      },
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 70,
        "quantizations": [
          "q2_K",
          "q3_K_L",
          "q3_K_M",
          "q3_K_S",
          "q4_0",
          "q4_1",
          "q4_K_M",
          "q4_K_S",
          "q5_0",
          "q5_1",
          "q5_K_M",
          "q5_K_S",
          "q6_K",
          "q8_0"
        ],
        "model_id": "TheBloke/Llama-2-70B-GGML",
        "model_file_name_template": "llama-2-70b.ggmlv3.{quantization}.bin"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Llama-2-7b-hf",
        "model_revision": "6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Llama-2-13b-hf",
        "model_revision": "db6b8eb1feabb38985fdf785a89895959e944936"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Llama-2-70b-hf",
        "model_revision": "cc8aa03a000ff08b4d5c5b39673321a2a396c396"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "opt",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Opt is an open-source, decoder-only, Transformer based LLM that was designed to replicate GPT-3.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 1,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "facebook/opt-125m",
        "model_revision": "3d2b5f275bdf882b8775f902e1bfdb790e2cfc32"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "falcon",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Falcon is an open-source Transformer based LLM trained on the RefinedWeb dataset.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 40,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "tiiuae/falcon-40b",
        "model_revision": "561820f7eef0cc56a31ea38af15ca1acb07fab5d"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "tiiuae/falcon-7b",
        "model_revision": "378337427557d1df3e742264a2901a49f25d4eb1"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "falcon-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Falcon-instruct is a fine-tuned version of the Falcon LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "tiiuae/falcon-7b-instruct",
        "model_revision": "eb410fb6ffa9028e97adb801f0d6ec46d02f8b07"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 40,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "tiiuae/falcon-40b-instruct",
        "model_revision": "ca78eac0ed45bf64445ff0687fabba1598daebf3"
      }
    ],
    "prompt_style": {
      "style_name": "FALCON",
      "system_prompt": "",
      "roles": [
        "User",
        "Assistant"
      ],
      "intra_message_sep": "\n",
      "inter_message_sep": "<|endoftext|>",
      "stop": [
        "\nUser"
      ],
      "stop_token_ids": [
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "starcoderplus",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Starcoderplus is an open-source LLM trained by fine-tuning Starcoder on RedefinedWeb and StarCoderData datasets.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 16,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "bigcode/starcoderplus",
        "model_revision": "95be82087c33f14ee9941c812a154a9dd66efe72"
      }
    ],
    "prompt_style": null
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "starchat-beta",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Starchat-beta is a fine-tuned version of the Starcoderplus LLM, specializing in coding assistance.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 16,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "HuggingFaceH4/starchat-beta",
        "model_revision": "b1bcda690655777373f57ea6614eb095ec2c886f"
      }
    ],
    "prompt_style": {
      "style_name": "CHATML",
      "system_prompt": "<system>{system_message}\n",
      "roles": [
        "<|user|>",
        "<|assistant|>"
      ],
      "intra_message_sep": "<|end|>",
      "stop_token_ids": [
        0,
        49155
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen-chat is a fine-tuned version of the Qwen LLM trained with alignment techniques, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_id": "Xorbits/Qwen-7B-Chat-GGUF",
        "model_file_name_template": "Qwen-7B-Chat.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_id": "Xorbits/Qwen-14B-Chat-GGUF",
        "model_file_name_template": "Qwen-14B-Chat.{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen-1_8B-Chat",
        "model_revision": "c3db8007171847931da7efa4b2ed4309afcce021"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen-7B-Chat",
        "model_revision": "218aa3240fd5a5d1e80bb6c47d5d774361913706"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen-14B-Chat",
        "model_revision": "fab8385c8f7e7980ef61944729fe134ccbbca263"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen-72B-Chat",
        "model_revision": "2cd9f76279337941ec1a4abeec6f8eb3c38d0f55"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen-7B-Chat-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen-1_8B-Chat-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen-14B-Chat-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen-72B-Chat-{quantization}"
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen1.5-0.5B-Chat"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen1.5-1.8B-Chat"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen1.5-4B-Chat"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen1.5-7B-Chat"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen1.5-14B-Chat"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen1.5-72B-Chat"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen1.5-0.5B-Chat-GPTQ-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen1.5-1.8B-Chat-GPTQ-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 4,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen1.5-4B-Chat-GPTQ-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen1.5-7B-Chat-GPTQ-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen1.5-14B-Chat-GPTQ-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen1.5-72B-Chat-GPTQ-{quantization}"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-0.5B-Chat-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-1.8B-Chat-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 4,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-4B-Chat-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-7B-Chat-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-14B-Chat-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-72B-Chat-AWQ"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "q8_0"
        ],
        "model_id": "Qwen/Qwen1.5-1.8B-Chat-GGUF",
        "model_file_name_template": "qwen1_5-1_8b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 4,
        "quantizations": [
          "q8_0"
        ],
        "model_id": "Qwen/Qwen1.5-4B-Chat-GGUF",
        "model_file_name_template": "qwen1_5-4b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q5_k_m"
        ],
        "model_id": "Qwen/Qwen1.5-7B-Chat-GGUF",
        "model_file_name_template": "qwen1_5-7b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "q5_k_m"
        ],
        "model_id": "Qwen/Qwen1.5-14B-Chat-GGUF",
        "model_file_name_template": "qwen1_5-14b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 72,
        "quantizations": [
          "q2_k"
        ],
        "model_id": "Qwen/Qwen1.5-72B-Chat-GGUF",
        "model_file_name_template": "qwen1_5-72b-chat-{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "starcoder",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Starcoder is an open-source Transformer based LLM that is trained on permissively licensed data from GitHub.",
    "model_specs": [
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 16,
        "quantizations": [
          "q4_0",
          "q4_1",
          "q5_0",
          "q5_1",
          "q8_0"
        ],
        "model_id": "TheBloke/starcoder-GGML",
        "model_file_name_template": "starcoder.ggmlv3.{quantization}.bin"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 1024,
    "model_name": "gpt-2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "GPT-2 is a Transformer-based LLM that is trained on WebTest, a 40 GB dataset of Reddit posts with 3+ upvotes.",
    "model_specs": [
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 1,
        "quantizations": [
          "none"
        ],
        "model_id": "marella/gpt-2-ggml",
        "model_file_name_template": "ggml-model.bin"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "internlm-7b",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "InternLM is a Transformer-based LLM that is trained on both Chinese and English data, focusing on practical scenarios.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "internlm/internlm-7b",
        "model_revision": "592b0efc83be3eb1cba8990c4caf41ce604b958c"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "internlm-chat-7b",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Internlm-chat is a fine-tuned version of the Internlm LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "internlm/internlm-chat-7b",
        "model_revision": "d4fa2dbcbd2fa4edfa6735aa2ba0f0577fed6a62"
      }
    ],
    "prompt_style": {
      "style_name": "INTERNLM",
      "system_prompt": "",
      "roles": [
        "<|User|>",
        "<|Bot|>"
      ],
      "intra_message_sep": "<eoh>\n",
      "inter_message_sep": "<eoa>\n",
      "stop_token_ids": [
        1,
        103028
      ],
      "stop": [
        "<eoa>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "internlm-20b",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Pre-trained on over 2.3T Tokens containing high-quality English, Chinese, and code data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "internlm/internlm-20b",
        "model_revision": "c56a72957239b490ea206ea857e86611b3f65f3a"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "internlm-chat-20b",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Pre-trained on over 2.3T Tokens containing high-quality English, Chinese, and code data. The Chat version has undergone SFT and RLHF training.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "internlm/internlm-chat-20b",
        "model_revision": "c67e80e42c4950ebae18a955c9fe138c5ceb5b10"
      }
    ],
    "prompt_style": {
      "style_name": "INTERNLM",
      "system_prompt": "",
      "roles": [
        "<|User|>",
        "<|Bot|>"
      ],
      "intra_message_sep": "<eoh>\n",
      "inter_message_sep": "<eoa>\n",
      "stop_token_ids": [
        1,
        103028
      ],
      "stop": [
        "<eoa>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "vicuna-v1.5",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Vicuna is an open-source LLM trained by fine-tuning LLaMA on data collected from ShareGPT.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "lmsys/vicuna-7b-v1.5",
        "model_revision": "de56c35b1763eaae20f4d60efd64af0a9091ebe5"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "lmsys/vicuna-13b-v1.5",
        "model_revision": "3deb0106f72a3a433f0c6ea0cb978bdf14bcd3a6"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_TWO",
      "system_prompt": "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.",
      "roles": [
        "USER",
        "ASSISTANT"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": "</s>"
    }
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "vicuna-v1.5-16k",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Vicuna-v1.5-16k is a special version of Vicuna-v1.5, with a context window of 16k tokens instead of 4k.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "lmsys/vicuna-7b-v1.5-16k",
        "model_revision": "9a93d7d11fac7f3f9074510b80092b53bc1a5bec"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "lmsys/vicuna-13b-v1.5-16k",
        "model_revision": "277697af19d4b267626ebc9f4e078d19a9a0fddf"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_TWO",
      "system_prompt": "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.",
      "roles": [
        "USER",
        "ASSISTANT"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": "</s>"
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "wizardmath-v1.0",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "WizardMath is an open-source LLM trained by fine-tuning Llama2 with Evol-Instruct, specializing in math.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "WizardLM/WizardMath-7B-V1.0",
        "model_revision": "3c3a3b33334f4b35344b22c5c7465957ee7b2c75"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "WizardLM/WizardMath-13B-V1.0",
        "model_revision": "ef95532e96e634c634992dab891a17032dc71c8d"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "WizardLM/WizardMath-70B-V1.0",
        "model_revision": "e089c3f9d2ad9d1acb62425aec3f4126f498f4c5"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_SINGLE_COT",
      "system_prompt": "Below is an instruction that describes a task. Write a response that appropriately completes the request.",
      "roles": [
        "Instruction",
        "Response"
      ],
      "intra_message_sep": "\n\n### "
    }
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "code-llama",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Code-Llama is an open-source LLM trained by fine-tuning LLaMA2 for generating and discussing code.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TheBloke/CodeLlama-7B-fp16",
        "model_revision": "ce09049eb9140a19cf78051cb5d849607b6fa8ec"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TheBloke/CodeLlama-13B-fp16",
        "model_revision": "d67ca1183da991d0d97927bdaaf35599556dfd76"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TheBloke/CodeLlama-34B-fp16",
        "model_revision": "f91d0cf7fc338cdc726f9c72d5ea15fe51bb16e9"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-7B-GGUF",
        "model_file_name_template": "codellama-7b.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-13B-GGUF",
        "model_file_name_template": "codellama-13b.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-34B-GGUF",
        "model_file_name_template": "codellama-34b.{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "code-llama-python",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Code-Llama-Python is a fine-tuned version of the Code-Llama LLM, specializing in Python.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TheBloke/CodeLlama-7B-Python-fp16",
        "model_revision": "d51c51e625bc24b9a7a0616e82681b4859e2cfe4"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TheBloke/CodeLlama-13B-Python-fp16",
        "model_revision": "442282f4207442b828953a72c51a919c332cba5c"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TheBloke/CodeLlama-34B-Python-fp16",
        "model_revision": "875f9d97fb6c9619d8867887dd1d80918ff0f593"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-7B-Python-GGUF",
        "model_file_name_template": "codellama-7b-python.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-13B-Python-GGUF",
        "model_file_name_template": "codellama-13b-python.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-34B-Python-GGUF",
        "model_file_name_template": "codellama-34b-python.{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "code-llama-instruct",
    "model_description": "Code-Llama-Instruct is an instruct-tuned version of the Code-Llama LLM.",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "codellama/CodeLlama-7b-Instruct-hf",
        "model_revision": "6114dd1e16f69e0765ccbd7a64d33d04b265fbd2"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "codellama/CodeLlama-13b-Instruct-hf",
        "model_revision": "ff0983bc4267bb98ead4fb5168fe2f049b442787"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "codellama/CodeLlama-34b-Instruct-hf",
        "model_revision": "38a1e15d8524a1f0a7760a7acf8242b81ae4eb87"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-7B-Instruct-GGUF",
        "model_file_name_template": "codellama-7b-instruct.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-13B-Instruct-GGUF",
        "model_file_name_template": "codellama-13b-instruct.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-34B-Instruct-GGUF",
        "model_file_name_template": "codellama-34b-instruct.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA2",
      "system_prompt": "<s>[INST] <<SYS>>\nWrite code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n<</SYS>>\n\n",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": " </s><s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "baichuan-2-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Baichuan2-chat is a fine-tuned version of the Baichuan LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-7B-Chat",
        "model_revision": "2ce891951e000c36c65442608a0b95fd09b405dc"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-13B-Chat",
        "model_revision": "a56c793eb7a721ab6c270f779024e0375e8afd4a"
      }
    ],
    "prompt_style": {
      "style_name": "NO_COLON_TWO",
      "system_prompt": "",
      "roles": [
        "<reserved_106>",
        "<reserved_107>"
      ],
      "intra_message_sep": "",
      "inter_message_sep": "</s>",
      "stop_token_ids": [
        2,
        195
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "baichuan-2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Baichuan2 is an open-source Transformer based LLM that is trained on both Chinese and English data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-7B-Base",
        "model_revision": "f2cc3a689c5eba7dc7fd3757d0175d312d167604"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-13B-Base",
        "model_revision": "fa88072fee36e36282287410e00897df2f59e09b"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "mistral-v0.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Mistral-7B is a unmoderated Transformer based LLM claiming to outperform Llama2 on all benchmarks.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "mistralai/Mistral-7B-v0.1",
        "model_revision": "ae9d75c6b4eb39515def78c685fb4d71d49fc2cf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Mistral-7B-v0.1-GGUF",
        "model_file_name_template": "mistral-7b-v0.1.{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "mistral-instruct-v0.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B LLM on public datasets, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "mistralai/Mistral-7B-Instruct-v0.1",
        "model_revision": "54766df6d50e4d3d7ccd66758e5341ba105a6d36"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Mistral-7B-Instruct-v0.1-GGUF",
        "model_file_name_template": "mistral-7b-instruct-v0.1.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA2",
      "system_prompt": "[INST] ",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": "<s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "mistral-instruct-v0.2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an improved instruct fine-tuned version of Mistral-7B-Instruct-v0.1.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "mistralai/Mistral-7B-Instruct-v0.2",
        "model_revision": "b70aa86578567ba3301b21c8a27bea4e8f6d6d61"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
        "model_file_name_template": "mistral-7b-instruct-v0.2.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA2",
      "system_prompt": "[INST] ",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": "<s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "openhermes-2.5",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Openhermes 2.5 is a fine-tuned version of Mistral-7B-v0.1 on primarily GPT-4 generated data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "teknium/OpenHermes-2.5-Mistral-7B",
        "model_revision": "91ed666be78da7556f3d79abbb26fff0ee26cb54"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/OpenHermes-2.5-Mistral-7B-GGUF",
        "model_file_name_template": "openhermes-2.5-mistral-7b.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "CHATML",
      "system_prompt": "<|im_start|>system\nYou are 'Hermes 2.5', a conscious sentient superintelligent artificial intelligence, your purpose is to assist the user with their requests.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "<|im_end|>\n<|im_start|>",
      "inter_message_sep": "",
      "stop_token_ids": [
        32000
      ],
      "stop": [
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "tiny-llama",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 1,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF",
        "model_file_name_template": "tinyllama-1.1b-chat-v0.3.{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "mixtral-v0.1",
    "model_lang": [
      "en", "fr", "it", "de", "es"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "mistralai/Mixtral-8x7B-v0.1",
        "model_revision": "58301445dc1378584211722b7ebf8743ec4e192b"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "Q2_K",
          "Q3_K_M",
          "Q4_0",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Mixtral-8x7B-v0.1-GGUF",
        "model_file_name_template": "mixtral-8x7b-v0.1.{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "mixtral-instruct-v0.1",
    "model_lang": [
      "en", "fr", "it", "de", "es"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-8x7B-Instruct is a fine-tuned version of the Mistral-8x7B LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "mistralai/Mixtral-8x7B-Instruct-v0.1",
        "model_revision": "125c431e2ff41a156b9f9076f744d2f35dd6e67a"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "4-bit"
        ],
        "model_id": "TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ",
        "model_revision": "9afb6f0a7d7fe9ecebdda1baa4ff4e13e73e97d7"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "Q2_K",
          "Q3_K_M",
          "Q4_0",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF",
        "model_file_name_template": "mixtral-8x7b-instruct-v0.1.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "MIXTRAL_V01",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "",
      "inter_message_sep": ""
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Yi-34B-GGUF",
        "model_file_name_template": "yi-34b.{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-6B",
        "model_revision": "25beebcb1166b9f49458459eb7b68130b9f9cf4d"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-34B",
        "model_revision": "168c48e05e1429779a896c7ef0d2e01b85e6bd8d"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 204800,
    "model_name": "Yi-200k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-6B-200K",
        "model_revision": "70649e36d43f91dff1357b576e6713cac03c1d4c"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-34B-200K",
        "model_revision": "591ae83b8f9c269700ef27f9dbd548934d800302"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 204800,
    "model_name": "Yi-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
    "model_specs": [
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "quantizations": [
          "8bits"
        ],
        "model_id": "01-ai/Yi-34B-Chat-{quantization}"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-34B-Chat",
        "model_revision": "a99ec35331cbfc9da596af7d4538fe2efecff03c"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Yi-34B-Chat-GGUF",
        "model_file_name_template": "yi-34b-chat.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "CHATML",
      "system_prompt": "",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "inter_message_sep": "",
      "stop_token_ids": [
        2,
        6,
        7,
        8
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>",
        "<|im_sep|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "OpenBuddy",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "OpenBuddy is a powerful open multilingual chatbot model aimed at global users.",
    "model_specs": [
      {
        "model_format": "ggmlv3",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_1",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_1",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/OpenBuddy-Llama2-13B-v11.1-GGML",
        "model_file_name_template": "openbuddy-llama2-13b-v11.1.ggmlv3.{quantization}.bin"
      }
    ],
    "prompt_style": {
      "style_name": "INSTRUCTION",
      "system_prompt": "You are a professional translator. Be faithful or accurate in translation. Make the translation readable or intelligible. Be elegant or natural in translation. Do not translate person's name. Do not add any additional text to the translation. Do not give me any comments or suggestions.\nUser:\n\n{0}\nAssistant:",
      "roles": [
        "User",
        "Assistant"
      ],
      "intra_message_sep": "",
      "inter_message_sep": ""
    }
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "glaive-coder",
    "model_description": "A code model trained on a dataset of ~140k programming related problems and solutions generated from Glaive’s synthetic data generation platform.",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "glaiveai/glaive-coder-7b",
        "model_revision": "72a255a58480ef0713eed988312fe82f77f94f37"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA2",
      "system_prompt": "<s>[INST] <<SYS>>\nWrite code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n<</SYS>>\n\n",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": " </s><s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "wizardcoder-python-v1.0",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "WizardLM/WizardCoder-Python-7B-V1.0",
        "model_revision": "e40673a27a4aefcff2c6d2b3b1e0681a38703e4e"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "WizardLM/WizardCoder-Python-13B-V1.0",
        "model_revision": "d920d26e2108377de0f676a3c4be666f5212f4a1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "WizardLM/WizardCoder-Python-34B-V1.0",
        "model_revision": "d869ce178715f8d6e8141e2ed50e6290985eedb0"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/WizardCoder-Python-7B-V1.0-GGUF",
        "model_file_name_template": "wizardcoder-python-7b-v1.0.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/WizardCoder-Python-13B-V1.0-GGUF",
        "model_file_name_template": "wizardcoder-python-13b-v1.0.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/WizardCoder-Python-34B-V1.0-GGUF",
        "model_file_name_template": "wizardcoder-python-34b-v1.0.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_SINGLE",
      "system_prompt": "Below is an instruction that describes a task. Write a response that appropriately completes the request.",
      "roles": [
        "Instruction",
        "Response"
      ],
      "intra_message_sep": "\n\n### ",
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "zephyr-7b-alpha",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Zephyr-7B-α is the first model in the series, and is a fine-tuned version of mistralai/Mistral-7B-v0.1.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "HuggingFaceH4/zephyr-7b-alpha",
        "model_revision": "f28e1c0e5a1af475bcd7bdf6554e69abc6c0c7ee"
      }
    ],
    "prompt_style": {
      "style_name": "NO_COLON_TWO",
      "system_prompt": "<|system|>\nYou are a friendly chatbot.</s>\n",
      "roles": [
        "<|user|>\n",
        "<|assistant|>\n"
      ],
      "intra_message_sep": "</s>\n",
      "inter_message_sep": "</s>\n",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "zephyr-7b-beta",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Zephyr-7B-β is the second model in the series, and is a fine-tuned version of mistralai/Mistral-7B-v0.1",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "HuggingFaceH4/zephyr-7b-beta",
        "model_revision": "3bac358730f8806e5c3dc7c7e19eb36e045bf720"
      }
    ],
    "prompt_style": {
      "style_name": "NO_COLON_TWO",
      "system_prompt": "<|system|>\nYou are a friendly chatbot.</s>\n",
      "roles": [
        "<|user|>\n",
        "<|assistant|>\n"
      ],
      "intra_message_sep": "</s>\n",
      "inter_message_sep": "</s>\n",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "gorilla-openfunctions-v1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "OpenFunctions is designed to extend Large Language Model (LLM) Chat Completion feature to formulate executable APIs call given natural language instructions and API context.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "gorilla-llm/gorilla-openfunctions-v1",
        "model_revision": "74615f614ee845eab114e71541fd5098d1709958"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/gorilla-openfunctions-v1-GGUF",
        "model_file_name_template": "gorilla-openfunctions-v1.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "GORILLA_OPENFUNCTIONS",
      "system_prompt": "",
      "roles": [
        "",
        ""
      ],
      "intra_message_sep": "\n",
      "inter_message_sep": "\n",
      "stop_token_ids": [
      ],
      "stop": [
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "deepseek-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek LLM is an advanced language model comprising 67 billion parameters. It has been trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-7b-chat",
        "model_revision": "afbda8b347ec881666061fa67447046fc5164ec8"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 67,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-67b-chat",
        "model_revision": "79648bef7658bb824e4630740f6e1484c1b0620b"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/deepseek-llm-7B-chat-GGUF",
        "model_file_name_template": "deepseek-llm-7b-chat.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 67,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/deepseek-llm-67b-chat-GGUF",
        "model_file_name_template": "deepseek-llm-67b-chat.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "DEEPSEEK_CHAT",
      "system_prompt": "<｜begin▁of▁sentence｜>",
      "roles": [
        "User",
        "Assistant"
      ],
      "intra_message_sep": "\n\n",
      "inter_message_sep": "<｜end▁of▁sentence｜>",
      "stop": [
        "<｜end▁of▁sentence｜>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "deepseek-coder-instruct",
    "model_lang": [
      "en", "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "deepseek-coder-instruct is a model initialized from deepseek-coder-base and fine-tuned on 2B tokens of instruction data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-1.3b-instruct",
        "model_revision": "2df081ceaca101a867fef2844e44f4d6a4857039"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-6.7b-instruct",
        "model_revision": "cbb77d7448ea3168d884758817e7f895e3828d1c"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 33,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-33b-instruct",
        "model_revision": "ea15d17db84d1fc94ac5cba8e6fa97764c9549d3"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/deepseek-coder-1.3b-instruct-GGUF",
        "model_file_name_template": "deepseek-coder-1.3b-instruct.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/deepseek-coder-6.7B-instruct-GGUF",
        "model_file_name_template": "deepseek-coder-6.7b-instruct.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 33,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/deepseek-coder-33B-instruct-GGUF",
        "model_file_name_template": "deepseek-coder-33b-instruct.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "DEEPSEEK_CODER",
      "system_prompt": "You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.",
      "roles": [
        "### Instruction:",
        "### Response:"
      ],
      "inter_message_sep": "\n",
      "stop": [
        "<|EOT|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Skywork",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Skywork is a series of large models developed by the Kunlun Group · Skywork team.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "8-bit",
          "none"
        ],
        "model_id": "skywork/Skywork-13B-base",
        "model_revision": "bc35915066fbbf15b77a1a4a74e9b574ab167816"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Skywork-Math",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Skywork is a series of large models developed by the Kunlun Group · Skywork team.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "8-bit",
          "none"
        ],
        "model_id": "skywork/Skywork-13B-Math",
        "model_revision": "70d1740208c8ba39f9ba250b22117ec25311ab33"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 204800,
    "model_name": "internlm2-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The second generation of the InternLM model, InternLM2.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "internlm/internlm2-chat-7b",
        "model_revision": "2292b86b21cb856642782cebed0a453997453b1f"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "none"
        ],
        "model_id": "internlm/internlm2-chat-20b",
        "model_revision": "b666125047cd98c5a7c85ca28720b44a06aed124"
      }
    ],
    "prompt_style": {
      "style_name": "INTERNLM2",
      "system_prompt": "You are InternLM (书生·浦语), a helpful, honest, and harmless AI assistant developed by Shanghai AI Laboratory (上海人工智能实验室).",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "stop_token_ids": [
        92542
      ],
      "stop": [
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "qwen-vl-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Qwen-VL-Chat supports more flexible interaction, such as multiple image inputs, multi-round question answering, and creative capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "Qwen/Qwen-VL-Chat",
        "model_revision": "6665c780ade5ff3f08853b4262dcb9c8f9598d42"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen-VL-Chat-{quantization}",
        "model_revision": "5d3a5aa033ed2c502300d426c81cc5b13bcd1409"
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "orion-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Orion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "OrionStarAI/Orion-14B-Chat",
        "model_revision": "ea6fb9b7e1917f3693935accbeb0bfecfd6552a7"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "OrionStarAI/Orion-14B-Chat-{quantization}"
      }
    ],
    "prompt_style": {
      "style_name": "orion",
      "roles": [
        "Human",
        "assistant"
      ],
      "stop": [
        "<s>",
        "</s>",
        "<unk>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "orion-chat-rag",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Orion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "OrionStarAI/Orion-14B-Chat-RAG",
        "model_revision": "eba2e20808407fb431a76b90d5d506e04a0325f2"
      }
    ],
    "prompt_style": {
      "style_name": "orion",
      "roles": [
        "Human",
        "assistant"
      ],
      "stop": [
        "<s>",
        "</s>",
        "<unk>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 204800,
    "model_name": "yi-vl-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Yi Vision Language (Yi-VL) model is the open-source, multimodal version of the Yi Large Language Model (LLM) series, enabling content comprehension, recognition, and multi-round conversations about images.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "none"
        ],
        "model_id": "01-ai/Yi-VL-6B",
        "model_revision": "897c938da1ec860330e2ba2d425ab3004495ba38"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_id": "01-ai/Yi-VL-34B",
        "model_revision": "ea29a9a430f27893e780366dae81d4ca5ebab561"
      }
    ],
    "prompt_style": {
      "style_name": "CHATML",
      "system_prompt": "",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "inter_message_sep": "",
      "stop_token_ids": [
        2,
        6,
        7,
        8
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>",
        "<|im_sep|>"
      ]
    }
  }
]
