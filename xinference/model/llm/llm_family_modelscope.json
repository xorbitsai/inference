[
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "llama-2-chat",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Llama-2-Chat is a fine-tuned version of the Llama-2 LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_id": "Xorbits/Llama-2-7b-Chat-GGUF",
        "model_file_name_template": "llama-2-7b-chat.{quantization}.gguf",
        "model_hub": "modelscope",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_id": "Xorbits/Llama-2-13b-Chat-GGUF",
        "model_file_name_template": "llama-2-13b-chat.{quantization}.gguf",
        "model_hub": "modelscope",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "modelscope/Llama-2-7b-chat-ms",
        "model_hub": "modelscope",
        "model_revision": "v1.0.5"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "modelscope/Llama-2-13b-chat-ms",
        "model_hub": "modelscope",
        "model_revision": "v1.0.2"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "modelscope/Llama-2-70b-chat-ms",
        "model_hub": "modelscope",
        "model_revision": "v1.0.1"
      }
    ],
    "chat_template": "{% if messages[0]['role'] == 'system' %}{% set system_message = '<<SYS>>\n' + messages[0]['content'] | trim + '\n<</SYS>>\n\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ '<s>' + '[INST] ' + content | trim + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + content | trim + ' ' + '</s>' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
        2
    ],
    "stop": []
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "llama-3",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3-8B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3-70B",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "llama-3-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks..",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3-8B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3-70B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "swift/Meta-Llama-3-8B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "swift/Meta-Llama-3-70B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = '<|begin_of_text|>' + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}",
    "stop_token_ids": [
      128001,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "llama-3.1",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-8B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-70B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 405,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-405B",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "llama-3.1-instruct",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "The Llama 3.1 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks..",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct-AWQ-INT4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "quantizations": [
          "Q3_K_L",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct-GGUF",
        "model_file_name_template": "Meta-Llama-3.1-8B-Instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-70B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-70B-Instruct-GPTQ-INT4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 405,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-405B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 405,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-405B-Instruct-AWQ-INT4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 405,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\"' + arg_val + '\"' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n            {{- '\"parameters\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      128001,
      128008,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>",
      "<|eom_id|>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "llama-3.2-vision-instruct",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
	"chat",
	"vision"
    ],
    "model_description": "Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image...",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 11,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Llama-3.2-11B-Vision-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 90,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Llama-3.2-90B-Vision-Instruct",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n' }}{% if message['content'] is string %}{{ message['content'] }}{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' %}{{ '<|image|>' }}{% elif content['type'] == 'text' %}{{ content['text'] }}{% endif %}{% endfor %}{% endif %}{{ '<|eot_id|>' }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}",
    "stop_token_ids": [
	128001,
	128008,
	128009
    ],
    "stop": [
      "<|end_of_text|>",
	"<|eot_id|>",
	"<|eom_id|>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "llama-3.2-vision",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
	"generate",
	"vision"
    ],
    "model_description": "The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image...",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 11,
        "quantizations": [
          "none"
        ],
          "model_id": "LLM-Research/Llama-3.2-11B-Vision",
	  "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 90,
        "quantizations": [
          "none"
        ],
          "model_id": "LLM-Research/Llama-3.2-90B-Vision",
	  "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "tiny-llama",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 1,
        "quantizations": [
          "Q2_K"
        ],
        "model_id": "Xorbits/TinyLlama-1.1B-step-50K-105b-GGUF",
        "model_hub": "modelscope",
        "model_revision": "v0.0.1",
        "model_file_name_template": "ggml-model-{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "baichuan-2-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Baichuan2-chat is a fine-tuned version of the Baichuan LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-7B-Chat",
        "model_hub": "modelscope",
        "model_revision": "v1.0.4"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-13B-Chat",
        "model_hub": "modelscope",
        "model_revision": "v1.0.3"
      }
    ],
    "chat_template": "{{ (messages|selectattr('role', 'equalto', 'system')|list|last).content|trim if (messages|selectattr('role', 'equalto', 'system')|list) else '' }}\n\n{% for message in messages %}\n{% if message['role'] == 'user' %}\n<reserved_106>\n{{ message['content']|trim -}}\n{% if not loop.last %}\n\n\n{% endif %}\n{% elif message['role'] == 'assistant' %}\n<reserved_107>\n{{ message['content']|trim -}}\n{% if not loop.last %}\n\n\n{% endif %}\n{% endif %}\n{% endfor %}\n{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}\n<reserved_107>\n{% endif %}",
    "stop_token_ids": [
      2,
      195
    ],
    "stop": []
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "baichuan-2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Baichuan2 is an open-source Transformer based LLM that is trained on both Chinese and English data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-7B-Base",
        "model_revision": "v1.0.2",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-13B-Base",
        "model_revision": "v1.0.3",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "glm4-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "GLM4 is the open source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-4-9b-chat",
        "model_revision": "master"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "Q2_K",
          "IQ3_XS",
          "IQ3_S",
          "IQ3_M",
          "Q3_K_S",
          "Q3_K_L",
          "Q3_K",
          "IQ4_XS",
          "IQ4_NL",
          "Q4_K_S",
          "Q4_K",
          "Q5_K_S",
          "Q5_K",
          "Q6_K",
          "Q8_0",
          "BF16",
          "FP16"
        ],
        "model_file_name_template": "glm-4-9b-chat.{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "LLM-Research/glm-4-9b-chat-GGUF",
        "model_revision": "master"
      }
    ],
    "chat_template": "[gMASK]<sop>{% for item in messages %}{% if item['tools'] is defined %}<|system|>\n你是一个名为 ChatGLM 的人工智能助手。你是基于智谱AI训练的语言模型 GLM-4 模型开发的，你的任务是针对用户的问题和要求提供适当的答复和支持。\n\n# 可用工具{% set tools = item['tools'] %}{% for tool in tools %}{% if tool['type'] == 'function' %}\n\n## {{ tool['function']['name'] }}\n\n{{ tool['function'] | tojson(indent=4) }}\n在调用上述函数时，请使用 Json 格式表示调用的参数。{% elif tool['type'] == 'python' %}\n\n## python\n\n当你向 `python` 发送包含 Python 代码的消息时，该代码将会在一个有状态的 Jupyter notebook 环境中执行。\n`python` 返回代码执行的输出，或在执行 60 秒后返回超时。\n`/mnt/data` 将会持久化存储你的文件。在此会话中，`python` 无法访问互联网。不要使用 `python` 进行任何网络请求或者在线 API 调用，这些在线内容的访问将不会成功。{% elif tool['type'] == 'simple_browser' %}\n\n## simple_browser\n\n你可以使用 `simple_browser` 工具。该工具支持以下函数：\n`search(query: str, recency_days: int)`：使用搜索引擎进行查询并显示结果，可以使用 `recency_days` 参数控制搜索内容的时效性。\n`mclick(ids: list[int])`：获取一系列指定 id 的页面内容。每次调用时，须选择3-10个页面。选择多个角度的页面，同时尽可能选择可信任的信息来源。考虑到部分页面是无法加载的，你也可以多打开一些可能有用的页面而不用担心内容过多。\n`open_url(url: str)`：打开指定的 URL。\n\n使用 `【{引用 id}†{引用文本}】` 来引用内容。\n\n操作步骤：1. 使用 `search` 来获得信息列表; 2. 使用 `mclick` 来获取指定 ID 页面的内容; 3. 根据获得的内容进行回复。在回复中应当引用信息来源。\n 如果用户提供了 URL，也可以用 `open_url` 直接打开页面。\n如果初次搜索结果没有找到合适的信息，也可以再次使用 `search` 进行搜索。{% elif tool['type'] == 'cogview' %}\n\n## cogview\n\n如果用户的请求中包含了对图像的描述，你可以使用 `cogview` 来生成图像并展示给用户。你需要向 `cogview` 发送图像描述，规则：\n- 发送给 `cogview` 的消息必须使用英语。用户的中文描述必须完全翻译为英语。\n- 应当尽可能详细地描述图像生成的需求，需求描述约 100 英文单词。\n- 保持用户原始描述的意图。不要虚构内容或者没见过的人物。\n- 如无特殊说明，所在地为中国，持有中国立场并遵循中国社会主义价值观。{% endif %}{% endfor %}{% endif %}{% if item['content'] %}<|{{ item['role'] }}|>{{ item['metadata'] }}\n{{ item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ]
  },
  {
    "version": 1,
    "context_length": 1048576,
    "model_name": "glm4-chat-1m",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "GLM4 is the open source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-4-9b-chat-1m",
        "model_revision": "master"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "Q2_K",
          "IQ3_XS",
          "IQ3_S",
          "IQ3_M",
          "Q3_K_S",
          "Q3_K_L",
          "Q3_K",
          "IQ4_XS",
          "IQ4_NL",
          "Q4_K_S",
          "Q4_K",
          "Q5_K_S",
          "Q5_K",
          "Q6_K",
          "Q8_0",
          "BF16",
          "FP16"
        ],
        "model_file_name_template": "glm-4-9b-chat-1m.{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "LLM-Research/glm-4-9b-chat-1m-GGUF",
        "model_revision": "master"
      }
    ],
    "chat_template": "[gMASK]<sop>{% for item in messages %}{% if item['tools'] is defined %}<|system|>\n你是一个名为 GLM-4 的人工智能助手。你是基于智谱AI训练的语言模型 GLM-4 模型开发的，你的任务是针对用户的问题和要求提供适当的答复和支持。\n\n# 可用工具{% set tools = item['tools'] %}{% for tool in tools %}{% if tool['type'] == 'function' %}\n\n## {{ tool['function']['name'] }}\n\n{{ tool['function'] | tojson(indent=4) }}\n在调用上述函数时，请使用 Json 格式表示调用的参数。{% elif tool['type'] == 'python' %}\n\n## python\n\n当你向 `python` 发送包含 Python 代码的消息时，该代码将会在一个有状态的 Jupyter notebook 环境中执行。\n`python` 返回代码执行的输出，或在执行 60 秒后返回超时。\n`/mnt/data` 将会持久化存储你的文件。在此会话中，`python` 无法访问互联网。不要使用 `python` 进行任何网络请求或者在线 API 调用，这些在线内容的访问将不会成功。{% elif tool['type'] == 'simple_browser' %}\n\n## simple_browser\n\n你可以使用 `simple_browser` 工具。该工具支持以下函数：\n`search(query: str, recency_days: int)`：使用搜索引擎进行查询并显示结果，可以使用 `recency_days` 参数控制搜索内容的时效性。\n`mclick(ids: list[int])`：获取一系列指定 id 的页面内容。每次调用时，须选择3-10个页面。选择多个角度的页面，同时尽可能选择可信任的信息来源。考虑到部分页面是无法加载的，你也可以多打开一些可能有用的页面而不用担心内容过多。\n`open_url(url: str)`：打开指定的 URL。\n\n使用 `【{引用 id}†{引用文本}】` 来引用内容。\n\n操作步骤：1. 使用 `search` 来获得信息列表; 2. 使用 `mclick` 来获取指定 ID 页面的内容; 3. 根据获得的内容进行回复。在回复中应当引用信息来源。\n 如果用户提供了 URL，也可以用 `open_url` 直接打开页面。\n如果初次搜索结果没有找到合适的信息，也可以再次使用 `search` 进行搜索。{% elif tool['type'] == 'cogview' %}\n\n## cogview\n\n如果用户的请求中包含了对图像的描述，你可以使用 `cogview` 来生成图像并展示给用户。你需要向 `cogview` 发送图像描述，规则：\n- 发送给 `cogview` 的消息必须使用英语。用户的中文描述必须完全翻译为英语。\n- 应当尽可能详细地描述图像生成的需求，需求描述约 100 英文单词。\n- 保持用户原始描述的意图。不要虚构内容或者没见过的人物。\n- 如无特殊说明，所在地为中国，持有中国立场并遵循中国社会主义价值观。{% endif %}{% endfor %}{% endif %}{% if item['content'] %}<|{{ item['role'] }}|>{{ item['metadata'] }}\n{{ item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "glm-4v",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "GLM4 is the open source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-4v-9b",
        "model_revision": "master"
      }
    ],
    "chat_template": "",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "codegeex4",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "the open-source version of the latest CodeGeeX4 model series",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "ZhipuAI/codegeex4-all-9b",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "IQ2_M",
          "IQ3_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K_L",
          "Q8_0"
        ],
        "model_file_name_template": "codegeex4-all-9b-{quantization}.gguf",
        "model_id": "ZhipuAI/codegeex4-all-9b-GGUF",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ '<|system|>\n' + item['content'] }}{% elif loop.first %}{{ '<|system|>\n你是一位智能编程助手，你叫CodeGeeX。你会为用户回答关于编程、代码、计算机方面的任何问题，并提供格式规范、可以执行、准确安全的代码，并在必要时提供详细的解释。' }}{% endif %}{% if item['role'] == 'user' %}{{ '<|user|>\n' + item['content'] }}{% elif item['role'] == 'assistant' %}{{ '<|assistant|>\n' + item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n' }}{% endif %}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "xverse-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "XVERSE-7B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "xverse/XVERSE-7B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "xverse/XVERSE-13B-Chat",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ '<|system|> \n' + item['content'] }}{% endif %}{% if item['role'] == 'user' %}{{ '<|user|> \n' + item['content'] }}{% elif item['role'] == 'assistant' %}{{ '<|assistant|> \n' + item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% endif %}",
    "stop_token_ids": [
      3
    ],
    "stop": [
      "<|endoftext|>"
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "xverse",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "XVERSE is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-7B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-13B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 65,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-65B",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "internlm2.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "InternLM2.5 series of the InternLM model.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "none"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm2_5-1_8b-chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm2_5-7b-chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm2_5-7b-chat-gguf",
        "model_file_name_template": "internlm2_5-7b-chat-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "none"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm2_5-20b-chat",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{{ '<s>' }}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      2,
      92542
    ],
    "stop": [
      "</s>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 262144,
    "model_name": "internlm2.5-chat-1m",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "InternLM2.5 series of the InternLM model supports 1M long-context",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm2_5-7b-chat-1m",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{{ '<s>' }}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      2,
      92542
    ],
    "stop": [
      "</s>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "wizardcoder-python-v1.0",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/WizardCoder-Python-13B-V1.0",
        "model_revision": "v1.0.0"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/WizardCoder-Python-34B-V1.0",
        "model_revision": "v1.0.0"
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ item['content'] + '\n\n### ' }}{% elif loop.first %}{{ 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### ' }}{% endif %}{% if item['role'] == 'user' %}{{ 'Instruction: ' + item['content'] + '\n\n### ' }}{% elif item['role'] == 'assistant' %}{{ 'Response: ' + item['content'] + '\n\n### ' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Response: Let\\'s think step by step.' }}{% endif %}",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "code-llama",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Code-Llama is an open-source LLM trained by fine-tuning LLaMA2 for generating and discussing code.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-7b-hf",
        "model_revision": "v1.0.2"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-13b-hf",
        "model_revision": "v1.0.1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-34b-hf",
        "model_revision": "v1.0.1"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8194,
    "model_name": "codeshell",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "CodeShell is a multi-language code LLM developed by the Knowledge Computing Lab of Peking University. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "WisdomShell/CodeShell-7B",
        "model_revision": "master",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8194,
    "model_name": "codeshell-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "CodeShell is a multi-language code LLM developed by the Knowledge Computing Lab of Peking University.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "WisdomShell/CodeShell-7B-Chat",
        "model_revision": "master",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% for item in messages %}{% if item['role'] == 'user' %}{{ '## human: ' + item['content'] + '|<end>|' }}{% elif item['role'] == 'assistant' %}{{ '## assistant: ' + item['content'] + '|<end>|' }}{% endif %}{% endfor %}{{ '## assistant: ' }}",
    "stop_token_ids": [
      70000
    ],
    "stop": [
      "<|endoftext|>",
      "|||",
      "|<end>|"
    ]
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "code-llama-instruct",
    "model_description": "Code-Llama-Instruct is an instruct-tuned version of the Code-Llama LLM.",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-7b-Instruct-hf",
        "model_revision": "v1.0.1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-13b-Instruct-hf",
        "model_revision": "v1.0.1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-34b-Instruct-hf",
        "model_revision": "v1.0.2"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-7B-Instruct-GGUF",
        "model_file_name_template": "codellama-7b-instruct.{quantization}.gguf",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-13B-Instruct-GGUF",
        "model_file_name_template": "codellama-13b-instruct.{quantization}.gguf",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-34B-Instruct-GGUF",
        "model_file_name_template": "codellama-34b-instruct.{quantization}.gguf",
        "model_revision": "v0.1.0"
      }
    ],
    "chat_template": "{% if messages[0]['role'] == 'system' %}{% set system_message = '<<SYS>>\n' + messages[0]['content'] | trim + '\n<</SYS>>\n\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ '<s>' + '[INST] ' + content | trim + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + content | trim + ' ' + '</s>' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
        2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "mistral-v0.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Mistral-7B is a unmoderated Transformer based LLM claiming to outperform Llama2 on all benchmarks.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Mistral-7B-v0.1",
        "model_revision": "v1.0.0"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Mistral-7B-v0.1-GGUF",
        "model_file_name_template": "mistral-7b-v0.1.{quantization}.gguf",
        "model_revision": "v1.0.0"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "code-llama-python",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Code-Llama-Python is a fine-tuned version of the Code-Llama LLM, specializing in Python.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "Xorbits/CodeLlama-7B-Python-GGUF",
        "model_hub": "modelscope",
        "model_revision": "v1.0.0",
        "model_file_name_template": "codellama-7b-python.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-13B-Python-GGUF",
        "model_file_name_template": "codellama-13b-python.{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-13B-Python-fp16",
        "model_revision": "v1.0.0"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-7B-Python-fp16",
        "model_revision": "v1.0.0"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-13b-Python-hf",
        "model_revision": "v1.0.1"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "mixtral-v0.1",
    "model_lang": [
      "en",
      "fr",
      "it",
      "de",
      "es"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/Mixtral-8x7B-v0.1",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "mixtral-instruct-v0.1",
    "model_lang": [
      "en",
      "fr",
      "it",
      "de",
      "es"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-8x7B-Instruct is a fine-tuned version of the Mistral-8x7B LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/Mixtral-8x7B-Instruct-v0.1",
        "model_revision": "master"
      }
    ],
    "chat_template": "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n    {%- endif %}\n    {%- if message['role'] == 'user' %}\n        {%- if loop.first and system_message is defined %}\n            {{- ' [INST] ' + system_message + '\n\n' + message['content'] + ' [/INST]' }}\n        {%- else %}\n            {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n        {%- endif %}\n    {%- elif message['role'] == 'assistant' %}\n        {{- ' ' + message['content'] + '</s>'}}\n    {%- else %}\n        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-6B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-9B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-34B",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 262144,
    "model_name": "Yi-200k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI. The first public release contains two bilingual (English/Chinese) base models with the parameter sizes of 6B and 34B. Both of them are trained with 4K sequence length and can be extended to 32K during inference time.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-6B-200K",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-34B-200K",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
    "model_specs": [
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "quantizations": [
          "8bits"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-34B-Chat-{quantization}",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-6B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-34B-Chat",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      2,
      6,
      7,
      8
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>",
      "<|im_sep|>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi-1.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-6B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-9B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-34B",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi-1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-6B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-9B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-34B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 6,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-6B-Chat-GPTQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 9,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-9B-Chat-GPTQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-34B-Chat-GPTQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 6,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-6B-Chat-AWQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 9,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-9B-Chat-AWQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 34,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-34B-Chat-AWQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\n' + content + '<|im_end|>\n<|im_start|>assistant\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\n' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      2,
      6,
      7,
      8
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>",
      "<|im_sep|>"
    ]
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "Yi-1.5-chat-16k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-9B-Chat-16K",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-34B-Chat-16K",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\n' + content + '<|im_end|>\n<|im_start|>assistant\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\n' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      2,
      6,
      7,
      8
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>",
      "<|im_sep|>"
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "wizardmath-v1.0",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "WizardMath is an open-source LLM trained by fine-tuning Llama2 with Evol-Instruct, specializing in math.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/WizardMath-7B-V1.0",
        "model_revision": "v1.0.0"
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ item['content'] + '\n\n### ' }}{% elif loop.first %}{{ 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### ' }}{% endif %}{% if item['role'] == 'user' %}{{ 'Instruction: ' + item['content'] + '\n\n### ' }}{% elif item['role'] == 'assistant' %}{{ 'Response: ' + item['content'] + '\n\n### ' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Response: Let\\'s think step by step.' }}{% endif %}",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "mistral-instruct-v0.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B LLM on public datasets, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Mistral-7B-Instruct-v0.1",
        "model_revision": "v1.0.0"
      }
    ],
    "chat_template": "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n    {%- endif %}\n    {%- if message['role'] == 'user' %}\n        {%- if loop.first and system_message is defined %}\n            {{- ' [INST] ' + system_message + '\n\n' + message['content'] + ' [/INST]' }}\n        {%- else %}\n            {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n        {%- endif %}\n    {%- elif message['role'] == 'assistant' %}\n        {{- ' ' + message['content'] + '</s>'}}\n    {%- else %}\n        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "mistral-instruct-v0.2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an improved instruct fine-tuned version of Mistral-7B-Instruct-v0.1.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/Mistral-7B-Instruct-v0.2"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Mistral-7B-Instruct-v0.2-GGUF",
        "model_file_name_template": "mistral-7b-instruct-v0.2.{quantization}.gguf"
      }
    ],
    "chat_template": "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n    {%- endif %}\n    {%- if message['role'] == 'user' %}\n        {%- if loop.first and system_message is defined %}\n            {{- ' [INST] ' + system_message + '\n\n' + message['content'] + ' [/INST]' }}\n        {%- else %}\n            {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n        {%- endif %}\n    {%- elif message['role'] == 'assistant' %}\n        {{- ' ' + message['content'] + '</s>'}}\n    {%- else %}\n        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 1024000,
    "model_name": "mistral-nemo-instruct",
    "model_lang": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "zh",
      "ru",
      "ja"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-Nemo-Base-2407",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "quantizations": [
          "none"
        ],
        "model_id": "AI-ModelScope/Mistral-Nemo-Instruct-2407",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 12,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Mistral-Nemo-Instruct-2407-gptq-4bit",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS][\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- '{\"type\": \"function\", \"function\": {' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- '\"' + key + '\": \"' + val + '\"' }}\n                    {%- else %}\n                        {{- '\"' + key + '\": ' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST]\" + system_message + \"\n\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST]\" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\n        {{- \"[TOOL_CALLS][\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + '</s>' }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- message[\"content\"] + '</s>'}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- '[TOOL_RESULTS]{\"content\": ' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "mistral-large-instruct",
    "model_lang": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "zh",
      "ru",
      "ja",
      "ko"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-Large-Instruct-2407 is an advanced dense Large Language Model (LLM) of 123B parameters with state-of-the-art reasoning, knowledge and coding capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 123,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Mistral-Large-Instruct-2407",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 123,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "LLM-Research/Mistral-Large-Instruct-2407-bnb-4bit",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS][\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- '{\"type\": \"function\", \"function\": {' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- '\"' + key + '\": \"' + val + '\"' }}\n                    {%- else %}\n                        {{- '\"' + key + '\": ' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST]\" + system_message + \"\n\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST]\" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\n        {{- \"[TOOL_CALLS][\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + '</s>' }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- message[\"content\"] + '</s>'}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- '[TOOL_RESULTS]{\"content\": ' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Qwen-chat is a fine-tuned version of the Qwen LLM trained with alignment techniques, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Qwen-7B-Chat-GGUF",
        "model_file_name_template": "Qwen-7B-Chat.{quantization}.gguf",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Qwen-14B-Chat-GGUF",
        "model_file_name_template": "Qwen-14B-Chat.{quantization}.gguf",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen-1_8B-Chat",
        "model_revision": "v1.0.0"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen-7B-Chat",
        "model_revision": "v1.1.9"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen-72B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen-14B-Chat",
        "model_hub": "modelscope",
        "model_revision": "v1.0.7"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen-1_8B-Chat-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen-7B-Chat-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "v1.1.7"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen-14B-Chat-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "v1.0.7"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen-72B-Chat-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ '<|im_start|>system\n' + item['content'] + '<|im_end|>\n' }}{% elif loop.first %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{% if item['role'] == 'user' %}{{ '<|im_start|>user\n' + item['content'] + '<|im_end|>' }}{% elif item['role'] == 'assistant' %}{{ '<|im_start|>assistant\n' + item['content'] + '<|im_end|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-0.5B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-1.8B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-4B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-7B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-14B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-32B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-72B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 110,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-110B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-0.5B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-1.8B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 4,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-4B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-7B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-14B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-32B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-72B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 110,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-110B-Chat-GPTQ-Int4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-0.5B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-1.8B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 4,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-4B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-7B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-14B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-32B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-72B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 110,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-110B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-0.5B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-0_5b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-1.8B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-1_8b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 4,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-4B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-4b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-7B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-7b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-14B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-14b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-32B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-32b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 72,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_k_m"
        ],
        "model_id": "qwen/Qwen1.5-72B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-72b-chat-{quantization}.gguf",
        "model_file_name_split_template": "qwen1_5-72b-chat-{quantization}.gguf.{part}",
        "quantization_parts": {
          "q4_k_m": [
            "a",
            "b"
          ]
        }
      }
    ],
    "chat_template": "{%- macro json_to_python_type(json_spec) %}\n    {%- set basic_type_map = {\n    \"string\": \"str\",\n    \"number\": \"float\",\n    \"integer\": \"int\",\n    \"boolean\": \"bool\"\n} %}\n    {%- if basic_type_map[json_spec.type] is defined %}\n        {{- basic_type_map[json_spec.type] }}\n    {%- elif json_spec.type == \"array\" %}\n        {{- \"list[\" +  json_to_python_type(json_spec|items) + \"]\" }}\n    {%- elif json_spec.type == \"object\" %}\n        {%- if json_spec.additionalProperties is defined %}\n            {{- \"dict[str, \" + json_to_python_type(json_spec.additionalProperties) + ']' }}\n        {%- else %}\n            {{- \"dict\" }}\n        {%- endif %}\n    {%- elif json_spec.type is iterable %}\n        {{- \"Union[\" }}\n        {%- for t in json_spec.type %}\n            {{- json_to_python_type({\"type\": t}) }}\n            {%- if not loop.last %}\n                {{- \",\" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \"]\" }}\n    {%- else %}\n        {{- \"Any\" }}\n    {%- endif %}\n{%- endmacro %}\n\n{%- if tools %}\n    {{- '<|im_start|>system\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] + '\n\n' }}\n    {%- endif %}\n    {{- '# Tools\n\n' }}\n    {{- \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- '{\"type\": \"function\", \"function\": ' }}\n        {{- '{\"name\": ' + tool.name + '\", ' }}\n        {{- '\"description\": \"' + tool.name + '(' }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {{- param_name + \": \" + json_to_python_type(param_fields) }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \")\" }}\n        {%- if tool.return is defined %}\n            {{- \" -> \" + json_to_python_type(tool.return) }}\n        {%- endif %}\n        {{- \" - \" + tool.description + \"\n\n\" }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {%- if loop.first %}\n                {{- \"    Args:\n\" }}\n            {%- endif %}\n            {{- \"        \" + param_name + \"(\" + json_to_python_type(param_fields) + \"): \" + param_fields.description|trim }}\n        {%- endfor %}\n        {%- if tool.return is defined and tool.return.description is defined %}\n            {{- \"\n    Returns:\n        \" + tool.return.description }}\n        {%- endif %}\n        {{- '\"' }}\n        {{- ', \"parameters\": ' }}\n        {%- if tool.parameters.properties | length == 0 %}\n            {{- \"{}\" }}\n        {%- else %}\n            {{- tool.parameters|tojson }}\n        {%- endif %}\n        {{- \"}\" }}\n        {%- if not loop.last %}\n            {{- \"\n\" }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \" </tools>\" }}\n    {{- 'Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"}\n' }}\n    {{- \"For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n\" }}\n    {{- \"<tool_call>\n\" }}\n    {{- '{\"name\": <function-name>, \"arguments\": <args-json-object>}\n' }}\n    {{- '</tool_call><|im_end|>\n' }}\n{%- else %}\n    {%- if messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and message.tool_calls is not defined) %}\n        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role + '\n<tool_call>\n' }}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '{' }}\n            {{- '\"name\": \"' }}\n            {{- tool_call.name }}\n            {%- if tool_call.arguments is defined %}\n                {{- ', ' }}\n                {{- '\"arguments\": ' }}\n                {{- tool_call.arguments|tojson }}\n            {%- endif %}\n            {{- '\"}' }}\n            {{- '\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if not message.name is defined %}\n            {{- raise_exception(\"Tool response dicts require a 'name' key indicating the name of the called function!\") }}\n        {%- endif %}\n        {{- '<|im_start|>user\n<tool_response>\n' }}\n        {{- '{\"name\": \"' }}\n        {{- message.name }}\n        {{- '\", \"content\": ' }}\n        {{- message.content|tojson + '}' }}\n        {{- '\n</tool_response><|im_end|>\n' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen1.5-moe-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen1.5-MoE is a transformer-based MoE decoder-only language model pretrained on a large amount of data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "2_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-MoE-A2.7B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "2_7",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- macro json_to_python_type(json_spec) %}\n    {%- set basic_type_map = {\n    \"string\": \"str\",\n    \"number\": \"float\",\n    \"integer\": \"int\",\n    \"boolean\": \"bool\"\n} %}\n    {%- if basic_type_map[json_spec.type] is defined %}\n        {{- basic_type_map[json_spec.type] }}\n    {%- elif json_spec.type == \"array\" %}\n        {{- \"list[\" +  json_to_python_type(json_spec|items) + \"]\" }}\n    {%- elif json_spec.type == \"object\" %}\n        {%- if json_spec.additionalProperties is defined %}\n            {{- \"dict[str, \" + json_to_python_type(json_spec.additionalProperties) + ']' }}\n        {%- else %}\n            {{- \"dict\" }}\n        {%- endif %}\n    {%- elif json_spec.type is iterable %}\n        {{- \"Union[\" }}\n        {%- for t in json_spec.type %}\n            {{- json_to_python_type({\"type\": t}) }}\n            {%- if not loop.last %}\n                {{- \",\" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \"]\" }}\n    {%- else %}\n        {{- \"Any\" }}\n    {%- endif %}\n{%- endmacro %}\n\n{%- if tools %}\n    {{- '<|im_start|>system\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] + '\n\n' }}\n    {%- endif %}\n    {{- '# Tools\n\n' }}\n    {{- \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- '{\"type\": \"function\", \"function\": ' }}\n        {{- '{\"name\": ' + tool.name + '\", ' }}\n        {{- '\"description\": \"' + tool.name + '(' }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {{- param_name + \": \" + json_to_python_type(param_fields) }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \")\" }}\n        {%- if tool.return is defined %}\n            {{- \" -> \" + json_to_python_type(tool.return) }}\n        {%- endif %}\n        {{- \" - \" + tool.description + \"\n\n\" }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {%- if loop.first %}\n                {{- \"    Args:\n\" }}\n            {%- endif %}\n            {{- \"        \" + param_name + \"(\" + json_to_python_type(param_fields) + \"): \" + param_fields.description|trim }}\n        {%- endfor %}\n        {%- if tool.return is defined and tool.return.description is defined %}\n            {{- \"\n    Returns:\n        \" + tool.return.description }}\n        {%- endif %}\n        {{- '\"' }}\n        {{- ', \"parameters\": ' }}\n        {%- if tool.parameters.properties | length == 0 %}\n            {{- \"{}\" }}\n        {%- else %}\n            {{- tool.parameters|tojson }}\n        {%- endif %}\n        {{- \"}\" }}\n        {%- if not loop.last %}\n            {{- \"\n\" }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \" </tools>\" }}\n    {{- 'Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"}\n' }}\n    {{- \"For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n\" }}\n    {{- \"<tool_call>\n\" }}\n    {{- '{\"name\": <function-name>, \"arguments\": <args-json-object>}\n' }}\n    {{- '</tool_call><|im_end|>\n' }}\n{%- else %}\n    {%- if messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and message.tool_calls is not defined) %}\n        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role + '\n<tool_call>\n' }}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '{' }}\n            {{- '\"name\": \"' }}\n            {{- tool_call.name }}\n            {%- if tool_call.arguments is defined %}\n                {{- ', ' }}\n                {{- '\"arguments\": ' }}\n                {{- tool_call.arguments|tojson }}\n            {%- endif %}\n            {{- '\"}' }}\n            {{- '\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if not message.name is defined %}\n            {{- raise_exception(\"Tool response dicts require a 'name' key indicating the name of the called function!\") }}\n        {%- endif %}\n        {{- '<|im_start|>user\n<tool_response>\n' }}\n        {{- '{\"name\": \"' }}\n        {{- message.name }}\n        {{- '\", \"content\": ' }}\n        {{- message.content|tojson + '}' }}\n        {{- '\n</tool_response><|im_end|>\n' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 65536,
    "model_name": "codeqwen1.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "CodeQwen1.5 is the Code-Specific version of Qwen1.5. It is a transformer-based decoder-only language model pretrained on a large amount of data of codes.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/CodeQwen1.5-7B",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 65536,
    "model_name": "codeqwen1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "CodeQwen1.5 is the Code-Specific version of Qwen1.5. It is a transformer-based decoder-only language model pretrained on a large amount of data of codes.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/CodeQwen1.5-7B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "codeqwen-1_5-7b-chat-{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/CodeQwen1.5-7B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/CodeQwen1.5-7B-Chat-AWQ",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2 is the new series of Qwen large language models",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2-7B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2-72B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2-7B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2-72B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-7B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-72B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 7,
        "quantizations": [
          "fp8"
        ],
        "model_id": "liuzhenghua/Qwen2-7B-FP8-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 72,
        "quantizations": [
          "fp8"
        ],
        "model_id": "liuzhenghua/Qwen2-72B-FP8-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "4-bit"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct-MLX",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "4-bit"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct-MLX",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "qwen/Qwen2-7B-Instruct-MLX",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct-GGUF",
        "model_file_name_template": "qwen2-0_5b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct-GGUF",
        "model_file_name_template": "qwen2-1_5b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "qwen/Qwen2-7B-Instruct-GGUF",
        "model_file_name_template": "qwen2-7b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 72,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "qwen/Qwen2-72B-Instruct-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen2-72b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2-72b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q5_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q5_k_m": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q6_k": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q8_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "fp16": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ]
        }
      }
    ],
    "chat_template": "{%- macro json_to_python_type(json_spec) %}\n    {%- set basic_type_map = {\n    \"string\": \"str\",\n    \"number\": \"float\",\n    \"integer\": \"int\",\n    \"boolean\": \"bool\"\n} %}\n    {%- if basic_type_map[json_spec.type] is defined %}\n        {{- basic_type_map[json_spec.type] }}\n    {%- elif json_spec.type == \"array\" %}\n        {{- \"list[\" +  json_to_python_type(json_spec|items) + \"]\" }}\n    {%- elif json_spec.type == \"object\" %}\n        {%- if json_spec.additionalProperties is defined %}\n            {{- \"dict[str, \" + json_to_python_type(json_spec.additionalProperties) + ']' }}\n        {%- else %}\n            {{- \"dict\" }}\n        {%- endif %}\n    {%- elif json_spec.type is iterable %}\n        {{- \"Union[\" }}\n        {%- for t in json_spec.type %}\n            {{- json_to_python_type({\"type\": t}) }}\n            {%- if not loop.last %}\n                {{- \",\" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \"]\" }}\n    {%- else %}\n        {{- \"Any\" }}\n    {%- endif %}\n{%- endmacro %}\n\n{%- if tools %}\n    {{- '<|im_start|>system\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] + '\n\n' }}\n    {%- endif %}\n    {{- '# Tools\n\n' }}\n    {{- \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- '{\"type\": \"function\", \"function\": ' }}\n        {{- '{\"name\": ' + tool.name + '\", ' }}\n        {{- '\"description\": \"' + tool.name + '(' }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {{- param_name + \": \" + json_to_python_type(param_fields) }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \")\" }}\n        {%- if tool.return is defined %}\n            {{- \" -> \" + json_to_python_type(tool.return) }}\n        {%- endif %}\n        {{- \" - \" + tool.description + \"\n\n\" }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {%- if loop.first %}\n                {{- \"    Args:\n\" }}\n            {%- endif %}\n            {{- \"        \" + param_name + \"(\" + json_to_python_type(param_fields) + \"): \" + param_fields.description|trim }}\n        {%- endfor %}\n        {%- if tool.return is defined and tool.return.description is defined %}\n            {{- \"\n    Returns:\n        \" + tool.return.description }}\n        {%- endif %}\n        {{- '\"' }}\n        {{- ', \"parameters\": ' }}\n        {%- if tool.parameters.properties | length == 0 %}\n            {{- \"{}\" }}\n        {%- else %}\n            {{- tool.parameters|tojson }}\n        {%- endif %}\n        {{- \"}\" }}\n        {%- if not loop.last %}\n            {{- \"\n\" }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \" </tools>\" }}\n    {{- 'Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"}\n' }}\n    {{- \"For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n\" }}\n    {{- \"<tool_call>\n\" }}\n    {{- '{\"name\": <function-name>, \"arguments\": <args-json-object>}\n' }}\n    {{- '</tool_call><|im_end|>\n' }}\n{%- else %}\n    {%- if messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and message.tool_calls is not defined) %}\n        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role + '\n<tool_call>\n' }}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '{' }}\n            {{- '\"name\": \"' }}\n            {{- tool_call.name }}\n            {%- if tool_call.arguments is defined %}\n                {{- ', ' }}\n                {{- '\"arguments\": ' }}\n                {{- tool_call.arguments|tojson }}\n            {%- endif %}\n            {{- '\"}' }}\n            {{- '\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if not message.name is defined %}\n            {{- raise_exception(\"Tool response dicts require a 'name' key indicating the name of the called function!\") }}\n        {%- endif %}\n        {{- '<|im_start|>user\n<tool_response>\n' }}\n        {{- '{\"name\": \"' }}\n        {{- message.name }}\n        {{- '\", \"content\": ' }}\n        {{- message.content|tojson + '}' }}\n        {{- '\n</tool_response><|im_end|>\n' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2-moe-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2 is the new series of Qwen large language models. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2-57B-A14B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-57B-A14B-Instruct-GPTQ-Int4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "qwen/Qwen2-57B-A14B-Instruct-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen2-57b-a14b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2-57b-a14b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q8_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "fp16": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ]
        }
      }
    ],
    "chat_template": "{%- macro json_to_python_type(json_spec) %}\n    {%- set basic_type_map = {\n    \"string\": \"str\",\n    \"number\": \"float\",\n    \"integer\": \"int\",\n    \"boolean\": \"bool\"\n} %}\n    {%- if basic_type_map[json_spec.type] is defined %}\n        {{- basic_type_map[json_spec.type] }}\n    {%- elif json_spec.type == \"array\" %}\n        {{- \"list[\" +  json_to_python_type(json_spec|items) + \"]\" }}\n    {%- elif json_spec.type == \"object\" %}\n        {%- if json_spec.additionalProperties is defined %}\n            {{- \"dict[str, \" + json_to_python_type(json_spec.additionalProperties) + ']' }}\n        {%- else %}\n            {{- \"dict\" }}\n        {%- endif %}\n    {%- elif json_spec.type is iterable %}\n        {{- \"Union[\" }}\n        {%- for t in json_spec.type %}\n            {{- json_to_python_type({\"type\": t}) }}\n            {%- if not loop.last %}\n                {{- \",\" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \"]\" }}\n    {%- else %}\n        {{- \"Any\" }}\n    {%- endif %}\n{%- endmacro %}\n\n{%- if tools %}\n    {{- '<|im_start|>system\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] + '\n\n' }}\n    {%- endif %}\n    {{- '# Tools\n\n' }}\n    {{- \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- '{\"type\": \"function\", \"function\": ' }}\n        {{- '{\"name\": ' + tool.name + '\", ' }}\n        {{- '\"description\": \"' + tool.name + '(' }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {{- param_name + \": \" + json_to_python_type(param_fields) }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \")\" }}\n        {%- if tool.return is defined %}\n            {{- \" -> \" + json_to_python_type(tool.return) }}\n        {%- endif %}\n        {{- \" - \" + tool.description + \"\n\n\" }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {%- if loop.first %}\n                {{- \"    Args:\n\" }}\n            {%- endif %}\n            {{- \"        \" + param_name + \"(\" + json_to_python_type(param_fields) + \"): \" + param_fields.description|trim }}\n        {%- endfor %}\n        {%- if tool.return is defined and tool.return.description is defined %}\n            {{- \"\n    Returns:\n        \" + tool.return.description }}\n        {%- endif %}\n        {{- '\"' }}\n        {{- ', \"parameters\": ' }}\n        {%- if tool.parameters.properties | length == 0 %}\n            {{- \"{}\" }}\n        {%- else %}\n            {{- tool.parameters|tojson }}\n        {%- endif %}\n        {{- \"}\" }}\n        {%- if not loop.last %}\n            {{- \"\n\" }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \" </tools>\" }}\n    {{- 'Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"}\n' }}\n    {{- \"For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n\" }}\n    {{- \"<tool_call>\n\" }}\n    {{- '{\"name\": <function-name>, \"arguments\": <args-json-object>}\n' }}\n    {{- '</tool_call><|im_end|>\n' }}\n{%- else %}\n    {%- if messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and message.tool_calls is not defined) %}\n        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role + '\n<tool_call>\n' }}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '{' }}\n            {{- '\"name\": \"' }}\n            {{- tool_call.name }}\n            {%- if tool_call.arguments is defined %}\n                {{- ', ' }}\n                {{- '\"arguments\": ' }}\n                {{- tool_call.arguments|tojson }}\n            {%- endif %}\n            {{- '\"}' }}\n            {{- '\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if not message.name is defined %}\n            {{- raise_exception(\"Tool response dicts require a 'name' key indicating the name of the called function!\") }}\n        {%- endif %}\n        {{- '<|im_start|>user\n<tool_response>\n' }}\n        {{- '{\"name\": \"' }}\n        {{- message.name }}\n        {{- '\", \"content\": ' }}\n        {{- message.content|tojson + '}' }}\n        {{- '\n</tool_response><|im_end|>\n' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "deepseek-vl-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "DeepSeek-VL possesses general multimodal understanding capabilities, capable of processing logical diagrams, web pages, formula recognition, scientific literature, natural images, and embodied intelligence in complex scenarios.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-vl-1.3b-chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-vl-7b-chat",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "",
    "stop_token_ids": [
      100001
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "deepseek",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "DDeepSeek LLM, trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-7b-base",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 67,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-67b-base",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "deepseek-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek LLM is an advanced language model comprising 67 billion parameters. It has been trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-7b-chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 67,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-67b-chat",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ '<｜begin▁of▁sentence｜>' }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n\n' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + '<｜end▁of▁sentence｜>' }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}",
    "stop_token_ids": [
      100001
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ]
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "deepseek-coder",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-1.3b-base",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-6.7b-base",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 33,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-33b-base",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "deepseek-coder-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "deepseek-coder-instruct is a model initialized from deepseek-coder-base and fine-tuned on 2B tokens of instruction data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-1.3b-instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-6.7b-instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 33,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-33b-instruct",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}\n{% set add_generation_prompt = false %}\n{% endif %}\n{%- set ns = namespace(found=false) -%}\n{%- for message in messages -%}\n    {%- if message['role'] == 'system' -%}\n        {%- set ns.found = true -%}\n    {%- endif -%}\n{%- endfor -%}\n{{'<｜begin▁of▁sentence｜>'}}{%- if not ns.found -%}\n{{'You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n'}}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'system' %}\n{{ message['content'] }}\n    {%- else %}\n        {%- if message['role'] == 'user' %}\n{{'### Instruction:\n' + message['content'] + '\n'}}\n        {%- else %}\n{{'### Response:\n' + message['content'] + '\n<|EOT|>\n'}}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{% if add_generation_prompt %}\n{{'### Response:'}}\n{% endif %}",
    "stop_token_ids": [
      32021
    ],
    "stop": [
      "<|EOT|>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Skywork",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Skywork is a series of large models developed by the Kunlun Group · Skywork team.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "skywork/Skywork-13B-base",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Skywork-Math",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Skywork is a series of large models developed by the Kunlun Group · Skywork team.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "skywork/Skywork-13B-Math",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 204800,
    "model_name": "internlm2-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The second generation of the InternLM model, InternLM2.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm2-chat-7b",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "none"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm2-chat-20b",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "chat_template": "{{ '<s>' }}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      2,
      92542
    ],
    "stop": [
      "</s>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "qwen-vl-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Qwen-VL-Chat supports more flexible interaction, such as multiple image inputs, multi-round question answering, and creative capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen-VL-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen-VL-Chat-{quantization}",
        "model_revision": "master"
      }
    ],
    "chat_template": "",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "orion-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Orion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "OrionStarAI/Orion-14B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "OrionStarAI/Orion-14B-Chat-{quantization}"
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.first %}{{ '<s>' }}{% endif %}{% if message['role'] == 'user' %}{{ 'Human: ' + message['content'] + '\n\nAssistant: ' + '</s>' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + '</s>' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2,
      0
    ],
    "stop": [
      "<s>",
      "</s>",
      "<unk>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "orion-chat-rag",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Orion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_hub": "modelscope",
        "model_id": "OrionStarAI/Orion-14B-Chat-RAG"
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.first %}{{ '<s>' }}{% endif %}{% if message['role'] == 'user' %}{{ 'Human: ' + message['content'] + '\n\nAssistant: ' + '</s>' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + '</s>' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2,
      0
    ],
    "stop": [
      "<s>",
      "</s>",
      "<unk>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "yi-vl-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Yi Vision Language (Yi-VL) model is the open-source, multimodal version of the Yi Large Language Model (LLM) series, enabling content comprehension, recognition, and multi-round conversations about images.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-VL-6B"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-VL-34B"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      2,
      6,
      7,
      8
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>",
      "<|im_sep|>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "gemma-it",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/gemma-2b-it"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/gemma-7b-it"
      }
    ],
    "chat_template": "{{ '<bos>' }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}",
    "stop_token_ids": [
      1,
      106,
      107
    ],
    "stop": [
      "<eos>",
      "<end_of_turn>",
      "<start_of_turn>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "gemma-2-it",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "LLM-Research/gemma-2-2b-it",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "AI-ModelScope/gemma-2-9b-it",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 27,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "AI-ModelScope/gemma-2-27b-it",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_K_L",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_L",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q6_K_L",
          "Q8_0",
          "f32"
        ],
        "model_id": "LLM-Research/gemma-2-9b-it-GGUF",
        "model_file_name_template": "gemma-2-9b-it-{quantization}.gguf",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{{ '<bos>' }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}",
    "stop_token_ids": [
      1,
      106,
      107
    ],
    "stop": [
      "<eos>",
      "<end_of_turn>",
      "<start_of_turn>"
    ]
  },
  {
    "version":1,
    "context_length":2048,
    "model_name":"OmniLMM",
    "model_lang":[
      "en",
      "zh"
    ],
    "model_ability":[
      "chat",
      "vision"
    ],
    "model_description":"OmniLMM is a family of open-source large multimodal models (LMMs) adept at vision & language modeling.",
    "model_specs":[
      {
        "model_format":"pytorch",
        "model_size_in_billions":3,
        "quantizations":[
          "none"
        ],
        "model_id":"OpenBMB/MiniCPM-V",
        "model_hub":"modelscope",
        "model_revision":"master"
      },
      {
        "model_format":"pytorch",
        "model_size_in_billions":12,
        "quantizations":[
          "none"
        ],
        "model_id":"OpenBMB/OmniLMM-12B",
        "model_hub":"modelscope",
        "model_revision":"master"
      }
    ],
    "chat_template": "",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-sft-bf16",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/miniCPM-bf16",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{'<用户>' + message['content'].strip() + '<AI>'}}{% else %}{{message['content'].strip()}}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-sft-fp32",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-2B-sft-fp32",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{'<用户>' + message['content'].strip() + '<AI>'}}{% else %}{{message['content'].strip()}}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-dpo-bf16",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-2B-dpo-bf16",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{'<用户>' + message['content'].strip() + '<AI>'}}{% else %}{{message['content'].strip()}}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-dpo-fp16",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-2B-dpo-fp16",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{'<用户>' + message['content'].strip() + '<AI>'}}{% else %}{{message['content'].strip()}}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-dpo-fp32",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-2B-dpo-fp32",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{'<用户>' + message['content'].strip() + '<AI>'}}{% else %}{{message['content'].strip()}}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ]
  },
  {
    "version":1,
    "context_length":8192,
    "model_name":"MiniCPM-Llama3-V-2_5",
    "model_lang":[
      "en",
      "zh"
    ],
    "model_ability":[
      "chat",
      "vision"
    ],
    "model_description":"MiniCPM-Llama3-V 2.5 is the latest model in the MiniCPM-V series. The model is built on SigLip-400M and Llama3-8B-Instruct with a total of 8B parameters.",
    "model_specs":[
      {
        "model_format":"pytorch",
        "model_size_in_billions":8,
        "quantizations":[
          "none"
        ],
        "model_hub": "modelscope",
        "model_id":"OpenBMB/MiniCPM-Llama3-V-2_5",
        "model_revision":"master"
      },
      {
        "model_format":"pytorch",
        "model_size_in_billions":8,
        "quantizations":[
          "int4"
        ],
        "model_hub": "modelscope",
        "model_id":"OpenBMB/MiniCPM-Llama3-V-2_5-{quantization}",
        "model_revision":"master"
      }
    ],
    "chat_template": "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = '<|begin_of_text|>' + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}",
    "stop_token_ids": [
      128001
    ],
    "stop": [
      "<|end_of_text|>"
    ]
  },
  {
    "version":1,
    "context_length":32768,
    "model_name":"MiniCPM-V-2.6",
    "model_lang":[
      "en",
      "zh"
    ],
    "model_ability":[
      "chat",
      "vision"
    ],
    "model_description":"MiniCPM-V 2.6 is the latest model in the MiniCPM-V series. The model is built on SigLip-400M and Qwen2-7B with a total of 8B parameters.",
    "model_specs":[
      {
        "model_format":"pytorch",
        "model_size_in_billions":8,
        "quantizations":[
          "none"
        ],
        "model_hub": "modelscope",
        "model_id":"OpenBMB/MiniCPM-V-2_6",
        "model_revision":"master"
      },
      {
        "model_format":"pytorch",
        "model_size_in_billions":8,
        "quantizations":[
          "4-bit"
        ],
        "model_hub": "modelscope",
        "model_id":"OpenBMB/MiniCPM-V-2_6-int4",
        "model_revision":"master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "aquila2",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Aquila2 series models are the base language models",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "BAAI/Aquila2-34B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "BAAI/Aquila2-70B-Expr",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "aquila2-chat",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Aquila2-chat series models are the chat models",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "BAAI/AquilaChat2-34B",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "BAAI/AquilaChat2-34B-Int4-GPTQ",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "BAAI/AquilaChat2-70B-Expr",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ item['content'] + '\n' }}{% endif %}{% if item['role'] == 'user' %}{{ 'USER: ' + item['content'] + '\n' }}{% elif item['role'] == 'assistant' %}{{ 'ASSISTANT: ' + item['content'] + '\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT: ' }}{% endif %}",
    "stop_token_ids": [
      100006,
      100007
    ],
    "stop": [
      "[CLS]",
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "aquila2-chat-16k",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "AquilaChat2-16k series models are the long-text chat models",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "BAAI/AquilaChat2-34B-16K",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ item['content'] + '\n' }}{% endif %}{% if item['role'] == 'user' %}{{ 'USER: ' + item['content'] + '\n' }}{% elif item['role'] == 'assistant' %}{{ 'ASSISTANT: ' + item['content'] + '\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT: ' }}{% endif %}",
    "stop_token_ids": [
      100006,
      100007
    ],
    "stop": [
      "[CLS]",
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "c4ai-command-r-v01",
    "model_lang": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "ja",
      "ko",
      "zh",
      "ar"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "C4AI Command-R is a research release of a 35 billion parameter highly performant generative model.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 35,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/c4ai-command-r-v01",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 35,
        "quantizations": [
          "4-bit"
        ],
        "model_hub": "modelscope",
        "model_id": "mirror013/c4ai-command-r-v01-4bit",
        "model_revision": "master"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 35,
        "quantizations": [
          "Q2_K",
          "Q3_K_M",
          "Q4_K_M",
          "Q5_K_M"
        ],
        "model_id": "mirror013/C4AI-Command-R-v01-GGUF",
        "model_file_name_template": "c4ai-command-r-v01-{quantization}.gguf",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 104,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/c4ai-command-r-plus",
        "model_revision": "master"
      }
    ],
    "chat_template": "{{ '<BOS_TOKEN>' }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = 'You are Command-R, a brilliant, sophisticated, AI-assistant trained to assist human users by providing thorough responses. You are trained by Cohere.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + system_message + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}",
    "stop_token_ids": [
      6,
      255001
    ],
    "stop": [
      "<EOS_TOKEN>",
      "<|END_OF_TURN_TOKEN|>"
    ]
  },
  {
    "version": 1,
    "context_length": 128000,
    "model_name": "phi-3-mini-128k-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "LLM-Research/Phi-3-mini-128k-instruct",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n' + message['content'] + '<|end|>\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n' }}{% else %}{{ '<|endoftext|>' }}{% endif %}",
    "stop_token_ids":[
      32000,
      32001,
      32007
    ],
    "stop": [
      "<|endoftext|>",
      "<|assistant|>",
      "<|end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "phi-3-mini-4k-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Phi-3-Mini-4k-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "LLM-Research/Phi-3-mini-4k-instruct",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n' + message['content'] + '<|end|>\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n' }}{% else %}{{ '<|endoftext|>' }}{% endif %}",
    "stop_token_ids":[
      32000,
      32001,
      32007
    ],
    "stop": [
      "<|endoftext|>",
      "<|assistant|>",
      "<|end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "internvl-chat",
    "model_lang": [
        "en",
        "zh"
    ],
    "model_ability": [
        "chat",
        "vision"
    ],
    "model_description": "InternVL 1.5 is an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. ",
    "model_specs": [
        {
            "model_format": "pytorch",
            "model_size_in_billions": 26,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL-Chat-V1-5",
            "model_revision": "master"
        }
    ],
    "chat_template": "{{ '<s>' }}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      2,
      92542,
      92543
    ],
    "stop": [
      "</s>",
      "<|im_end|>",
      "<|im_start|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "internvl2",
    "model_lang": [
        "en",
        "zh"
    ],
    "model_ability": [
        "chat",
        "vision"
    ],
    "model_description": "InternVL 2 is an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. ",
    "model_specs": [

        {
            "model_format": "pytorch",
            "model_size_in_billions": 1,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-1B",
            "model_revision": "master"
        },
      {
            "model_format": "pytorch",
            "model_size_in_billions": 2,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-2B",
            "model_revision": "master"
        },
        {
            "model_format": "awq",
            "model_size_in_billions": 2,
            "quantizations": [
              "Int4"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-2B-AWQ",
            "model_revision": "master"
        },
        {
            "model_format": "pytorch",
            "model_size_in_billions": 4,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-4B",
            "model_revision": "master"
        },
        {
            "model_format": "pytorch",
            "model_size_in_billions": 8,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-8B",
            "model_revision": "master"
        },
        {
            "model_format": "awq",
            "model_size_in_billions": 8,
            "quantizations": [
              "Int4"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-8B-AWQ",
            "model_revision": "master"
        },
        {
            "model_format": "pytorch",
            "model_size_in_billions": 26,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-26B",
            "model_revision": "master"
        },
        {
            "model_format": "awq",
            "model_size_in_billions": 26,
            "quantizations": [
              "Int4"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-26B-AWQ",
            "model_revision": "master"
        },
        {
            "model_format": "pytorch",
            "model_size_in_billions": 40,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-40B",
            "model_revision": "master"
        },
        {
            "model_format": "awq",
            "model_size_in_billions": 40,
            "quantizations": [
              "Int4"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-40B-AWQ",
            "model_revision": "master"
        },
        {
            "model_format": "pytorch",
            "model_size_in_billions": 76,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-Llama3-76B",
            "model_revision": "master"
        },
        {
            "model_format": "awq",
            "model_size_in_billions": 76,
            "quantizations": [
              "Int4"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-Llama3-76B-AWQ",
            "model_revision": "master"
        }
    ],
    "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [],
    "stop": []
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "cogvlm2",
    "model_lang": [
        "en",
        "zh"
    ],
    "model_ability": [
        "chat",
        "vision"
    ],
    "model_description": "CogVLM2 have achieved good results in many lists compared to the previous generation of CogVLM open source models. Its excellent performance can compete with some non-open source models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/cogvlm2-llama3-chinese-chat-19B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "int4"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/cogvlm2-llama3-chinese-chat-19B-{quantization}",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = '<|begin_of_text|>' + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% else %}{{ '<|end_of_text|>' }}{% endif %}",
    "stop_token_ids": [
      128001,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "cogvlm2-video-llama3-chat",
    "model_lang": [
        "en",
        "zh"
    ],
    "model_ability": [
        "chat",
        "vision"
    ],
    "model_description": "CogVLM2-Video achieves state-of-the-art performance on multiple video question answering tasks.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/cogvlm2-video-llama3-chat",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = '<|begin_of_text|>' + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% else %}{{ '<|end_of_text|>' }}{% endif %}",
    "stop_token_ids": [
      128001,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "telechat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The TeleChat is a large language model developed and trained by China Telecom Artificial Intelligence Technology Co., LTD. The 7B model base is trained with 1.5 trillion Tokens and 3 trillion Tokens and Chinese high-quality corpus.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TeleAI/telechat-7B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "int4",
          "int8"
        ],
        "model_id": "TeleAI/telechat-7B-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TeleAI/TeleChat-12B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 12,
        "quantizations": [
          "int4",
          "int8"
        ],
        "model_id": "TeleAI/TeleChat-12B-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 52,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TeleAI/TeleChat-52B",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "chat_template": "{{ (messages|selectattr('role', 'equalto', 'system')|list|last).content|trim if (messages|selectattr('role', 'equalto', 'system')|list) else '' }}{%- for message in messages -%}{%- if message['role'] == 'user' -%}{{- '<_user>' + message['content'] +'<_bot>' -}}{%- elif message['role'] == 'assistant' -%}{{- message['content'] + '<_end>' -}}{%- endif -%}{%- endfor -%}",
    "stop": [
      "<_end>",
      "<_start>"
    ],
    "stop_token_ids": [
      160133,
      160132
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2-vl-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Qwen2-VL: To See the World More Clearly.Qwen2-VL is the latest version of the vision language models in the Qwen model familities.",
    "model_specs":[
      {
        "model_format":"pytorch",
        "model_size_in_billions":7,
        "quantizations":[
          "none"
        ],
        "model_hub": "modelscope",
        "model_id":"qwen/Qwen2-VL-7B-Instruct",
        "model_revision":"master"
      },
      {
        "model_format":"gptq",
        "model_size_in_billions":7,
        "quantizations":[
          "Int8"
        ],
        "model_hub": "modelscope",
        "model_id":"qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8",
        "model_revision":"master"
      },
      {
        "model_format":"gptq",
        "model_size_in_billions":7,
        "quantizations":[
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id":"qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4",
        "model_revision":"master"
      },
      {
        "model_format":"awq",
        "model_size_in_billions":7,
        "quantizations":[
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id":"qwen/Qwen2-VL-7B-Instruct-AWQ",
        "model_revision":"master"
      },
      {
        "model_format":"pytorch",
        "model_size_in_billions":2,
        "quantizations":[
          "none"
        ],
        "model_hub": "modelscope",
        "model_id":"qwen/Qwen2-VL-2B-Instruct",
        "model_revision":"master"
      },
      {
        "model_format":"gptq",
        "model_size_in_billions":2,
        "quantizations":[
          "Int8"
        ],
        "model_hub": "modelscope",
        "model_id":"qwen/Qwen2-VL-2B-Instruct-GPTQ-Int8",
        "model_revision":"master"
      },
      {
        "model_format":"gptq",
        "model_size_in_billions":2,
        "quantizations":[
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id":"qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4",
        "model_revision":"master"
      },
      {
        "model_format":"awq",
        "model_size_in_billions":2,
        "quantizations":[
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id":"qwen/Qwen2-VL-2B-Instruct-AWQ",
        "model_revision":"master"
      },
      {
        "model_format":"pytorch",
        "model_size_in_billions":72,
        "quantizations":[
          "none"
        ],
        "model_id":"qwen/Qwen2-VL-72B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format":"awq",
        "model_size_in_billions":72,
        "quantizations":[
          "Int4"
        ],
        "model_id":"qwen/Qwen2-VL-72B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format":"gptq",
        "model_size_in_billions":72,
        "quantizations":[
          "Int4",
          "Int8"
        ],
        "model_id":"qwen/Qwen2-VL-72B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "minicpm3-4b",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM3-4B is the 3rd generation of MiniCPM series. The overall performance of MiniCPM3-4B surpasses Phi-3.5-mini-Instruct and GPT-3.5-Turbo-0125, being comparable with many recent 7B~9B models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM3-4B",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 4,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM3-4B-GPTQ-Int4",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2-audio-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "audio"
    ],
    "model_description": "Qwen2-Audio: A large-scale audio-language model which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2-Audio-7B-Instruct",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant",
      "roles": [
        "user",
        "assistant"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2-audio",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "audio"
    ],
    "model_description": "Qwen2-Audio: A large-scale audio-language model which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2-Audio-7B",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant",
      "roles": [
        "user",
        "assistant"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 128000,
    "model_name": "deepseek-v2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 16,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-V2-Lite",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 236,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-V2",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 128000,
    "model_name": "deepseek-v2-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 16,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-V2-Lite-Chat",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 236,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-V2-Chat",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ '<｜begin▁of▁sentence｜>' }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n\n' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + '<｜end▁of▁sentence｜>' }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}",
    "stop_token_ids": [
      100001
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ]
  },
  {
    "version": 1,
    "context_length": 128000,
    "model_name": "deepseek-v2-chat-0628",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek-V2-Chat-0628 is an improved version of DeepSeek-V2-Chat. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 236,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-V2-Chat-0628",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ '<｜begin▁of▁sentence｜>' }}{% for message in messages %}{% if message['role'] == 'user' %}{{ '<｜User｜>' + message['content'] }}{% elif message['role'] == 'assistant' %}{{ '<｜Assistant｜>' + message['content'] + '<｜end▁of▁sentence｜>' }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<｜Assistant｜>' }}{% endif %}",
    "stop_token_ids": [
      100001
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ]
  },
  {
    "version": 1,
    "context_length": 128000,
    "model_name": "deepseek-v2.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. The new model integrates the general and coding abilities of the two previous versions.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 236,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-V2.5",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}    {%- if message['role'] == 'system' %}        {% set ns.system_prompt = message['content'] %}    {%- endif %}{%- endfor %}{{'<｜begin▁of▁sentence｜>'}}{{ns.system_prompt}}{%- for message in messages %}    {%- if message['role'] == 'user' %}    {%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}    {%- endif %}    {%- if message['role'] == 'assistant' and message['content'] is none %}        {%- set ns.is_tool = false -%}        {%- for tool in message['tool_calls']%}            {%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}            {%- set ns.is_first = true -%}            {%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}                   {%- endif %}        {%- endfor %}    {%- endif %}    {%- if message['role'] == 'assistant' and message['content'] is not none %}        {%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}        {%- set ns.is_tool = false -%}        {%- else %}{{'<｜Assistant｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}        {%- endif %}    {%- endif %}    {%- if message['role'] == 'tool' %}        {%- set ns.is_tool = true -%}        {%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}        {%- set ns.is_output_first = false %}        {%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}        {%- endif %}    {%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}",
    "stop_token_ids": [
      100001
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "yi-coder-chat",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer than 10 billion parameters.Excelling in long-context understanding with a maximum context length of 128K tokens.Supporting 52 major programming languages, including popular ones such as Java, Python, JavaScript, and C++.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-Coder-9B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-Coder-1.5B-Chat",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\n' + system_message + '<|im_end|>\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\n' + content + '<|im_end|>\n<|im_start|>assistant\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\n' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2,
      6,
      7
    ],
    "stop": [
      "<|startoftext|>",
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "yi-coder",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer than 10 billion parameters.Excelling in long-context understanding with a maximum context length of 128K tokens.Supporting 52 major programming languages, including popular ones such as Java, Python, JavaScript, and C++.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-Coder-9B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-Coder-1.5B",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-0.5B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-1.5B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 3,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-3B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-7B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-14B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-32B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-72B",
        "model_revision": "master",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2.5-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-0.5B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-1.5B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 3,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-3B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-7B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-14B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-32B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-72B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-0.5B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-1.5B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 3,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-3B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-7B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-14B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-32B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-72B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 3,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-3B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-7B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions":14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-14B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-32B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-72B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen2.5-0.5B-Instruct-GGUF",
        "model_file_name_template": "qwen2.5-0.5b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen2.5-1.5B-Instruct-GGUF",
        "model_file_name_template": "qwen2.5-1.5b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 3,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen2.5-3B-Instruct-GGUF",
        "model_file_name_template": "qwen2.5-3b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen2.5-7B-Instruct-GGUF",
        "model_file_name_template": "qwen2_5-7b-instruct-{quantization}.gguf",
        "model_hub": "modelscope",
        "model_file_name_split_template": "qwen2.5-7b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q4_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q4_k_m": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q5_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q5_k_m": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q6_k": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q8_0": [
            "00001-of-00002",
            "00002-of-00002"
          ]
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen2.5-14B-Instruct-GGUF",
        "model_file_name_template": "qwen2.5-14b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2.5-14b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q2_k": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q3_k_m": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q4_0": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ],
          "q4_k_m": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ],
          "q5_0": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ],
          "q5_k_m": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ],
          "q6_k": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ],
          "q8_0": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ]
        },
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen2.5-32B-Instruct-GGUF",
        "model_file_name_template": "qwen2_5-32b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2.5-32b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q2_k": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ],
          "q3_k_m": [
            "00001-of-00005",
            "00002-of-00005",
            "00003-of-00005",
            "00004-of-00005",
            "00005-of-00005"
          ],
          "q4_0": [
            "00001-of-00005",
            "00002-of-00005",
            "00003-of-00005",
            "00004-of-00005",
            "00005-of-00005"
          ],
          "q4_k_m": [
            "00001-of-00005",
            "00002-of-00005",
            "00003-of-00005",
            "00004-of-00005",
            "00005-of-00005"
          ],
          "q5_0": [
            "00001-of-00006",
            "00002-of-00006",
            "00003-of-00006",
            "00004-of-00006",
            "00005-of-00006",
            "00006-of-00006"
          ],
          "q5_k_m": [
            "00001-of-00006",
            "00002-of-00006",
            "00003-of-00006",
            "00004-of-00006",
            "00005-of-00006",
            "00006-of-00006"
          ],
          "q6_k": [
            "00001-of-00007",
            "00002-of-00007",
            "00003-of-00007",
            "00004-of-00007",
            "00005-of-00007",
            "00006-of-00007",
            "00007-of-00007"
          ],
          "q8_0": [
            "00001-of-00009",
            "00002-of-00009",
            "00003-of-00009",
            "00004-of-00009",
            "00005-of-00009",
            "00006-of-00009",
            "00007-of-00009",
            "00008-of-00009",
            "00009-of-00009"
          ]
        },
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 72,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen2.5-72B-Instruct-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen2_5-72b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2.5-72b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
           "q2_k": [
            "00001-of-00007",
            "00002-of-00007",
            "00003-of-00007",
            "00004-of-00007",
            "00005-of-00007",
            "00006-of-00007",
            "00007-of-00007"
          ],
          "q3_k_m": [
            "00001-of-00009",
            "00002-of-00009",
            "00003-of-00009",
            "00004-of-00009",
            "00005-of-00009",
            "00006-of-00009",
            "00007-of-00009",
            "00008-of-00009",
            "00009-of-00009"
          ],
          "q4_0": [
            "00001-of-00011",
            "00002-of-00011",
            "00003-of-00011",
            "00004-of-00011",
            "00005-of-00011",
            "00006-of-00011",
            "00007-of-00011",
            "00008-of-00011",
            "00009-of-00011",
            "00010-of-00011",
            "00011-of-00011"
          ],
          "q4_k_m": [
            "00001-of-00012",
            "00002-of-00012",
            "00003-of-00012",
            "00004-of-00012",
            "00005-of-00012",
            "00006-of-00012",
            "00007-of-00012",
            "00008-of-00012",
            "00009-of-00012",
            "00010-of-00012",
            "00011-of-00012",
            "00012-of-00012"
          ],
          "q5_0": [
            "00001-of-00013",
            "00002-of-00013",
            "00003-of-00013",
            "00004-of-00013",
            "00005-of-00013",
            "00006-of-00013",
            "00007-of-00013",
            "00008-of-00013",
            "00009-of-00013",
            "00010-of-00013",
            "00011-of-00013",
            "00012-of-00013",
            "00013-of-00013"
          ],
          "q5_k_m": [
            "00001-of-00014",
            "00002-of-00014",
            "00003-of-00014",
            "00004-of-00014",
            "00005-of-00014",
            "00006-of-00014",
            "00007-of-00014",
            "00008-of-00014",
            "00009-of-00014",
            "00010-of-00014",
            "00011-of-00014",
            "00012-of-00014",
            "00013-of-00014",
            "00014-of-00014"
          ],
          "q6_k": [
            "00001-of-00016",
            "00002-of-00016",
            "00003-of-00016",
            "00004-of-00016",
            "00005-of-00016",
            "00006-of-00016",
            "00007-of-00016",
            "00008-of-00016",
            "00009-of-00016",
            "00010-of-00016",
            "00011-of-00016",
            "00012-of-00016",
            "00013-of-00016",
            "00014-of-00016",
            "00015-of-00016",
            "00016-of-00016"
          ],
          "q8_0": [
            "00001-of-00021",
            "00002-of-00021",
            "00003-of-00021",
            "00004-of-00021",
            "00005-of-00021",
            "00006-of-00021",
            "00007-of-00021",
            "00008-of-00021",
            "00009-of-00021",
            "00010-of-00021",
            "00011-of-00021",
            "00012-of-00021",
            "00013-of-00021",
            "00014-of-00021",
            "00015-of-00021",
            "00016-of-00021",
            "00017-of-00021",
            "00018-of-00021",
            "00019-of-00021",
            "00020-of-00021",
            "00021-of-00021"
          ]
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 3,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "okwinds/Qwen2.5-3B-Instruct-MLX-4bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 3,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "okwinds/Qwen2.5-3B-Instruct-MLX-8bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "okwinds/Qwen2.5-7B-Instruct-MLX-4bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "okwinds/Qwen2.5-7B-Instruct-MLX-8bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "okwinds/Qwen2.5-14B-Instruct-MLX-4bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 14,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "okwinds/Qwen2.5-14B-Instruct-MLX-8bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "quantizations": [
          "2-bit"
        ],
        "model_id": "okwinds/Qwen2.5-32B-Instruct-MLX-2bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "okwinds/Qwen2.5-32B-Instruct-MLX-4bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "okwinds/Qwen2.5-32B-Instruct-MLX-8bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "quantizations": [
          "2-bit"
        ],
        "model_id": "okwinds/Qwen2.5-32B-Instruct-MLX-2bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "okwinds/Qwen2.5-72B-Instruct-MLX-4bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "okwinds/Qwen2.5-72B-Instruct-MLX-8bit",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2.5-coder",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-0.5B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-1.5B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "3",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-3B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-7B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-14B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-32B",
        "model_revision": "master",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2.5-coder-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-0.5B-Instruct",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-1.5B-Instruct",
        "model_revision": "master",
        "model_hub": "modelscope"
      },      {
        "model_format": "pytorch",
        "model_size_in_billions": "3",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-3B-Instruct",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-7B-Instruct",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-14B-Instruct",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-32B-Instruct",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-{quantization}",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-{quantization}",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 3,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-{quantization}",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-{quantization}",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-{quantization}",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-{quantization}",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 3,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-Coder-3B-Instruct-AWQ",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-Coder-7B-Instruct-AWQ",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-Coder-14B-Instruct-AWQ",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-Coder-32B-Instruct-AWQ",
        "model_revision": "master",
        "model_hub": "modelscope"
      },

      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF",
        "model_file_name_template": "qwen2.5-coder-1.5b-instruct-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2.5-Coder-7B-Instruct-GGUF",
        "model_file_name_template": "qwen2.5-coder-7b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2.5-coder-7b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q4_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q4_k_m": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q5_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q5_k_m": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q6_k": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q8_0": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ]
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  }
]
