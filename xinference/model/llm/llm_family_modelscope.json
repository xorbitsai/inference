[
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "llama-2-chat",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Llama-2-Chat is a fine-tuned version of the Llama-2 LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_id": "Xorbits/Llama-2-7b-Chat-GGUF",
        "model_file_name_template": "llama-2-7b-chat.{quantization}.gguf",
        "model_hub": "modelscope",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_id": "Xorbits/Llama-2-13b-Chat-GGUF",
        "model_file_name_template": "llama-2-13b-chat.{quantization}.gguf",
        "model_hub": "modelscope",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "modelscope/Llama-2-7b-chat-ms",
        "model_hub": "modelscope",
        "model_revision": "v1.0.5"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "none"
        ],
        "model_id": "modelscope/Llama-2-13b-chat-ms",
        "model_hub": "modelscope",
        "model_revision": "v1.0.2"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_id": "modelscope/Llama-2-70b-chat-ms",
        "model_hub": "modelscope",
        "model_revision": "v1.0.1"
      }
    ],
    "chat_template": "{% if messages[0]['role'] == 'system' %}{% set system_message = '<<SYS>>\n' + messages[0]['content'] | trim + '\n<</SYS>>\n\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ '<s>' + '[INST] ' + content | trim + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + content | trim + ' ' + '</s>' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      2
    ],
    "stop": []
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "llama-3",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3-8B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3-70B",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "llama-3-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks..",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3-8B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3-70B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "swift/Meta-Llama-3-8B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "swift/Meta-Llama-3-70B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = '<|begin_of_text|>' + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}",
    "stop_token_ids": [
      128001,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "llama-3.1",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-8B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-70B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 405,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-405B",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "llama-3.1-instruct",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "The Llama 3.1 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks..",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct-AWQ-INT4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "quantizations": [
          "Q3_K_L",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct-GGUF",
        "model_file_name_template": "Meta-Llama-3.1-8B-Instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-70B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-70B-Instruct-GPTQ-INT4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 405,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-405B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 405,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-405B-Instruct-AWQ-INT4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 405,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\"' + arg_val + '\"' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n            {{- '\"parameters\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      128001,
      128008,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>",
      "<|eom_id|>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "llama-3.2-vision-instruct",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image...",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 11,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Llama-3.2-11B-Vision-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 90,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Llama-3.2-90B-Vision-Instruct",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n' }}{% if message['content'] is string %}{{ message['content'] }}{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' %}{{ '<|image|>' }}{% elif content['type'] == 'text' %}{{ content['text'] }}{% endif %}{% endfor %}{% endif %}{{ '<|eot_id|>' }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}",
    "stop_token_ids": [
      128001,
      128008,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>",
      "<|eom_id|>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "llama-3.2-vision",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "generate",
      "vision"
    ],
    "model_description": "The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image...",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 11,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Llama-3.2-11B-Vision",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 90,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Llama-3.2-90B-Vision",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "llama-3.3-instruct",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "The Llama 3.3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks..",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Llama-3.3-70B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 70,
        "quantizations": [
          "Q3_K_L",
          "Q4_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "quantization_parts": {
          "Q6_K": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "Q8_0": [
            "00001-of-00002",
            "00002-of-00002"
          ]
        },
        "model_id": "lmstudio-community/Llama-3.3-70B-Instruct-GGUF",
        "model_file_name_template": "Llama-3.3-70B-Instruct-{quantization}.gguf",
        "model_file_name_split_template": "Llama-3.3-70B-Instruct-{quantization}-{part}.gguf",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\"' + arg_val + '\"' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n            {{- '\"parameters\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      128001,
      128008,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>",
      "<|eom_id|>"
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "tiny-llama",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 1,
        "quantizations": [
          "Q2_K"
        ],
        "model_id": "Xorbits/TinyLlama-1.1B-step-50K-105b-GGUF",
        "model_hub": "modelscope",
        "model_revision": "v0.0.1",
        "model_file_name_template": "ggml-model-{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "baichuan-2-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Baichuan2-chat is a fine-tuned version of the Baichuan LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-7B-Chat",
        "model_hub": "modelscope",
        "model_revision": "v1.0.4"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-13B-Chat",
        "model_hub": "modelscope",
        "model_revision": "v1.0.3"
      }
    ],
    "chat_template": "{{ (messages|selectattr('role', 'equalto', 'system')|list|last).content|trim if (messages|selectattr('role', 'equalto', 'system')|list) else '' }}\n\n{% for message in messages %}\n{% if message['role'] == 'user' %}\n<reserved_106>\n{{ message['content']|trim -}}\n{% if not loop.last %}\n\n\n{% endif %}\n{% elif message['role'] == 'assistant' %}\n<reserved_107>\n{{ message['content']|trim -}}\n{% if not loop.last %}\n\n\n{% endif %}\n{% endif %}\n{% endfor %}\n{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}\n<reserved_107>\n{% endif %}",
    "stop_token_ids": [
      2,
      195
    ],
    "stop": []
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "baichuan-2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Baichuan2 is an open-source Transformer based LLM that is trained on both Chinese and English data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-7B-Base",
        "model_revision": "v1.0.2",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-13B-Base",
        "model_revision": "v1.0.3",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "glm4-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "GLM4 is the open source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-4-9b-chat-hf",
        "model_revision": "master"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "Q2_K",
          "IQ3_XS",
          "IQ3_S",
          "IQ3_M",
          "Q3_K_S",
          "Q3_K_L",
          "Q3_K",
          "IQ4_XS",
          "IQ4_NL",
          "Q4_K_S",
          "Q4_K",
          "Q5_K_S",
          "Q5_K",
          "Q6_K",
          "Q8_0",
          "BF16",
          "FP16"
        ],
        "model_file_name_template": "glm-4-9b-chat.{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "LLM-Research/glm-4-9b-chat-GGUF",
        "model_revision": "master"
      }
    ],
    "chat_template": "[gMASK]<sop>{% for item in messages %}{% if item['tools'] is defined %}<|system|>\n你是一个名为 ChatGLM 的人工智能助手。你是基于智谱AI训练的语言模型 GLM-4 模型开发的，你的任务是针对用户的问题和要求提供适当的答复和支持。\n\n# 可用工具{% set tools = item['tools'] %}{% for tool in tools %}{% if tool['type'] == 'function' %}\n\n## {{ tool['function']['name'] }}\n\n{{ tool['function'] | tojson(indent=4) }}\n在调用上述函数时，请使用 Json 格式表示调用的参数。{% elif tool['type'] == 'python' %}\n\n## python\n\n当你向 `python` 发送包含 Python 代码的消息时，该代码将会在一个有状态的 Jupyter notebook 环境中执行。\n`python` 返回代码执行的输出，或在执行 60 秒后返回超时。\n`/mnt/data` 将会持久化存储你的文件。在此会话中，`python` 无法访问互联网。不要使用 `python` 进行任何网络请求或者在线 API 调用，这些在线内容的访问将不会成功。{% elif tool['type'] == 'simple_browser' %}\n\n## simple_browser\n\n你可以使用 `simple_browser` 工具。该工具支持以下函数：\n`search(query: str, recency_days: int)`：使用搜索引擎进行查询并显示结果，可以使用 `recency_days` 参数控制搜索内容的时效性。\n`mclick(ids: list[int])`：获取一系列指定 id 的页面内容。每次调用时，须选择3-10个页面。选择多个角度的页面，同时尽可能选择可信任的信息来源。考虑到部分页面是无法加载的，你也可以多打开一些可能有用的页面而不用担心内容过多。\n`open_url(url: str)`：打开指定的 URL。\n\n使用 `【{引用 id}†{引用文本}】` 来引用内容。\n\n操作步骤：1. 使用 `search` 来获得信息列表; 2. 使用 `mclick` 来获取指定 ID 页面的内容; 3. 根据获得的内容进行回复。在回复中应当引用信息来源。\n 如果用户提供了 URL，也可以用 `open_url` 直接打开页面。\n如果初次搜索结果没有找到合适的信息，也可以再次使用 `search` 进行搜索。{% elif tool['type'] == 'cogview' %}\n\n## cogview\n\n如果用户的请求中包含了对图像的描述，你可以使用 `cogview` 来生成图像并展示给用户。你需要向 `cogview` 发送图像描述，规则：\n- 发送给 `cogview` 的消息必须使用英语。用户的中文描述必须完全翻译为英语。\n- 应当尽可能详细地描述图像生成的需求，需求描述约 100 英文单词。\n- 保持用户原始描述的意图。不要虚构内容或者没见过的人物。\n- 如无特殊说明，所在地为中国，持有中国立场并遵循中国社会主义价值观。{% endif %}{% endfor %}{% endif %}{% if item['content'] %}<|{{ item['role'] }}|>{{ item['metadata'] }}\n{{ item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ]
  },
  {
    "version": 1,
    "context_length": 1048576,
    "model_name": "glm4-chat-1m",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "GLM4 is the open source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-4-9b-chat-1m-hf",
        "model_revision": "master"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "Q2_K",
          "IQ3_XS",
          "IQ3_S",
          "IQ3_M",
          "Q3_K_S",
          "Q3_K_L",
          "Q3_K",
          "IQ4_XS",
          "IQ4_NL",
          "Q4_K_S",
          "Q4_K",
          "Q5_K_S",
          "Q5_K",
          "Q6_K",
          "Q8_0",
          "BF16",
          "FP16"
        ],
        "model_file_name_template": "glm-4-9b-chat-1m.{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "LLM-Research/glm-4-9b-chat-1m-GGUF",
        "model_revision": "master"
      }
    ],
    "chat_template": "[gMASK]<sop>{% for item in messages %}{% if item['tools'] is defined %}<|system|>\n你是一个名为 GLM-4 的人工智能助手。你是基于智谱AI训练的语言模型 GLM-4 模型开发的，你的任务是针对用户的问题和要求提供适当的答复和支持。\n\n# 可用工具{% set tools = item['tools'] %}{% for tool in tools %}{% if tool['type'] == 'function' %}\n\n## {{ tool['function']['name'] }}\n\n{{ tool['function'] | tojson(indent=4) }}\n在调用上述函数时，请使用 Json 格式表示调用的参数。{% elif tool['type'] == 'python' %}\n\n## python\n\n当你向 `python` 发送包含 Python 代码的消息时，该代码将会在一个有状态的 Jupyter notebook 环境中执行。\n`python` 返回代码执行的输出，或在执行 60 秒后返回超时。\n`/mnt/data` 将会持久化存储你的文件。在此会话中，`python` 无法访问互联网。不要使用 `python` 进行任何网络请求或者在线 API 调用，这些在线内容的访问将不会成功。{% elif tool['type'] == 'simple_browser' %}\n\n## simple_browser\n\n你可以使用 `simple_browser` 工具。该工具支持以下函数：\n`search(query: str, recency_days: int)`：使用搜索引擎进行查询并显示结果，可以使用 `recency_days` 参数控制搜索内容的时效性。\n`mclick(ids: list[int])`：获取一系列指定 id 的页面内容。每次调用时，须选择3-10个页面。选择多个角度的页面，同时尽可能选择可信任的信息来源。考虑到部分页面是无法加载的，你也可以多打开一些可能有用的页面而不用担心内容过多。\n`open_url(url: str)`：打开指定的 URL。\n\n使用 `【{引用 id}†{引用文本}】` 来引用内容。\n\n操作步骤：1. 使用 `search` 来获得信息列表; 2. 使用 `mclick` 来获取指定 ID 页面的内容; 3. 根据获得的内容进行回复。在回复中应当引用信息来源。\n 如果用户提供了 URL，也可以用 `open_url` 直接打开页面。\n如果初次搜索结果没有找到合适的信息，也可以再次使用 `search` 进行搜索。{% elif tool['type'] == 'cogview' %}\n\n## cogview\n\n如果用户的请求中包含了对图像的描述，你可以使用 `cogview` 来生成图像并展示给用户。你需要向 `cogview` 发送图像描述，规则：\n- 发送给 `cogview` 的消息必须使用英语。用户的中文描述必须完全翻译为英语。\n- 应当尽可能详细地描述图像生成的需求，需求描述约 100 英文单词。\n- 保持用户原始描述的意图。不要虚构内容或者没见过的人物。\n- 如无特殊说明，所在地为中国，持有中国立场并遵循中国社会主义价值观。{% endif %}{% endfor %}{% endif %}{% if item['content'] %}<|{{ item['role'] }}|>{{ item['metadata'] }}\n{{ item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "glm-4v",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "GLM4 is the open source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-4v-9b",
        "model_revision": "master"
      }
    ],
    "chat_template": "",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "codegeex4",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "the open-source version of the latest CodeGeeX4 model series",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "none"
        ],
        "model_id": "ZhipuAI/codegeex4-all-9b",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "IQ2_M",
          "IQ3_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K_L",
          "Q8_0"
        ],
        "model_file_name_template": "codegeex4-all-9b-{quantization}.gguf",
        "model_id": "ZhipuAI/codegeex4-all-9b-GGUF",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ '<|system|>\n' + item['content'] }}{% elif loop.first %}{{ '<|system|>\n你是一位智能编程助手，你叫CodeGeeX。你会为用户回答关于编程、代码、计算机方面的任何问题，并提供格式规范、可以执行、准确安全的代码，并在必要时提供详细的解释。' }}{% endif %}{% if item['role'] == 'user' %}{{ '<|user|>\n' + item['content'] }}{% elif item['role'] == 'assistant' %}{{ '<|assistant|>\n' + item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n' }}{% endif %}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "xverse-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "XVERSE-7B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "xverse/XVERSE-7B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "xverse/XVERSE-13B-Chat",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ '<|system|> \n' + item['content'] }}{% endif %}{% if item['role'] == 'user' %}{{ '<|user|> \n' + item['content'] }}{% elif item['role'] == 'assistant' %}{{ '<|assistant|> \n' + item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% endif %}",
    "stop_token_ids": [
      3
    ],
    "stop": [
      "<|endoftext|>"
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "xverse",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "XVERSE is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "xverse/XVERSE-7B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "none"
        ],
        "model_id": "xverse/XVERSE-13B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 65,
        "quantizations": [
          "none"
        ],
        "model_id": "xverse/XVERSE-65B",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "wizardcoder-python-v1.0",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/WizardCoder-Python-13B-V1.0",
        "model_revision": "v1.0.0"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/WizardCoder-Python-34B-V1.0",
        "model_revision": "v1.0.0"
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ item['content'] + '\n\n### ' }}{% elif loop.first %}{{ 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### ' }}{% endif %}{% if item['role'] == 'user' %}{{ 'Instruction: ' + item['content'] + '\n\n### ' }}{% elif item['role'] == 'assistant' %}{{ 'Response: ' + item['content'] + '\n\n### ' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Response: Let\\'s think step by step.' }}{% endif %}",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "code-llama",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Code-Llama is an open-source LLM trained by fine-tuning LLaMA2 for generating and discussing code.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-7b-hf",
        "model_revision": "v1.0.2"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-13b-hf",
        "model_revision": "v1.0.1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-34b-hf",
        "model_revision": "v1.0.1"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8194,
    "model_name": "codeshell",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "CodeShell is a multi-language code LLM developed by the Knowledge Computing Lab of Peking University. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "WisdomShell/CodeShell-7B",
        "model_revision": "master",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8194,
    "model_name": "codeshell-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "CodeShell is a multi-language code LLM developed by the Knowledge Computing Lab of Peking University.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "WisdomShell/CodeShell-7B-Chat",
        "model_revision": "master",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% for item in messages %}{% if item['role'] == 'user' %}{{ '## human: ' + item['content'] + '|<end>|' }}{% elif item['role'] == 'assistant' %}{{ '## assistant: ' + item['content'] + '|<end>|' }}{% endif %}{% endfor %}{{ '## assistant: ' }}",
    "stop_token_ids": [
      70000
    ],
    "stop": [
      "<|endoftext|>",
      "|||",
      "|<end>|"
    ]
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "code-llama-instruct",
    "model_description": "Code-Llama-Instruct is an instruct-tuned version of the Code-Llama LLM.",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-7b-Instruct-hf",
        "model_revision": "v1.0.1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-13b-Instruct-hf",
        "model_revision": "v1.0.1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-34b-Instruct-hf",
        "model_revision": "v1.0.2"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-7B-Instruct-GGUF",
        "model_file_name_template": "codellama-7b-instruct.{quantization}.gguf",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-13B-Instruct-GGUF",
        "model_file_name_template": "codellama-13b-instruct.{quantization}.gguf",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-34B-Instruct-GGUF",
        "model_file_name_template": "codellama-34b-instruct.{quantization}.gguf",
        "model_revision": "v0.1.0"
      }
    ],
    "chat_template": "{% if messages[0]['role'] == 'system' %}{% set system_message = '<<SYS>>\n' + messages[0]['content'] | trim + '\n<</SYS>>\n\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ '<s>' + '[INST] ' + content | trim + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + content | trim + ' ' + '</s>' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "mistral-v0.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Mistral-7B is a unmoderated Transformer based LLM claiming to outperform Llama2 on all benchmarks.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Mistral-7B-v0.1",
        "model_revision": "v1.0.0"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Mistral-7B-v0.1-GGUF",
        "model_file_name_template": "mistral-7b-v0.1.{quantization}.gguf",
        "model_revision": "v1.0.0"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "code-llama-python",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Code-Llama-Python is a fine-tuned version of the Code-Llama LLM, specializing in Python.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "Xorbits/CodeLlama-7B-Python-GGUF",
        "model_hub": "modelscope",
        "model_revision": "v1.0.0",
        "model_file_name_template": "codellama-7b-python.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-13B-Python-GGUF",
        "model_file_name_template": "codellama-13b-python.{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-13B-Python-fp16",
        "model_revision": "v1.0.0"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-7B-Python-fp16",
        "model_revision": "v1.0.0"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-13b-Python-hf",
        "model_revision": "v1.0.1"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "mixtral-v0.1",
    "model_lang": [
      "en",
      "fr",
      "it",
      "de",
      "es"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/Mixtral-8x7B-v0.1",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "mixtral-instruct-v0.1",
    "model_lang": [
      "en",
      "fr",
      "it",
      "de",
      "es"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-8x7B-Instruct is a fine-tuned version of the Mistral-8x7B LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/Mixtral-8x7B-Instruct-v0.1",
        "model_revision": "master"
      }
    ],
    "chat_template": "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n    {%- endif %}\n    {%- if message['role'] == 'user' %}\n        {%- if loop.first and system_message is defined %}\n            {{- ' [INST] ' + system_message + '\n\n' + message['content'] + ' [/INST]' }}\n        {%- else %}\n            {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n        {%- endif %}\n    {%- elif message['role'] == 'assistant' %}\n        {{- ' ' + message['content'] + '</s>'}}\n    {%- else %}\n        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-6B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-9B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-34B",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 262144,
    "model_name": "Yi-200k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI. The first public release contains two bilingual (English/Chinese) base models with the parameter sizes of 6B and 34B. Both of them are trained with 4K sequence length and can be extended to 32K during inference time.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-6B-200K",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-34B-200K",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
    "model_specs": [
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "quantizations": [
          "8bits"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-34B-Chat-{quantization}",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-6B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-34B-Chat",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      2,
      6,
      7,
      8
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>",
      "<|im_sep|>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi-1.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-6B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-9B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-34B",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi-1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-6B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-9B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-34B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 6,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-6B-Chat-GPTQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 9,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-9B-Chat-GPTQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-34B-Chat-GPTQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 6,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-6B-Chat-AWQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 9,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-9B-Chat-AWQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 34,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-34B-Chat-AWQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\n' + content + '<|im_end|>\n<|im_start|>assistant\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\n' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      2,
      6,
      7,
      8
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>",
      "<|im_sep|>"
    ]
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "Yi-1.5-chat-16k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-9B-Chat-16K",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-34B-Chat-16K",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\n' + content + '<|im_end|>\n<|im_start|>assistant\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\n' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      2,
      6,
      7,
      8
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>",
      "<|im_sep|>"
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "wizardmath-v1.0",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "WizardMath is an open-source LLM trained by fine-tuning Llama2 with Evol-Instruct, specializing in math.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/WizardMath-7B-V1.0",
        "model_revision": "v1.0.0"
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ item['content'] + '\n\n### ' }}{% elif loop.first %}{{ 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### ' }}{% endif %}{% if item['role'] == 'user' %}{{ 'Instruction: ' + item['content'] + '\n\n### ' }}{% elif item['role'] == 'assistant' %}{{ 'Response: ' + item['content'] + '\n\n### ' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Response: Let\\'s think step by step.' }}{% endif %}",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "mistral-instruct-v0.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B LLM on public datasets, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Mistral-7B-Instruct-v0.1",
        "model_revision": "v1.0.0"
      }
    ],
    "chat_template": "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n    {%- endif %}\n    {%- if message['role'] == 'user' %}\n        {%- if loop.first and system_message is defined %}\n            {{- ' [INST] ' + system_message + '\n\n' + message['content'] + ' [/INST]' }}\n        {%- else %}\n            {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n        {%- endif %}\n    {%- elif message['role'] == 'assistant' %}\n        {{- ' ' + message['content'] + '</s>'}}\n    {%- else %}\n        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "mistral-instruct-v0.2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an improved instruct fine-tuned version of Mistral-7B-Instruct-v0.1.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/Mistral-7B-Instruct-v0.2"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Mistral-7B-Instruct-v0.2-GGUF",
        "model_file_name_template": "mistral-7b-instruct-v0.2.{quantization}.gguf"
      }
    ],
    "chat_template": "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n    {%- endif %}\n    {%- if message['role'] == 'user' %}\n        {%- if loop.first and system_message is defined %}\n            {{- ' [INST] ' + system_message + '\n\n' + message['content'] + ' [/INST]' }}\n        {%- else %}\n            {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n        {%- endif %}\n    {%- elif message['role'] == 'assistant' %}\n        {{- ' ' + message['content'] + '</s>'}}\n    {%- else %}\n        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 1024000,
    "model_name": "mistral-nemo-instruct",
    "model_lang": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "zh",
      "ru",
      "ja"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-Nemo-Base-2407",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "quantizations": [
          "none"
        ],
        "model_id": "AI-ModelScope/Mistral-Nemo-Instruct-2407",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 12,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Mistral-Nemo-Instruct-2407-gptq-4bit",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS][\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- '{\"type\": \"function\", \"function\": {' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- '\"' + key + '\": \"' + val + '\"' }}\n                    {%- else %}\n                        {{- '\"' + key + '\": ' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST]\" + system_message + \"\n\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST]\" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\n        {{- \"[TOOL_CALLS][\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + '</s>' }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- message[\"content\"] + '</s>'}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- '[TOOL_RESULTS]{\"content\": ' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "mistral-large-instruct",
    "model_lang": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "zh",
      "ru",
      "ja",
      "ko"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-Large-Instruct-2407 is an advanced dense Large Language Model (LLM) of 123B parameters with state-of-the-art reasoning, knowledge and coding capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 123,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Mistral-Large-Instruct-2407",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 123,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Mistral-Large-Instruct-2407-bnb-4bit",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- '<s>' }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS][\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- '{\"type\": \"function\", \"function\": {' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- '\"' + key + '\": \"' + val + '\"' }}\n                    {%- else %}\n                        {{- '\"' + key + '\": ' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST]\" + system_message + \"\n\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST]\" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\n        {{- \"[TOOL_CALLS][\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + '</s>' }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- message[\"content\"] + '</s>'}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- '[TOOL_RESULTS]{\"content\": ' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Qwen-chat is a fine-tuned version of the Qwen LLM trained with alignment techniques, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Qwen-7B-Chat-GGUF",
        "model_file_name_template": "Qwen-7B-Chat.{quantization}.gguf",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Qwen-14B-Chat-GGUF",
        "model_file_name_template": "Qwen-14B-Chat.{quantization}.gguf",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen-1_8B-Chat",
        "model_revision": "v1.0.0"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen-7B-Chat",
        "model_revision": "v1.1.9"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen-72B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen-14B-Chat",
        "model_hub": "modelscope",
        "model_revision": "v1.0.7"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen-1_8B-Chat-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen-7B-Chat-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "v1.1.7"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen-14B-Chat-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "v1.0.7"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen-72B-Chat-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for item in messages %}{% if loop.first and item['role'] == 'system' %}{{ '<|im_start|>system\n' + item['content'] + '<|im_end|>\n' }}{% elif loop.first %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{% if item['role'] == 'user' %}{{ '<|im_start|>user\n' + item['content'] + '<|im_end|>' }}{% elif item['role'] == 'assistant' %}{{ '<|im_start|>assistant\n' + item['content'] + '<|im_end|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen1.5-0.5B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen1.5-1.8B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen1.5-4B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen1.5-7B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen1.5-14B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen1.5-32B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen1.5-72B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 110,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen1.5-110B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-0.5B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-1.8B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 4,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-4B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-7B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-14B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-32B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-72B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 110,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-110B-Chat-GPTQ-Int4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-0.5B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-1.8B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 4,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-4B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-7B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-14B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-32B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-72B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 110,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-110B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-0.5B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-0_5b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-1.8B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-1_8b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 4,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-4B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-4b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-7B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-7b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-14B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-14b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-32B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-32b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 72,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_k_m"
        ],
        "model_id": "qwen/Qwen1.5-72B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-72b-chat-{quantization}.gguf",
        "model_file_name_split_template": "qwen1_5-72b-chat-{quantization}.gguf.{part}",
        "quantization_parts": {
          "q4_k_m": [
            "a",
            "b"
          ]
        }
      }
    ],
    "chat_template": "{%- macro json_to_python_type(json_spec) %}\n    {%- set basic_type_map = {\n    \"string\": \"str\",\n    \"number\": \"float\",\n    \"integer\": \"int\",\n    \"boolean\": \"bool\"\n} %}\n    {%- if basic_type_map[json_spec.type] is defined %}\n        {{- basic_type_map[json_spec.type] }}\n    {%- elif json_spec.type == \"array\" %}\n        {{- \"list[\" +  json_to_python_type(json_spec|items) + \"]\" }}\n    {%- elif json_spec.type == \"object\" %}\n        {%- if json_spec.additionalProperties is defined %}\n            {{- \"dict[str, \" + json_to_python_type(json_spec.additionalProperties) + ']' }}\n        {%- else %}\n            {{- \"dict\" }}\n        {%- endif %}\n    {%- elif json_spec.type is iterable %}\n        {{- \"Union[\" }}\n        {%- for t in json_spec.type %}\n            {{- json_to_python_type({\"type\": t}) }}\n            {%- if not loop.last %}\n                {{- \",\" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \"]\" }}\n    {%- else %}\n        {{- \"Any\" }}\n    {%- endif %}\n{%- endmacro %}\n\n{%- if tools %}\n    {{- '<|im_start|>system\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] + '\n\n' }}\n    {%- endif %}\n    {{- '# Tools\n\n' }}\n    {{- \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- '{\"type\": \"function\", \"function\": ' }}\n        {{- '{\"name\": ' + tool.name + '\", ' }}\n        {{- '\"description\": \"' + tool.name + '(' }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {{- param_name + \": \" + json_to_python_type(param_fields) }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \")\" }}\n        {%- if tool.return is defined %}\n            {{- \" -> \" + json_to_python_type(tool.return) }}\n        {%- endif %}\n        {{- \" - \" + tool.description + \"\n\n\" }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {%- if loop.first %}\n                {{- \"    Args:\n\" }}\n            {%- endif %}\n            {{- \"        \" + param_name + \"(\" + json_to_python_type(param_fields) + \"): \" + param_fields.description|trim }}\n        {%- endfor %}\n        {%- if tool.return is defined and tool.return.description is defined %}\n            {{- \"\n    Returns:\n        \" + tool.return.description }}\n        {%- endif %}\n        {{- '\"' }}\n        {{- ', \"parameters\": ' }}\n        {%- if tool.parameters.properties | length == 0 %}\n            {{- \"{}\" }}\n        {%- else %}\n            {{- tool.parameters|tojson }}\n        {%- endif %}\n        {{- \"}\" }}\n        {%- if not loop.last %}\n            {{- \"\n\" }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \" </tools>\" }}\n    {{- 'Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"}\n' }}\n    {{- \"For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n\" }}\n    {{- \"<tool_call>\n\" }}\n    {{- '{\"name\": <function-name>, \"arguments\": <args-json-object>}\n' }}\n    {{- '</tool_call><|im_end|>\n' }}\n{%- else %}\n    {%- if messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and message.tool_calls is not defined) %}\n        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role + '\n<tool_call>\n' }}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '{' }}\n            {{- '\"name\": \"' }}\n            {{- tool_call.name }}\n            {%- if tool_call.arguments is defined %}\n                {{- ', ' }}\n                {{- '\"arguments\": ' }}\n                {{- tool_call.arguments|tojson }}\n            {%- endif %}\n            {{- '\"}' }}\n            {{- '\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if not message.name is defined %}\n            {{- raise_exception(\"Tool response dicts require a 'name' key indicating the name of the called function!\") }}\n        {%- endif %}\n        {{- '<|im_start|>user\n<tool_response>\n' }}\n        {{- '{\"name\": \"' }}\n        {{- message.name }}\n        {{- '\", \"content\": ' }}\n        {{- message.content|tojson + '}' }}\n        {{- '\n</tool_response><|im_end|>\n' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen1.5-moe-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen1.5-MoE is a transformer-based MoE decoder-only language model pretrained on a large amount of data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "2_7",
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen1.5-MoE-A2.7B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "2_7",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- macro json_to_python_type(json_spec) %}\n    {%- set basic_type_map = {\n    \"string\": \"str\",\n    \"number\": \"float\",\n    \"integer\": \"int\",\n    \"boolean\": \"bool\"\n} %}\n    {%- if basic_type_map[json_spec.type] is defined %}\n        {{- basic_type_map[json_spec.type] }}\n    {%- elif json_spec.type == \"array\" %}\n        {{- \"list[\" +  json_to_python_type(json_spec|items) + \"]\" }}\n    {%- elif json_spec.type == \"object\" %}\n        {%- if json_spec.additionalProperties is defined %}\n            {{- \"dict[str, \" + json_to_python_type(json_spec.additionalProperties) + ']' }}\n        {%- else %}\n            {{- \"dict\" }}\n        {%- endif %}\n    {%- elif json_spec.type is iterable %}\n        {{- \"Union[\" }}\n        {%- for t in json_spec.type %}\n            {{- json_to_python_type({\"type\": t}) }}\n            {%- if not loop.last %}\n                {{- \",\" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \"]\" }}\n    {%- else %}\n        {{- \"Any\" }}\n    {%- endif %}\n{%- endmacro %}\n\n{%- if tools %}\n    {{- '<|im_start|>system\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] + '\n\n' }}\n    {%- endif %}\n    {{- '# Tools\n\n' }}\n    {{- \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- '{\"type\": \"function\", \"function\": ' }}\n        {{- '{\"name\": ' + tool.name + '\", ' }}\n        {{- '\"description\": \"' + tool.name + '(' }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {{- param_name + \": \" + json_to_python_type(param_fields) }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \")\" }}\n        {%- if tool.return is defined %}\n            {{- \" -> \" + json_to_python_type(tool.return) }}\n        {%- endif %}\n        {{- \" - \" + tool.description + \"\n\n\" }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {%- if loop.first %}\n                {{- \"    Args:\n\" }}\n            {%- endif %}\n            {{- \"        \" + param_name + \"(\" + json_to_python_type(param_fields) + \"): \" + param_fields.description|trim }}\n        {%- endfor %}\n        {%- if tool.return is defined and tool.return.description is defined %}\n            {{- \"\n    Returns:\n        \" + tool.return.description }}\n        {%- endif %}\n        {{- '\"' }}\n        {{- ', \"parameters\": ' }}\n        {%- if tool.parameters.properties | length == 0 %}\n            {{- \"{}\" }}\n        {%- else %}\n            {{- tool.parameters|tojson }}\n        {%- endif %}\n        {{- \"}\" }}\n        {%- if not loop.last %}\n            {{- \"\n\" }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \" </tools>\" }}\n    {{- 'Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"}\n' }}\n    {{- \"For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n\" }}\n    {{- \"<tool_call>\n\" }}\n    {{- '{\"name\": <function-name>, \"arguments\": <args-json-object>}\n' }}\n    {{- '</tool_call><|im_end|>\n' }}\n{%- else %}\n    {%- if messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and message.tool_calls is not defined) %}\n        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role + '\n<tool_call>\n' }}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '{' }}\n            {{- '\"name\": \"' }}\n            {{- tool_call.name }}\n            {%- if tool_call.arguments is defined %}\n                {{- ', ' }}\n                {{- '\"arguments\": ' }}\n                {{- tool_call.arguments|tojson }}\n            {%- endif %}\n            {{- '\"}' }}\n            {{- '\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if not message.name is defined %}\n            {{- raise_exception(\"Tool response dicts require a 'name' key indicating the name of the called function!\") }}\n        {%- endif %}\n        {{- '<|im_start|>user\n<tool_response>\n' }}\n        {{- '{\"name\": \"' }}\n        {{- message.name }}\n        {{- '\", \"content\": ' }}\n        {{- message.content|tojson + '}' }}\n        {{- '\n</tool_response><|im_end|>\n' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 65536,
    "model_name": "codeqwen1.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "CodeQwen1.5 is the Code-Specific version of Qwen1.5. It is a transformer-based decoder-only language model pretrained on a large amount of data of codes.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/CodeQwen1.5-7B",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 65536,
    "model_name": "codeqwen1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "CodeQwen1.5 is the Code-Specific version of Qwen1.5. It is a transformer-based decoder-only language model pretrained on a large amount of data of codes.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/CodeQwen1.5-7B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "codeqwen-1_5-7b-chat-{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/CodeQwen1.5-7B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/CodeQwen1.5-7B-Chat-AWQ",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2 is the new series of Qwen large language models",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2-7B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2-72B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2-7B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2-72B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-7B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-72B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 7,
        "quantizations": [
          "fp8"
        ],
        "model_id": "liuzhenghua/Qwen2-7B-FP8-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 72,
        "quantizations": [
          "fp8"
        ],
        "model_id": "liuzhenghua/Qwen2-72B-FP8-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "4bit"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct-MLX",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "4bit"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct-MLX",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "quantizations": [
          "4bit"
        ],
        "model_id": "qwen/Qwen2-7B-Instruct-MLX",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct-GGUF",
        "model_file_name_template": "qwen2-0_5b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct-GGUF",
        "model_file_name_template": "qwen2-1_5b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "qwen/Qwen2-7B-Instruct-GGUF",
        "model_file_name_template": "qwen2-7b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 72,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "qwen/Qwen2-72B-Instruct-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen2-72b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2-72b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q5_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q5_k_m": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q6_k": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q8_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "fp16": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ]
        }
      }
    ],
    "chat_template": "{%- macro json_to_python_type(json_spec) %}\n    {%- set basic_type_map = {\n    \"string\": \"str\",\n    \"number\": \"float\",\n    \"integer\": \"int\",\n    \"boolean\": \"bool\"\n} %}\n    {%- if basic_type_map[json_spec.type] is defined %}\n        {{- basic_type_map[json_spec.type] }}\n    {%- elif json_spec.type == \"array\" %}\n        {{- \"list[\" +  json_to_python_type(json_spec|items) + \"]\" }}\n    {%- elif json_spec.type == \"object\" %}\n        {%- if json_spec.additionalProperties is defined %}\n            {{- \"dict[str, \" + json_to_python_type(json_spec.additionalProperties) + ']' }}\n        {%- else %}\n            {{- \"dict\" }}\n        {%- endif %}\n    {%- elif json_spec.type is iterable %}\n        {{- \"Union[\" }}\n        {%- for t in json_spec.type %}\n            {{- json_to_python_type({\"type\": t}) }}\n            {%- if not loop.last %}\n                {{- \",\" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \"]\" }}\n    {%- else %}\n        {{- \"Any\" }}\n    {%- endif %}\n{%- endmacro %}\n\n{%- if tools %}\n    {{- '<|im_start|>system\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] + '\n\n' }}\n    {%- endif %}\n    {{- '# Tools\n\n' }}\n    {{- \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- '{\"type\": \"function\", \"function\": ' }}\n        {{- '{\"name\": ' + tool.name + '\", ' }}\n        {{- '\"description\": \"' + tool.name + '(' }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {{- param_name + \": \" + json_to_python_type(param_fields) }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \")\" }}\n        {%- if tool.return is defined %}\n            {{- \" -> \" + json_to_python_type(tool.return) }}\n        {%- endif %}\n        {{- \" - \" + tool.description + \"\n\n\" }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {%- if loop.first %}\n                {{- \"    Args:\n\" }}\n            {%- endif %}\n            {{- \"        \" + param_name + \"(\" + json_to_python_type(param_fields) + \"): \" + param_fields.description|trim }}\n        {%- endfor %}\n        {%- if tool.return is defined and tool.return.description is defined %}\n            {{- \"\n    Returns:\n        \" + tool.return.description }}\n        {%- endif %}\n        {{- '\"' }}\n        {{- ', \"parameters\": ' }}\n        {%- if tool.parameters.properties | length == 0 %}\n            {{- \"{}\" }}\n        {%- else %}\n            {{- tool.parameters|tojson }}\n        {%- endif %}\n        {{- \"}\" }}\n        {%- if not loop.last %}\n            {{- \"\n\" }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \" </tools>\" }}\n    {{- 'Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"}\n' }}\n    {{- \"For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n\" }}\n    {{- \"<tool_call>\n\" }}\n    {{- '{\"name\": <function-name>, \"arguments\": <args-json-object>}\n' }}\n    {{- '</tool_call><|im_end|>\n' }}\n{%- else %}\n    {%- if messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and message.tool_calls is not defined) %}\n        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role + '\n<tool_call>\n' }}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '{' }}\n            {{- '\"name\": \"' }}\n            {{- tool_call.name }}\n            {%- if tool_call.arguments is defined %}\n                {{- ', ' }}\n                {{- '\"arguments\": ' }}\n                {{- tool_call.arguments|tojson }}\n            {%- endif %}\n            {{- '\"}' }}\n            {{- '\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if not message.name is defined %}\n            {{- raise_exception(\"Tool response dicts require a 'name' key indicating the name of the called function!\") }}\n        {%- endif %}\n        {{- '<|im_start|>user\n<tool_response>\n' }}\n        {{- '{\"name\": \"' }}\n        {{- message.name }}\n        {{- '\", \"content\": ' }}\n        {{- message.content|tojson + '}' }}\n        {{- '\n</tool_response><|im_end|>\n' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2-moe-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2 is the new series of Qwen large language models. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2-57B-A14B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-57B-A14B-Instruct-GPTQ-Int4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "qwen/Qwen2-57B-A14B-Instruct-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen2-57b-a14b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2-57b-a14b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q8_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "fp16": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ]
        }
      }
    ],
    "chat_template": "{%- macro json_to_python_type(json_spec) %}\n    {%- set basic_type_map = {\n    \"string\": \"str\",\n    \"number\": \"float\",\n    \"integer\": \"int\",\n    \"boolean\": \"bool\"\n} %}\n    {%- if basic_type_map[json_spec.type] is defined %}\n        {{- basic_type_map[json_spec.type] }}\n    {%- elif json_spec.type == \"array\" %}\n        {{- \"list[\" +  json_to_python_type(json_spec|items) + \"]\" }}\n    {%- elif json_spec.type == \"object\" %}\n        {%- if json_spec.additionalProperties is defined %}\n            {{- \"dict[str, \" + json_to_python_type(json_spec.additionalProperties) + ']' }}\n        {%- else %}\n            {{- \"dict\" }}\n        {%- endif %}\n    {%- elif json_spec.type is iterable %}\n        {{- \"Union[\" }}\n        {%- for t in json_spec.type %}\n            {{- json_to_python_type({\"type\": t}) }}\n            {%- if not loop.last %}\n                {{- \",\" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \"]\" }}\n    {%- else %}\n        {{- \"Any\" }}\n    {%- endif %}\n{%- endmacro %}\n\n{%- if tools %}\n    {{- '<|im_start|>system\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] + '\n\n' }}\n    {%- endif %}\n    {{- '# Tools\n\n' }}\n    {{- \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- '{\"type\": \"function\", \"function\": ' }}\n        {{- '{\"name\": ' + tool.name + '\", ' }}\n        {{- '\"description\": \"' + tool.name + '(' }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {{- param_name + \": \" + json_to_python_type(param_fields) }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- endif %}\n        {%- endfor %}\n        {{- \")\" }}\n        {%- if tool.return is defined %}\n            {{- \" -> \" + json_to_python_type(tool.return) }}\n        {%- endif %}\n        {{- \" - \" + tool.description + \"\n\n\" }}\n        {%- for param_name, param_fields in tool.parameters.properties|items %}\n            {%- if loop.first %}\n                {{- \"    Args:\n\" }}\n            {%- endif %}\n            {{- \"        \" + param_name + \"(\" + json_to_python_type(param_fields) + \"): \" + param_fields.description|trim }}\n        {%- endfor %}\n        {%- if tool.return is defined and tool.return.description is defined %}\n            {{- \"\n    Returns:\n        \" + tool.return.description }}\n        {%- endif %}\n        {{- '\"' }}\n        {{- ', \"parameters\": ' }}\n        {%- if tool.parameters.properties | length == 0 %}\n            {{- \"{}\" }}\n        {%- else %}\n            {{- tool.parameters|tojson }}\n        {%- endif %}\n        {{- \"}\" }}\n        {%- if not loop.last %}\n            {{- \"\n\" }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \" </tools>\" }}\n    {{- 'Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"}\n' }}\n    {{- \"For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n\" }}\n    {{- \"<tool_call>\n\" }}\n    {{- '{\"name\": <function-name>, \"arguments\": <args-json-object>}\n' }}\n    {{- '</tool_call><|im_end|>\n' }}\n{%- else %}\n    {%- if messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and message.tool_calls is not defined) %}\n        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role + '\n<tool_call>\n' }}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '{' }}\n            {{- '\"name\": \"' }}\n            {{- tool_call.name }}\n            {%- if tool_call.arguments is defined %}\n                {{- ', ' }}\n                {{- '\"arguments\": ' }}\n                {{- tool_call.arguments|tojson }}\n            {%- endif %}\n            {{- '\"}' }}\n            {{- '\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if not message.name is defined %}\n            {{- raise_exception(\"Tool response dicts require a 'name' key indicating the name of the called function!\") }}\n        {%- endif %}\n        {{- '<|im_start|>user\n<tool_response>\n' }}\n        {{- '{\"name\": \"' }}\n        {{- message.name }}\n        {{- '\", \"content\": ' }}\n        {{- message.content|tojson + '}' }}\n        {{- '\n</tool_response><|im_end|>\n' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\n' }}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "deepseek-vl-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "DeepSeek-VL possesses general multimodal understanding capabilities, capable of processing logical diagrams, web pages, formula recognition, scientific literature, natural images, and embodied intelligence in complex scenarios.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-vl-1.3b-chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-vl-7b-chat",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "",
    "stop_token_ids": [
      100001
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "deepseek",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "DDeepSeek LLM, trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-7b-base",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 67,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-67b-base",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "deepseek-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek LLM is an advanced language model comprising 67 billion parameters. It has been trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-7b-chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 67,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-67b-chat",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ '<｜begin▁of▁sentence｜>' }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n\n' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + '<｜end▁of▁sentence｜>' }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}",
    "stop_token_ids": [
      100001
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ]
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "deepseek-coder",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-1.3b-base",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-6.7b-base",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 33,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-33b-base",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "deepseek-coder-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "deepseek-coder-instruct is a model initialized from deepseek-coder-base and fine-tuned on 2B tokens of instruction data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-1.3b-instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-6.7b-instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 33,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-33b-instruct",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}\n{% set add_generation_prompt = false %}\n{% endif %}\n{%- set ns = namespace(found=false) -%}\n{%- for message in messages -%}\n    {%- if message['role'] == 'system' -%}\n        {%- set ns.found = true -%}\n    {%- endif -%}\n{%- endfor -%}\n{{'<｜begin▁of▁sentence｜>'}}{%- if not ns.found -%}\n{{'You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n'}}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'system' %}\n{{ message['content'] }}\n    {%- else %}\n        {%- if message['role'] == 'user' %}\n{{'### Instruction:\n' + message['content'] + '\n'}}\n        {%- else %}\n{{'### Response:\n' + message['content'] + '\n<|EOT|>\n'}}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{% if add_generation_prompt %}\n{{'### Response:'}}\n{% endif %}",
    "stop_token_ids": [
      32021
    ],
    "stop": [
      "<|EOT|>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Skywork",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Skywork is a series of large models developed by the Kunlun Group · Skywork team.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "skywork/Skywork-13B-base",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Skywork-Math",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Skywork is a series of large models developed by the Kunlun Group · Skywork team.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "skywork/Skywork-13B-Math",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "qwen-vl-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Qwen-VL-Chat supports more flexible interaction, such as multiple image inputs, multi-round question answering, and creative capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen-VL-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen-VL-Chat-{quantization}",
        "model_revision": "master"
      }
    ],
    "chat_template": "",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "orion-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Orion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none"
        ],
        "model_id": "OrionStarAI/Orion-14B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "OrionStarAI/Orion-14B-Chat-{quantization}"
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.first %}{{ '<s>' }}{% endif %}{% if message['role'] == 'user' %}{{ 'Human: ' + message['content'] + '\n\nAssistant: ' + '</s>' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + '</s>' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2,
      0
    ],
    "stop": [
      "<s>",
      "</s>",
      "<unk>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "gemma-3-1b-it",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 1,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/gemma-3-1b-it",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 1,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_K_L",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_L",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q6_K_L",
          "Q8_0",
          "bf16"
        ],
        "model_id": "bartowski/google_gemma-3-1b-it-GGUF",
        "model_file_name_template": "google_gemma-3-1b-it-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 1,
        "quantizations": [
          "4bit",
          "6bit",
          "8bit",
          "fp16"
        ],
        "model_id": "mlx-community/gemma-3-1b-it-{quantization}",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{{ bos_token }}\n{%- if messages[0]['role'] == 'system' -%}\n    {%- if messages[0]['content'] is string -%}\n        {%- set first_user_prefix = messages[0]['content'] + '\n\n' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n\n' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message['role'] == 'assistant') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message['role'] -%}\n    {%- endif -%}\n    {{ '<start_of_turn>' + role + '\n' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message['content'] is string -%}\n        {{ message['content'] | trim }}\n    {%- elif message['content'] is iterable -%}\n        {%- for item in message['content'] -%}\n            {%- if item['type'] == 'image' -%}\n                {{ '<start_of_image>' }}\n            {%- elif item['type'] == 'text' -%}\n                {{ item['text'] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ '<end_of_turn>\n' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{'<start_of_turn>model\n'}}\n{%- endif -%}\n",
    "stop_token_ids": [
      1,
      106,
      107
    ],
    "stop": [
      "<eos>",
      "<end_of_turn>",
      "<start_of_turn>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "gemma-3-it",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/gemma-3-4b-it",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/gemma-3-12b-it",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 27,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/gemma-3-27b-it",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 4,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_K_L",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_L",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q6_K_L",
          "Q8_0",
          "bf16"
        ],
        "model_id": "bartowski/google_gemma-3-4b-it-GGUF",
        "model_file_name_template": "google_gemma-3-4b-it-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 12,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_K_L",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_L",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q6_K_L",
          "Q8_0",
          "bf16"
        ],
        "model_id": "bartowski/google_gemma-3-12b-it-GGUF",
        "model_file_name_template": "google_gemma-3-12b-it-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 27,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_K_L",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_L",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q6_K_L",
          "Q8_0",
          "bf16"
        ],
        "model_id": "bartowski/google_gemma-3-27b-it-GGUF",
        "model_file_name_template": "google_gemma-3-27b-it-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 4,
        "quantizations": [
          "4bit",
          "6bit",
          "8bit",
          "fp16"
        ],
        "model_id": "mlx-community/gemma-3-4b-it-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 12,
        "quantizations": [
          "4bit",
          "6bit",
          "8bit",
          "fp16"
        ],
        "model_id": "mlx-community/gemma-3-12b-it-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 27,
        "quantizations": [
          "4bit",
          "6bit",
          "8bit",
          "fp16"
        ],
        "model_id": "mlx-community/gemma-3-27b-it-{quantization}",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{{ bos_token }}\n{%- if messages[0]['role'] == 'system' -%}\n    {%- if messages[0]['content'] is string -%}\n        {%- set first_user_prefix = messages[0]['content'] + '\n\n' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n\n' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message['role'] == 'assistant') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message['role'] -%}\n    {%- endif -%}\n    {{ '<start_of_turn>' + role + '\n' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message['content'] is string -%}\n        {{ message['content'] | trim }}\n    {%- elif message['content'] is iterable -%}\n        {%- for item in message['content'] -%}\n            {%- if item['type'] == 'image' -%}\n                {{ '<start_of_image>' }}\n            {%- elif item['type'] == 'text' -%}\n                {{ item['text'] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ '<end_of_turn>\n' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{'<start_of_turn>model\n'}}\n{%- endif -%}\n",
    "stop_token_ids": [
      1,
      106,
      107
    ],
    "stop": [
      "<eos>",
      "<end_of_turn>",
      "<start_of_turn>"
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "OmniLMM",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "OmniLMM is a family of open-source large multimodal models (LMMs) adept at vision & language modeling.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 3,
        "quantizations": [
          "none"
        ],
        "model_id": "OpenBMB/MiniCPM-V",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "quantizations": [
          "none"
        ],
        "model_id": "OpenBMB/OmniLMM-12B",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "chat_template": "",
    "stop_token_ids": [
      2
    ],
    "stop": [
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-sft-bf16",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/miniCPM-bf16",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{'<用户>' + message['content'].strip() + '<AI>'}}{% else %}{{message['content'].strip()}}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-sft-fp32",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-2B-sft-fp32",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{'<用户>' + message['content'].strip() + '<AI>'}}{% else %}{{message['content'].strip()}}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-dpo-bf16",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-2B-dpo-bf16",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{'<用户>' + message['content'].strip() + '<AI>'}}{% else %}{{message['content'].strip()}}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-dpo-fp16",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-2B-dpo-fp16",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{'<用户>' + message['content'].strip() + '<AI>'}}{% else %}{{message['content'].strip()}}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-dpo-fp32",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-2B-dpo-fp32",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}{{'<用户>' + message['content'].strip() + '<AI>'}}{% else %}{{message['content'].strip()}}{% endif %}{% endfor %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "MiniCPM-Llama3-V-2_5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "MiniCPM-Llama3-V 2.5 is the latest model in the MiniCPM-V series. The model is built on SigLip-400M and Llama3-8B-Instruct with a total of 8B parameters.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-Llama3-V-2_5",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-Llama3-V-2_5-{quantization}",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = '<|begin_of_text|>' + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}",
    "stop_token_ids": [
      128001
    ],
    "stop": [
      "<|end_of_text|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "MiniCPM-V-2.6",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "MiniCPM-V 2.6 is the latest model in the MiniCPM-V series. The model is built on SigLip-400M and Qwen2-7B with a total of 8B parameters.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-V-2_6",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-V-2_6-int4",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ]
  },
  {
    "version": 1,
    "context_length": 128000,
    "model_name": "phi-3-mini-128k-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "LLM-Research/Phi-3-mini-128k-instruct",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n' + message['content'] + '<|end|>\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n' }}{% else %}{{ '<|endoftext|>' }}{% endif %}",
    "stop_token_ids": [
      32000,
      32001,
      32007
    ],
    "stop": [
      "<|endoftext|>",
      "<|assistant|>",
      "<|end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "phi-3-mini-4k-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Phi-3-Mini-4k-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "LLM-Research/Phi-3-mini-4k-instruct",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n' + message['content'] + '<|end|>\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n' }}{% else %}{{ '<|endoftext|>' }}{% endif %}",
    "stop_token_ids": [
      32000,
      32001,
      32007
    ],
    "stop": [
      "<|endoftext|>",
      "<|assistant|>",
      "<|end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "InternVL3",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "InternVL3, an advanced multimodal large language model (MLLM) series that demonstrates superior overall performance.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 1,
        "quantizations": [
          "none"
        ],
        "model_id": "OpenGVLab/InternVL3-1B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 1,
        "quantizations": [
          "Int4"
        ],
        "model_id": "OpenGVLab/InternVL3-1B-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_id": "OpenGVLab/InternVL3-2B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 2,
        "quantizations": [
          "Int4"
        ],
        "model_id": "OpenGVLab/InternVL3-2B-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_id": "OpenGVLab/InternVL3-8B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4"
        ],
        "model_id": "OpenGVLab/InternVL3-8B-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "none"
        ],
        "model_id": "OpenGVLab/InternVL3-9B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 9,
        "quantizations": [
          "Int4"
        ],
        "model_id": "OpenGVLab/InternVL3-9B-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none"
        ],
        "model_id": "OpenGVLab/InternVL3-14B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "OpenGVLab/InternVL3-14B-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 38,
        "quantizations": [
          "none"
        ],
        "model_id": "OpenGVLab/InternVL3-38B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 38,
        "quantizations": [
          "Int4"
        ],
        "model_id": "OpenGVLab/InternVL3-38B-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 78,
        "quantizations": [
          "none"
        ],
        "model_id": "OpenGVLab/InternVL3-78B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 78,
        "quantizations": [
          "Int4"
        ],
        "model_id": "OpenGVLab/InternVL3-78B-AWQ",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151645
    ],
    "stop": [
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "cogvlm2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "CogVLM2 have achieved good results in many lists compared to the previous generation of CogVLM open source models. Its excellent performance can compete with some non-open source models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/cogvlm2-llama3-chinese-chat-19B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/cogvlm2-llama3-chinese-chat-19B-{quantization}",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = '<|begin_of_text|>' + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% else %}{{ '<|end_of_text|>' }}{% endif %}",
    "stop_token_ids": [
      128001,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "cogvlm2-video-llama3-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "CogVLM2-Video achieves state-of-the-art performance on multiple video question answering tasks.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/cogvlm2-video-llama3-chat",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = '<|begin_of_text|>' + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% else %}{{ '<|end_of_text|>' }}{% endif %}",
    "stop_token_ids": [
      128001,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "telechat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The TeleChat is a large language model developed and trained by China Telecom Artificial Intelligence Technology Co., LTD. The 7B model base is trained with 1.5 trillion Tokens and 3 trillion Tokens and Chinese high-quality corpus.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "TeleAI/telechat-7B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "int4",
          "int8"
        ],
        "model_id": "TeleAI/telechat-7B-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "quantizations": [
          "none"
        ],
        "model_id": "TeleAI/TeleChat-12B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 12,
        "quantizations": [
          "int4",
          "int8"
        ],
        "model_id": "TeleAI/TeleChat-12B-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 52,
        "quantizations": [
          "none"
        ],
        "model_id": "TeleAI/TeleChat-52B",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "chat_template": "{{ (messages|selectattr('role', 'equalto', 'system')|list|last).content|trim if (messages|selectattr('role', 'equalto', 'system')|list) else '' }}{%- for message in messages -%}{%- if message['role'] == 'user' -%}{{- '<_user>' + message['content'] +'<_bot>' -}}{%- elif message['role'] == 'assistant' -%}{{- message['content'] + '<_end>' -}}{%- endif -%}{%- endfor -%}",
    "stop": [
      "<_end>",
      "<_start>"
    ],
    "stop_token_ids": [
      160133,
      160132
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2-vl-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Qwen2-VL: To See the World More Clearly.Qwen2-VL is the latest version of the vision language models in the Qwen model familities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2-VL-7B-Instruct",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int8"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2-VL-7B-Instruct-AWQ",
        "model_revision": "master"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "quantizations": [
          "8bit"
        ],
        "model_hub": "modelscope",
        "model_id": "okwinds/Qwen2-VL-7B-Instruct-MLX-8bit",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2-VL-2B-Instruct",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 2,
        "quantizations": [
          "Int8"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2-VL-2B-Instruct-GPTQ-Int8",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 2,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 2,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2-VL-2B-Instruct-AWQ",
        "model_revision": "master"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 2,
        "quantizations": [
          "4bit",
          "8bit"
        ],
        "model_hub": "modelscope",
        "model_id": "mlx-community/Qwen2-VL-2B-Instruct-{quantization}",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2-VL-72B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-VL-72B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2-VL-72B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "quantizations": [
          "4bit",
          "8bit"
        ],
        "model_hub": "modelscope",
        "model_id": "okwinds/Qwen2-VL-72B-Instruct-MLX-{quantization}",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ]
  },
  {
    "version": 1,
    "context_length": 128000,
    "model_name": "qwen2.5-vl-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Qwen2.5-VL: Qwen2.5-VL is the latest version of the vision language models in the Qwen model familities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 3,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen2.5-VL-3B-Instruct"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen2.5-VL-7B-Instruct"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen2.5-VL-32B-Instruct"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen2.5-VL-72B-Instruct"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 3,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen2.5-VL-3B-Instruct-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen2.5-VL-7B-Instruct-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen2.5-VL-32B-Instruct-AWQ"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen2.5-VL-72B-Instruct-AWQ"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 3,
        "quantizations": [
          "3bit",
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_hub": "modelscope",
        "model_id": "mlx-community/Qwen2.5-VL-3B-Instruct-{quantization}"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "quantizations": [
          "3bit",
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_hub": "modelscope",
        "model_id": "mlx-community/Qwen2.5-VL-7B-Instruct-{quantization}"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "quantizations": [
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_hub": "modelscope",
        "model_id": "mlx-community/Qwen2.5-VL-32B-Instruct-{quantization}"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "quantizations": [
          "3bit",
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_hub": "modelscope",
        "model_id": "mlx-community/Qwen2.5-VL-72B-Instruct-{quantization}"
      }
    ],
    "chat_template": "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2.5-omni",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision",
      "audio",
      "omni"
    ],
    "model_description": "Qwen2.5-Omni: the new flagship end-to-end multimodal model in the Qwen series.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 3,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen2.5-Omni-3B"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen2.5-Omni-7B"
      }
    ],
    "chat_template": "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ],
    "virtualenv": {
      "packages": [
        "git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview",
        "numpy==1.26.4",
        "qwen_omni_utils",
        "soundfile"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "minicpm3-4b",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM3-4B is the 3rd generation of MiniCPM series. The overall performance of MiniCPM3-4B surpasses Phi-3.5-mini-Instruct and GPT-3.5-Turbo-0125, being comparable with many recent 7B~9B models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM3-4B",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 4,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM3-4B-GPTQ-Int4",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      1,
      2
    ],
    "stop": [
      "<s>",
      "</s>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2-audio-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "audio"
    ],
    "model_description": "Qwen2-Audio: A large-scale audio-language model which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2-Audio-7B-Instruct",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant",
      "roles": [
        "user",
        "assistant"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2-audio",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "audio"
    ],
    "model_description": "Qwen2-Audio: A large-scale audio-language model which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2-Audio-7B",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant",
      "roles": [
        "user",
        "assistant"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 128000,
    "model_name": "deepseek-v2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 16,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-V2-Lite",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 236,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-V2",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 128000,
    "model_name": "deepseek-v2-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 16,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-V2-Lite-Chat",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 236,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-V2-Chat",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ '<｜begin▁of▁sentence｜>' }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n\n' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + '<｜end▁of▁sentence｜>' }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}",
    "stop_token_ids": [
      100001
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ]
  },
  {
    "version": 1,
    "context_length": 128000,
    "model_name": "deepseek-v2-chat-0628",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek-V2-Chat-0628 is an improved version of DeepSeek-V2-Chat. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 236,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-V2-Chat-0628",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ '<｜begin▁of▁sentence｜>' }}{% for message in messages %}{% if message['role'] == 'user' %}{{ '<｜User｜>' + message['content'] }}{% elif message['role'] == 'assistant' %}{{ '<｜Assistant｜>' + message['content'] + '<｜end▁of▁sentence｜>' }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<｜Assistant｜>' }}{% endif %}",
    "stop_token_ids": [
      100001
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ]
  },
  {
    "version": 1,
    "context_length": 128000,
    "model_name": "deepseek-v2.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. The new model integrates the general and coding abilities of the two previous versions.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 236,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-V2.5",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}    {%- if message['role'] == 'system' %}        {% set ns.system_prompt = message['content'] %}    {%- endif %}{%- endfor %}{{'<｜begin▁of▁sentence｜>'}}{{ns.system_prompt}}{%- for message in messages %}    {%- if message['role'] == 'user' %}    {%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}    {%- endif %}    {%- if message['role'] == 'assistant' and message['content'] is none %}        {%- set ns.is_tool = false -%}        {%- for tool in message['tool_calls']%}            {%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}            {%- set ns.is_first = true -%}            {%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}                   {%- endif %}        {%- endfor %}    {%- endif %}    {%- if message['role'] == 'assistant' and message['content'] is not none %}        {%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}        {%- set ns.is_tool = false -%}        {%- else %}{{'<｜Assistant｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}        {%- endif %}    {%- endif %}    {%- if message['role'] == 'tool' %}        {%- set ns.is_tool = true -%}        {%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}        {%- set ns.is_output_first = false %}        {%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}        {%- endif %}    {%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}",
    "stop_token_ids": [
      100001
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ]
  },
  {
    "version": 1,
    "context_length": 163840,
    "model_name": "deepseek-v3",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 671,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-V3",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 671,
        "quantizations": [
          "Int4"
        ],
        "model_id": "cognitivecomputations/DeepSeek-V3-awq",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 671,
        "quantizations": [
          "Q2_K_L",
          "Q2_K_XS",
          "Q3_K_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "unsloth/DeepSeek-V3-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "DeepSeek-V3-{quantization}/DeepSeek-V3-{quantization}.gguf",
        "model_file_name_split_template": "DeepSeek-V3-{quantization}/DeepSeek-V3-{quantization}-{part}.gguf",
        "quantization_parts": {
          "Q2_K_L": [
            "00001-of-00005",
            "00002-of-00005",
            "00003-of-00005",
            "00004-of-00005",
            "00005-of-00005"
          ],
          "Q2_K_XS": [
            "00001-of-00005",
            "00002-of-00005",
            "00003-of-00005",
            "00004-of-00005",
            "00005-of-00005"
          ],
          "Q3_K_M": [
            "00001-of-00007",
            "00002-of-00007",
            "00003-of-00007",
            "00004-of-00007",
            "00005-of-00007",
            "00006-of-00007",
            "00007-of-00007"
          ],
          "Q4_K_M": [
            "00001-of-00009",
            "00002-of-00009",
            "00003-of-00009",
            "00004-of-00009",
            "00005-of-00009",
            "00006-of-00009",
            "00007-of-00009",
            "00008-of-00009",
            "00009-of-00009"
          ],
          "Q5_K_M": [
            "00001-of-00010",
            "00002-of-00010",
            "00003-of-00010",
            "00004-of-00010",
            "00005-of-00010",
            "00006-of-00010",
            "00007-of-00010",
            "00008-of-00010",
            "00009-of-00010",
            "00010-of-00010"
          ],
          "Q6_K": [
            "00001-of-00012",
            "00002-of-00012",
            "00003-of-00012",
            "00004-of-00012",
            "00005-of-00012",
            "00006-of-00012",
            "00007-of-00012",
            "00008-of-00012",
            "00009-of-00012",
            "00010-of-00012",
            "00011-of-00012",
            "00012-of-00012"
          ],
          "Q8_0": [
            "00001-of-00016",
            "00002-of-00016",
            "00003-of-00016",
            "00004-of-00016",
            "00005-of-00016",
            "00006-of-00016",
            "00007-of-00016",
            "00008-of-00016",
            "00009-of-00016",
            "00010-of-00016",
            "00011-of-00016",
            "00012-of-00016",
            "00013-of-00016",
            "00014-of-00016",
            "00015-of-00016",
            "00016-of-00016"
          ]
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 671,
        "quantizations": [
          "3bit",
          "4bit"
        ],
        "model_id": "mlx-community/DeepSeek-V3-{quantization}",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% if messages %} {% if system or tools %} {% if system %} {{ system }} {% endif %} {% if tools %} {# Handle tools here if needed #} {% endif %} {% endif %} {% for message in messages %} {% set last = loop.index == loop.length %} {% if message.role == \"user\" %} <｜User｜> {% if tools and last %} Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.  Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. Do not use variables.  {{ tools }} {% endif %} {{ message.content }} {% if last %} <｜Assistant｜> {% endif %} {% elif message.role == \"assistant\" %} <｜Assistant｜> {% if message.tool_calls %} <｜tool▁calls▁begin｜> {% for tool in message.tool_calls %} <｜tool▁call▁begin｜> {\"name\": \"{{ tool.function.name }}\", \"parameters\": {{ tool.function.arguments }}} <｜tool▁call▁end｜> {% endfor %} <｜tool▁calls▁end｜> {% else %} {{ message.content }} {% if not last %} <｜end▁of▁sentence｜> {% endif %} {% endif %} {% elif message.role == \"tool\" %} <｜tool▁outputs▁begin｜> <｜tool▁output▁begin｜> {{ message.content }} <｜tool▁output▁end｜> <｜tool▁outputs▁end｜> {% if last and message.role != \"assistant\" %} <｜Assistant｜> {% endif %} {% endif %} {% endfor %} {% else %} {% if system %} {{ system }} {% endif %} {% if prompt %} <｜User｜> {{ prompt }} {% endif %} <｜Assistant｜> {{ response }} {% if response %} {{ response }} {% endif %} {% endif %}",
    "stop_token_ids": [
      1
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ]
  },
  {
    "version": 1,
    "context_length": 163840,
    "model_name": "deepseek-r1",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning"
    ],
    "model_description": "DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 671,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-R1",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 671,
        "quantizations": [
          "Int4"
        ],
        "model_id": "cognitivecomputations/DeepSeek-R1-awq",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 671,
        "quantizations": [
          "UD-IQ1_S",
          "UD-IQ1_M",
          "UD-IQ2_XXS",
          "UD-Q2_K_XL",
          "Q2_K",
          "Q2_K_L",
          "Q2_K_XS",
          "Q3_K_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "BF16"
        ],
        "model_id": "unsloth/DeepSeek-R1-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "DeepSeek-R1-{quantization}/DeepSeek-R1-{quantization}.gguf",
        "model_file_name_split_template": "DeepSeek-R1-{quantization}/DeepSeek-R1-{quantization}-{part}.gguf",
        "quantization_parts": {
          "UD-IQ1_S": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ],
          "UD-IQ1_M": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ],
          "UD-IQ2_XXS": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ],
          "UD-Q2_K_XL": [
            "00001-of-00005",
            "00002-of-00005",
            "00003-of-00005",
            "00004-of-00005",
            "00005-of-00005"
          ],
          "Q2_K": [
            "00001-of-00005",
            "00002-of-00005",
            "00003-of-00005",
            "00004-of-00005",
            "00005-of-00005"
          ],
          "Q2_K_L": [
            "00001-of-00005",
            "00002-of-00005",
            "00003-of-00005",
            "00004-of-00005",
            "00005-of-00005"
          ],
          "Q2_K_XS": [
            "00001-of-00005",
            "00002-of-00005",
            "00003-of-00005",
            "00004-of-00005",
            "00005-of-00005"
          ],
          "Q3_K_M": [
            "00001-of-00007",
            "00002-of-00007",
            "00003-of-00007",
            "00004-of-00007",
            "00005-of-00007",
            "00006-of-00007",
            "00007-of-00007"
          ],
          "Q4_K_M": [
            "00001-of-00009",
            "00002-of-00009",
            "00003-of-00009",
            "00004-of-00009",
            "00005-of-00009",
            "00006-of-00009",
            "00007-of-00009",
            "00008-of-00009",
            "00009-of-00009"
          ],
          "Q5_K_M": [
            "00001-of-00010",
            "00002-of-00010",
            "00003-of-00010",
            "00004-of-00010",
            "00005-of-00010",
            "00006-of-00010",
            "00007-of-00010",
            "00008-of-00010",
            "00009-of-00010",
            "00010-of-00010"
          ],
          "Q6_K": [
            "00001-of-00012",
            "00002-of-00012",
            "00003-of-00012",
            "00004-of-00012",
            "00005-of-00012",
            "00006-of-00012",
            "00007-of-00012",
            "00008-of-00012",
            "00009-of-00012",
            "00010-of-00012",
            "00011-of-00012",
            "00012-of-00012"
          ],
          "Q8_0": [
            "00001-of-00015",
            "00002-of-00015",
            "00003-of-00015",
            "00004-of-00015",
            "00005-of-00015",
            "00006-of-00015",
            "00007-of-00015",
            "00008-of-00015",
            "00009-of-00015",
            "00010-of-00015",
            "00011-of-00015",
            "00012-of-00015",
            "00013-of-00015",
            "00014-of-00015",
            "00015-of-00015"
          ],
          "BF16": [
            "00001-of-00030",
            "00002-of-00030",
            "00003-of-00030",
            "00004-of-00030",
            "00005-of-00030",
            "00006-of-00030",
            "00007-of-00030",
            "00008-of-00030",
            "00009-of-00030",
            "00010-of-00030",
            "00011-of-00030",
            "00012-of-00030",
            "00013-of-00030",
            "00014-of-00030",
            "00015-of-00030",
            "00016-of-00030",
            "00017-of-00030",
            "00018-of-00030",
            "00019-of-00030",
            "00020-of-00030",
            "00021-of-00030",
            "00022-of-00030",
            "00023-of-00030",
            "00024-of-00030",
            "00025-of-00030",
            "00026-of-00030",
            "00027-of-00030",
            "00028-of-00030",
            "00029-of-00030",
            "00030-of-00030"
          ]
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 671,
        "quantizations": [
          "2bit",
          "3bit",
          "4bit"
        ],
        "model_id": "mlx-community/DeepSeek-R1-{quantization}",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' in message %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- else %}{{'<｜Assistant｜>' + message['content'] + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- endfor %}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' not in message %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}",
    "stop_token_ids": [
      1
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>"
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-0.5B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-1.5B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 3,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-3B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-7B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-14B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-32B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-72B",
        "model_revision": "master",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2.5-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-0.5B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-1.5B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 3,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-3B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-7B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-14B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-32B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-72B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-0.5B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-1.5B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 3,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-3B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-7B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-14B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-32B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-72B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 3,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-3B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-7B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-14B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-32B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-72B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen2.5-0.5B-Instruct-GGUF",
        "model_file_name_template": "qwen2.5-0.5b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen2.5-1.5B-Instruct-GGUF",
        "model_file_name_template": "qwen2.5-1.5b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 3,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen2.5-3B-Instruct-GGUF",
        "model_file_name_template": "qwen2.5-3b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen2.5-7B-Instruct-GGUF",
        "model_file_name_template": "qwen2.5-7b-instruct-{quantization}.gguf",
        "model_hub": "modelscope",
        "model_file_name_split_template": "qwen2.5-7b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q4_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q4_k_m": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q5_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q5_k_m": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q6_k": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q8_0": [
            "00001-of-00002",
            "00002-of-00002"
          ]
        }
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen2.5-14B-Instruct-GGUF",
        "model_file_name_template": "qwen2.5-14b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2.5-14b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q2_k": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q3_k_m": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q4_0": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ],
          "q4_k_m": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ],
          "q5_0": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ],
          "q5_k_m": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ],
          "q6_k": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ],
          "q8_0": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ]
        },
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen2.5-32B-Instruct-GGUF",
        "model_file_name_template": "qwen2_5-32b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2.5-32b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q2_k": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ],
          "q3_k_m": [
            "00001-of-00005",
            "00002-of-00005",
            "00003-of-00005",
            "00004-of-00005",
            "00005-of-00005"
          ],
          "q4_0": [
            "00001-of-00005",
            "00002-of-00005",
            "00003-of-00005",
            "00004-of-00005",
            "00005-of-00005"
          ],
          "q4_k_m": [
            "00001-of-00005",
            "00002-of-00005",
            "00003-of-00005",
            "00004-of-00005",
            "00005-of-00005"
          ],
          "q5_0": [
            "00001-of-00006",
            "00002-of-00006",
            "00003-of-00006",
            "00004-of-00006",
            "00005-of-00006",
            "00006-of-00006"
          ],
          "q5_k_m": [
            "00001-of-00006",
            "00002-of-00006",
            "00003-of-00006",
            "00004-of-00006",
            "00005-of-00006",
            "00006-of-00006"
          ],
          "q6_k": [
            "00001-of-00007",
            "00002-of-00007",
            "00003-of-00007",
            "00004-of-00007",
            "00005-of-00007",
            "00006-of-00007",
            "00007-of-00007"
          ],
          "q8_0": [
            "00001-of-00009",
            "00002-of-00009",
            "00003-of-00009",
            "00004-of-00009",
            "00005-of-00009",
            "00006-of-00009",
            "00007-of-00009",
            "00008-of-00009",
            "00009-of-00009"
          ]
        },
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 72,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen2.5-72B-Instruct-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen2_5-72b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2.5-72b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q2_k": [
            "00001-of-00007",
            "00002-of-00007",
            "00003-of-00007",
            "00004-of-00007",
            "00005-of-00007",
            "00006-of-00007",
            "00007-of-00007"
          ],
          "q3_k_m": [
            "00001-of-00009",
            "00002-of-00009",
            "00003-of-00009",
            "00004-of-00009",
            "00005-of-00009",
            "00006-of-00009",
            "00007-of-00009",
            "00008-of-00009",
            "00009-of-00009"
          ],
          "q4_0": [
            "00001-of-00011",
            "00002-of-00011",
            "00003-of-00011",
            "00004-of-00011",
            "00005-of-00011",
            "00006-of-00011",
            "00007-of-00011",
            "00008-of-00011",
            "00009-of-00011",
            "00010-of-00011",
            "00011-of-00011"
          ],
          "q4_k_m": [
            "00001-of-00012",
            "00002-of-00012",
            "00003-of-00012",
            "00004-of-00012",
            "00005-of-00012",
            "00006-of-00012",
            "00007-of-00012",
            "00008-of-00012",
            "00009-of-00012",
            "00010-of-00012",
            "00011-of-00012",
            "00012-of-00012"
          ],
          "q5_0": [
            "00001-of-00013",
            "00002-of-00013",
            "00003-of-00013",
            "00004-of-00013",
            "00005-of-00013",
            "00006-of-00013",
            "00007-of-00013",
            "00008-of-00013",
            "00009-of-00013",
            "00010-of-00013",
            "00011-of-00013",
            "00012-of-00013",
            "00013-of-00013"
          ],
          "q5_k_m": [
            "00001-of-00014",
            "00002-of-00014",
            "00003-of-00014",
            "00004-of-00014",
            "00005-of-00014",
            "00006-of-00014",
            "00007-of-00014",
            "00008-of-00014",
            "00009-of-00014",
            "00010-of-00014",
            "00011-of-00014",
            "00012-of-00014",
            "00013-of-00014",
            "00014-of-00014"
          ],
          "q6_k": [
            "00001-of-00016",
            "00002-of-00016",
            "00003-of-00016",
            "00004-of-00016",
            "00005-of-00016",
            "00006-of-00016",
            "00007-of-00016",
            "00008-of-00016",
            "00009-of-00016",
            "00010-of-00016",
            "00011-of-00016",
            "00012-of-00016",
            "00013-of-00016",
            "00014-of-00016",
            "00015-of-00016",
            "00016-of-00016"
          ],
          "q8_0": [
            "00001-of-00021",
            "00002-of-00021",
            "00003-of-00021",
            "00004-of-00021",
            "00005-of-00021",
            "00006-of-00021",
            "00007-of-00021",
            "00008-of-00021",
            "00009-of-00021",
            "00010-of-00021",
            "00011-of-00021",
            "00012-of-00021",
            "00013-of-00021",
            "00014-of-00021",
            "00015-of-00021",
            "00016-of-00021",
            "00017-of-00021",
            "00018-of-00021",
            "00019-of-00021",
            "00020-of-00021",
            "00021-of-00021"
          ]
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 3,
        "quantizations": [
          "4bit"
        ],
        "model_id": "okwinds/Qwen2.5-3B-Instruct-MLX-4bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 3,
        "quantizations": [
          "8bit"
        ],
        "model_id": "okwinds/Qwen2.5-3B-Instruct-MLX-8bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "quantizations": [
          "4bit"
        ],
        "model_id": "okwinds/Qwen2.5-7B-Instruct-MLX-4bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "quantizations": [
          "8bit"
        ],
        "model_id": "okwinds/Qwen2.5-7B-Instruct-MLX-8bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 14,
        "quantizations": [
          "4bit"
        ],
        "model_id": "okwinds/Qwen2.5-14B-Instruct-MLX-4bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 14,
        "quantizations": [
          "8bit"
        ],
        "model_id": "okwinds/Qwen2.5-14B-Instruct-MLX-8bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "quantizations": [
          "2bit"
        ],
        "model_id": "okwinds/Qwen2.5-32B-Instruct-MLX-2bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "quantizations": [
          "4bit"
        ],
        "model_id": "okwinds/Qwen2.5-32B-Instruct-MLX-4bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "quantizations": [
          "8bit"
        ],
        "model_id": "okwinds/Qwen2.5-32B-Instruct-MLX-8bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "quantizations": [
          "2bit"
        ],
        "model_id": "okwinds/Qwen2.5-32B-Instruct-MLX-2bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "quantizations": [
          "4bit"
        ],
        "model_id": "okwinds/Qwen2.5-72B-Instruct-MLX-4bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "quantizations": [
          "8bit"
        ],
        "model_id": "okwinds/Qwen2.5-72B-Instruct-MLX-8bit",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "HuatuoGPT-o1-Qwen2.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "HuatuoGPT-o1 is a medical LLM designed for advanced medical reasoning. It generates a complex thought process, reflecting and refining its reasoning, before providing a final response.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "FreedomIntelligence/HuatuoGPT-o1-7B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "none"
        ],
        "model_id": "FreedomIntelligence/HuatuoGPT-o1-72B",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "HuatuoGPT-o1-LLaMA-3.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "HuatuoGPT-o1 is a medical LLM designed for advanced medical reasoning. It generates a complex thought process, reflecting and refining its reasoning, before providing a final response.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_id": "FreedomIntelligence/HuatuoGPT-o1-8B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_id": "FreedomIntelligence/HuatuoGPT-o1-70B",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\"' + arg_val + '\"' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n            {{- '\"parameters\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      128001,
      128008,
      128009
    ],
    "stop": [
      "<|end_of_text|>",
      "<|eot_id|>",
      "<|eom_id|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "DianJin-R1",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Tongyi DianJin is a financial intelligence solution platform built by Alibaba Cloud, dedicated to providing financial business developers with a convenient artificial intelligence application development environment.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "DianJin/DianJin-R1-7B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "none"
        ],
        "model_id": "DianJin/DianJin-R1-32B",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2.5-coder",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-0.5B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-1.5B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "3",
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-3B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-7B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-14B",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-32B",
        "model_revision": "master",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2.5-coder-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-0.5B-Instruct",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-1.5B-Instruct",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "3",
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-3B-Instruct",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-7B-Instruct",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-14B-Instruct",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "none"
        ],
        "model_id": "qwen/Qwen2.5-Coder-32B-Instruct",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-{quantization}",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-{quantization}",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 3,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-{quantization}",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-{quantization}",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-{quantization}",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-{quantization}",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 3,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-Coder-3B-Instruct-AWQ",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-Coder-7B-Instruct-AWQ",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-Coder-14B-Instruct-AWQ",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2.5-Coder-32B-Instruct-AWQ",
        "model_revision": "master",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF",
        "model_file_name_template": "qwen2.5-coder-1.5b-instruct-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen2.5-Coder-7B-Instruct-GGUF",
        "model_file_name_template": "qwen2.5-coder-7b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2.5-coder-7b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q4_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q4_k_m": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q5_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q5_k_m": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q6_k": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q8_0": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ]
        }
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "XiYanSQL-QwenCoder-2504",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "The XiYanSQL-QwenCoder models, as multi-dialect SQL base models, demonstrating robust SQL generation capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "XGenerationLab/XiYanSQL-QwenCoder-7B-2504",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "none"
        ],
        "model_id": "XGenerationLab/XiYanSQL-QwenCoder-32B-2504",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "QwQ-32B-Preview",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "QwQ-32B-Preview is an experimental research model developed by the Qwen Team, focused on advancing AI reasoning capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "none"
        ],
        "model_id": "Qwen/QwQ-32B-Preview",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "quantizations": [
          "4bit"
        ],
        "model_id": "okwinds/QwQ-32B-Preview-MLX-4bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "quantizations": [
          "8bit"
        ],
        "model_id": "okwinds/QwQ-32B-Preview-MLX-8bit",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "quantizations": [
          "Q3_K_L",
          "Q4_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "AI-ModelScope/QwQ-32B-Preview-GGUF",
        "model_file_name_template": "QwQ-32B-Preview-{quantization}.gguf",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "QwQ-32B",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "tools"
    ],
    "model_description": "QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "none"
        ],
        "model_id": "Qwen/QwQ-32B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/QwQ-32B-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "quantizations": [
          "3bit",
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_id": "mlx-community/QwQ-32B-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "quantizations": [
          "BF16",
          "IQ4_NL",
          "IQ4_XS",
          "Q2_K",
          "Q2_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_1",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "UD-IQ1_M",
          "UD-IQ1_S",
          "UD-IQ2_M",
          "UD-IQ2_XXS",
          "UD-IQ3_XXS",
          "UD-Q2_K_XL",
          "UD-Q3_K_XL",
          "UD-Q4_K_XL",
          "UD-Q5_K_XL",
          "UD-Q6_K_XL",
          "UD-Q8_K_XL",
          "Q8_0"
        ],
        "quantization_parts": {
          "BF16": [
            "00001-of-00002",
            "00002-of-00002"
          ]
        },
        "model_id": "unsloth/QwQ-32B-GGUF",
        "model_file_name_template": "QwQ-32B-{quantization}.gguf",
        "model_file_name_split_template": "BF16/QwQ-32B-{quantization}-{part}.gguf",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- '' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n  {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" and not message.tool_calls %}\n        {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n<think>\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>"
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "deepseek-r1-distill-qwen",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning"
    ],
    "model_description": "deepseek-r1-distill-qwen is distilled from DeepSeek-R1 based on Qwen",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF",
        "model_file_name_template": "DeepSeek-R1-Distill-Qwen-1.5B-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "3bit",
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_id": "mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "tclf90/deepseek-r1-distill-qwen-7b-gptq-int4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "F16"
        ],
        "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF",
        "model_file_name_template": "DeepSeek-R1-Distill-Qwen-7B-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "quantizations": [
          "3bit",
          "4bit",
          "6bit",
          "8bit"
        ],
        "model_id": "okwinds/DeepSeek-R1-Distill-Qwen-7B-MLX-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "F16"
        ],
        "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF",
        "model_file_name_template": "DeepSeek-R1-Distill-Qwen-14B-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 14,
        "quantizations": [
          "3bit",
          "4bit",
          "6bit",
          "8bit"
        ],
        "model_id": "okwinds/DeepSeek-R1-Distill-Qwen-14B-MLX-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_id": "tclf90/deepseek-r1-distill-qwen-32b-gptq-int4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "F16"
        ],
        "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF",
        "model_file_name_template": "DeepSeek-R1-Distill-Qwen-32B-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "quantizations": [
          "2bit",
          "3bit",
          "4bit",
          "6bit",
          "8bit"
        ],
        "model_id": "okwinds/DeepSeek-R1-Distill-Qwen-32B-MLX-{quantization}",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' in message %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- else %}{{'<｜Assistant｜>' + message['content'] + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- endfor %}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' not in message %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}",
    "stop_token_ids": [
      151643
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>"
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "deepseek-r1-distill-llama",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning"
    ],
    "model_description": "deepseek-r1-distill-llama is distilled from DeepSeek-R1 based on Llama",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "F16"
        ],
        "model_id": "unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF",
        "model_file_name_template": "DeepSeek-R1-Distill-Llama-8B-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "quantizations": [
          "3bit",
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_id": "okwinds/DeepSeek-R1-Distill-Llama-8B-MLX-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 70,
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "F16"
        ],
        "quantization_parts": {
          "Q6_K": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "Q8_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "F16": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ]
        },
        "model_id": "unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF",
        "model_file_name_template": "DeepSeek-R1-Distill-Qwen-7B-{quantization}.gguf",
        "model_file_name_split_template": "DeepSeek-R1-Distill-Llama-70B-{quantization}/DeepSeek-R1-Distill-Llama-70B-{quantization}-{part}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 70,
        "quantizations": [
          "3bit",
          "4bit",
          "6bit",
          "8bit"
        ],
        "model_id": "okwinds/DeepSeek-R1-Distill-Llama-70B-MLX-{quantization}",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}",
    "stop_token_ids": [
      151643
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>"
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "glm-edge-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The GLM-Edge series is our attempt to face the end-side real-life scenarios, which consists of two sizes of large-language dialogue models and multimodal comprehension models (GLM-Edge-1.5B-Chat, GLM-Edge-4B-Chat, GLM-Edge-V-2B, GLM-Edge-V-5B). Among them, the 1.5B / 2B model is mainly for platforms such as mobile phones and cars, and the 4B / 5B model is mainly for platforms such as PCs.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "none"
        ],
        "model_id": "ZhipuAI/glm-edge-1.5b-chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "4",
        "quantizations": [
          "none"
        ],
        "model_id": "ZhipuAI/glm-edge-4b-chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Q4_0",
          "Q4_1",
          "Q4_K",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_1",
          "Q5_K",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_file_name_template": "ggml-model-{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-edge-1.5b-chat-gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "F16"
        ],
        "model_file_name_template": "glm-edge-1.5B-chat-{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-edge-1.5b-chat-gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "4",
        "quantizations": [
          "Q4_0",
          "Q4_1",
          "Q4_K",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_1",
          "Q5_K",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_file_name_template": "ggml-model-{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-edge-4b-chat-gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "4",
        "quantizations": [
          "F16"
        ],
        "model_file_name_template": "glm-edge-4B-chat-{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-edge-4b-chat-gguf"
      }
    ],
    "chat_template": "{% for item in messages %}{% if item['role'] == 'system' %}<|system|>\n{{ item['content'] }}{% elif item['role'] == 'user' %}<|user|>\n{{ item['content'] }}{% elif item['role'] == 'assistant' %}<|assistant|>\n{{ item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>\n{% endif %}",
    "stop_token_ids": [
      59246,
      59253,
      59255
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "glm-edge-v",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "The GLM-Edge series is our attempt to face the end-side real-life scenarios, which consists of two sizes of large-language dialogue models and multimodal comprehension models (GLM-Edge-1.5B-Chat, GLM-Edge-4B-Chat, GLM-Edge-V-2B, GLM-Edge-V-5B). Among them, the 1.5B / 2B model is mainly for platforms such as mobile phones and cars, and the 4B / 5B model is mainly for platforms such as PCs.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "2",
        "quantizations": [
          "none"
        ],
        "model_id": "ZhipuAI/glm-edge-v-2b",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "5",
        "quantizations": [
          "none"
        ],
        "model_id": "ZhipuAI/glm-edge-v-5b",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "2",
        "quantizations": [
          "Q4_0",
          "Q4_1",
          "Q4_K",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_1",
          "Q5_K",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_file_name_template": "ggml-model-{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-edge-v-2b-gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "2",
        "quantizations": [
          "F16"
        ],
        "model_file_name_template": "glm-edge-v-2B-{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-edge-v-2b-gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "2",
        "quantizations": [
          "f16"
        ],
        "model_file_name_template": "mmproj-model-{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-edge-v-2b-gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "5",
        "quantizations": [
          "Q4_0",
          "Q4_1",
          "Q4_K",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_1",
          "Q5_K",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_file_name_template": "ggml-model-{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-edge-v-5b-gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "5",
        "quantizations": [
          "F16"
        ],
        "model_file_name_template": "glm-edge-v-5B-{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-edge-v-5b-gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "5",
        "quantizations": [
          "f16"
        ],
        "model_file_name_template": "mmproj-model-{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-edge-v-5b-gguf"
      }
    ],
    "chat_template": "{% for item in messages %}{% if item['role'] != 'system' %}<|{{ item['role'] }}|>\n{% for content in item['content'] %}{% if content['type'] == 'image' %}{% for _ in range(578) %}<|begin_of_image|>{% endfor %}{% elif content['type'] == 'text' %}{{ content['text'] }}{% endif %}{% endfor %}\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>\n{% endif %}",
    "stop_token_ids": [
      59246,
      59253,
      59255
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "QvQ-72B-Preview",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "QVQ-72B-Preview is an experimental research model developed by the Qwen team, focusing on enhancing visual reasoning capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "none"
        ],
        "model_id": "Qwen/QVQ-72B-Preview",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "quantizations": [
          "3bit",
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_id": "mlx-community/QVQ-72B-Preview-{quantization}",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "marco-o1",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "AIDC-AI/Marco-o1",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_1",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_1",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_file_name_template": "Marco-o1.{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "QuantFactory/Marco-o1-GGUF"
      }
    ],
    "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n\n你是一个经过良好训练的AI助手，你的名字是Marco-o1.由阿里国际数字商业集团的AI Business创造.\n        \n## 重要！！！！！\n当你回答问题时，你的思考应该在<Thought>内完成，<Output>内输出你的结果。\n<Thought>应该尽可能是英文，但是有2个特例，一个是对原文中的引用，另一个是是数学应该使用markdown格式，<Output>内的输出需要遵循用户输入的语言。\n        <|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "cogagent",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "The CogAgent-9B-20241220 model is based on GLM-4V-9B, a bilingual open-source VLM base model. Through data collection and optimization, multi-stage training, and strategy improvements, CogAgent-9B-20241220 achieves significant advancements in GUI perception, inference prediction accuracy, action space completeness, and task generalizability. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "9",
        "quantizations": [
          "none"
        ],
        "model_id": "ZhipuAI/cogagent-9b-20241220",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "internlm3-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "InternLM3 has open-sourced an 8-billion parameter instruction model, InternLM3-8B-Instruct, designed for general-purpose usage and advanced reasoning.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm3-8b-instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm3-8b-instruct-gptq-int4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm3-8b-instruct-awq",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm3-8b-instruct-gguf",
        "model_file_name_template": "internlm3-8b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "quantizations": [
          "4bit"
        ],
        "model_hub": "modelscope",
        "model_id": "mlx-community/internlm3-8b-instruct-{quantization}"
      }
    ],
    "chat_template": "{{ bos_token }}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
    "stop_token_ids": [
      2,
      128131
    ],
    "stop": [
      "</s>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 1010000,
    "model_name": "qwen2.5-instruct-1m",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Qwen2.5-1M is the long-context version of the Qwen2.5 series models, supporting a context length of up to 1M tokens.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "Qwen/Qwen2.5-7B-Instruct-1M",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none"
        ],
        "model_id": "Qwen/Qwen2.5-14B-Instruct-1M",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "moonlight-16b-a3b-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Kimi Muon is Scalable for LLM Training",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 3,
        "quantizations": [
          "none"
        ],
        "model_id": "moonshotai/Moonlight-16B-A3B-Instruct",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- for message in messages -%}{%- if loop.first and messages[0]['role'] != 'system' -%}<|im_system|>system<|im_middle|>You are a helpful assistant<|im_end|>{%- endif -%}{%- if message['role'] == 'system' -%}<|im_system|>{%- endif -%}{%- if message['role'] == 'user' -%}<|im_user|>{%- endif -%}{%- if message['role'] == 'assistant' -%}<|im_assistant|>{%- endif -%}{{ message['role'] }}<|im_middle|>{{message['content']}}<|im_end|>{%- endfor -%}{%- if add_generation_prompt -%}<|im_assistant|>assistant<|im_middle|>{%- endif -%}",
    "stop_token_ids": [
      163586
    ],
    "stop": [
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "fin-r1",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Fin-R1 is a large language model specifically designed for the field of financial reasoning",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "AI-ModelScope/Fin-R1",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "JunHowie/Fin-R1-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 7,
        "quantizations": [
          "FP8"
        ],
        "model_id": "JunHowie/Fin-R1-FP8-Dynamic",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "deepseek-vl2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 27,
        "activated_size_in_billions": "4_5",
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-vl2",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 16,
        "activated_size_in_billions": "2_8",
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-vl2-small",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 3,
        "activated_size_in_billions": 1,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-vl2-tiny",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "",
    "stop_token_ids": [
      1
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "seallms-v3",
    "model_lang": [
      "en",
      "zh",
      "id",
      "vi",
      "th",
      "ph",
      "ms",
      "mm",
      "kh",
      "la",
      "in"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "SeaLLMs - Large Language Models for Southeast Asia",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "none"
        ],
        "model_id": "SeaLLMs/SeaLLMs-v3-1.5B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "SeaLLMs/SeaLLMs-v3-7B-Chat",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\n' + system_message + '<|im_end|>\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\n' + content + '<|im_end|>\n<|im_start|>assistant\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\n' }}{% endif %}{% endfor %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "glm4-0414",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "The GLM family welcomes new members, the GLM-4-32B-0414 series models, featuring 32 billion parameters. Its performance is comparable to OpenAI’s GPT series and DeepSeek’s V3/R1 series",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "none"
        ],
        "model_id": "ZhipuAI/GLM-4-9B-0414",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "none"
        ],
        "model_id": "ZhipuAI/GLM-4-32B-0414",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 9,
        "quantizations": [
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_id": "mlx-community/GLM-4-9B-0414-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "quantizations": [
          "4bit",
          "8bit"
        ],
        "model_id": "mlx-community/GLM-4-32B-0414-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "IQ2_M",
          "IQ3_M",
          "IQ3_XS",
          "IQ3_XXS",
          "IQ4_NL",
          "IQ4_XS",
          "Q2_K",
          "Q2_K_L",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q3_K_XL",
          "Q4_0",
          "Q4_1",
          "Q4_K_L",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_L",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q6_K_L",
          "Q8_0",
          "bf16"
        ],
        "model_id": "bartowski/THUDM_GLM-4-9B-0414-GGUF",
        "model_file_name_template": "THUDM_GLM-4-9B-0414-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "quantizations": [
          "IQ2_M",
          "IQ2_S",
          "IQ2_XS",
          "IQ3_M",
          "IQ3_XS",
          "IQ3_XXS",
          "IQ4_NL",
          "IQ4_XS",
          "Q2_K",
          "Q2_K_L",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q3_K_XL",
          "Q4_0",
          "Q4_1",
          "Q4_K_L",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_L",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q6_K_L",
          "Q8_0"
        ],
        "model_id": "bartowski/THUDM_GLM-4-9B-0414-GGUF",
        "model_file_name_template": "THUDM_GLM-4-9B-0414-{quantization}.gguf",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "[gMASK]<sop>{%- if tools -%}<|system|>\n# 可用工具\n{% for tool in tools %}{%- set function = tool.function if tool.get(\"function\") else tool %}\n\n## {{ function.name }}\n\n{{ function | tojson(indent=4, ensure_ascii=False) }}\n在调用上述函数时，请使用 Json 格式表示调用的参数。{%- endfor %}{%- endif -%}{%- for msg in messages %}{%- if msg.role == 'system' %}<|system|>\n{{ msg.content }}{%- endif %}{%- endfor %}{%- for message in messages if message.role != 'system' %}{%- set role = message['role'] %}{%- set content = message['content'] %}{%- set meta = message.get(\"metadata\", \"\") %}{%- if role == 'user' %}<|user|>\n{{ content }}{%- elif role == 'assistant' and not meta %}<|assistant|>\n{{ content }}{%- elif role == 'assistant' and meta %}<|assistant|>{{ meta }} \n{{ content }}{%- elif role == 'observation' %}<|observation|>\n{{ content }}{%- endif %}{%- endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}",
    "stop_token_ids": [
      151329,
      151336,
      151338
    ],
    "stop": [
      "<|endoftext|>",
      "<|user|>",
      "<|observation|>"
    ],
    "virtualenv": {
      "packages": [
        "transformers>=4.51.3",
        "mlx-lm>=0.23.1 ; sys_platform=='darwin'",
        "numpy==1.26.4"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "Ovis2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Ovis (Open VISion) is a novel Multimodal Large Language Model (MLLM) architecture, designed to structurally align visual and textual embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 1,
        "quantizations": [
          "none"
        ],
        "model_id": "AIDC-AI/Ovis2-1B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_id": "AIDC-AI/Ovis2-2B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "none"
        ],
        "model_id": "AIDC-AI/Ovis2-4B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_id": "AIDC-AI/Ovis2-8B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 16,
        "quantizations": [
          "none"
        ],
        "model_id": "AIDC-AI/Ovis2-16B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_id": "AIDC-AI/Ovis2-34B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 2,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AIDC-AI/Ovis2-2B-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 4,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AIDC-AI/Ovis2-4B-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AIDC-AI/Ovis2-8B-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 16,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AIDC-AI/Ovis2-16B-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "AIDC-AI/Ovis2-34B-GPTQ-{quantization}",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
    "stop_token_ids": [
      151645,
      151643
    ],
    "stop": [
      "<|im_end|>",
      "<|endoftext|>"
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "skywork-or1-preview",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Skywork-OR1 (Open Reasoner 1) model series consists of powerful math and code reasoning models trained using large-scale rule-based reinforcement learning with carefully designed datasets and training recipes.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "none"
        ],
        "model_id": "Skywork/Skywork-OR1-32B-Preview",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4",
          "int8"
        ],
        "model_id": "JunHowie/Skywork-OR1-32B-Preview-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "Skywork/Skywork-OR1-7B-Preview",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "quantizations": [
          "IQ2_M",
          "IQ2_S",
          "IQ2_XS",
          "IQ3_M",
          "IQ3_XS",
          "IQ3_XXS",
          "IQ4_NL",
          "IQ4_XS",
          "Q2_K",
          "Q2_K_L",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q3_K_XL",
          "Q4_0",
          "Q4_1",
          "Q4_K_L",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_L",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q6_K_L",
          "Q8_0"
        ],
        "model_id": "bartowski/Skywork_Skywork-OR1-32B-Preview-GGUF",
        "model_file_name_template": "Skywork_Skywork-OR1-32B-Preview-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "IQ2_M",
          "IQ2_S",
          "IQ2_XS",
          "IQ3_M",
          "IQ3_XS",
          "IQ3_XXS",
          "IQ4_NL",
          "IQ4_XS",
          "Q2_K",
          "Q2_K_L",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q3_K_XL",
          "Q4_0",
          "Q4_1",
          "Q4_K_L",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_L",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q6_K_L",
          "Q8_0"
        ],
        "model_id": "bartowski/Skywork_Skywork-OR1-7B-Preview-GGUF",
        "model_file_name_template": "Skywork_Skywork-OR1-7B-Preview-{quantization}.gguf",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "skywork-or1",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "We release the final version of Skywork-OR1 (Open Reasoner 1) series of models, including",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "none"
        ],
        "model_id": "Skywork/Skywork-OR1-32B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int8",
          "Int4"
        ],
        "model_id": "JunHowie/Skywork-OR1-32B-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "Skywork/Skywork-OR1-7B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int8",
          "Int4"
        ],
        "model_id": "JunHowie/Skywork-OR1-7B-GPTQ-{quantization}",
        "model_hub": "modelscope"
      }
    ],
    "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}",
    "stop_token_ids": [
      151643
    ],
    "stop": [
      "<｜end▁of▁sentence｜>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>"
  },
  {
    "version": 1,
    "context_length": 40960,
    "model_name": "qwen3",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "reasoning",
      "hybrid",
      "tools"
    ],
    "model_description": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_6",
        "quantizations": [
          "none"
        ],
        "model_id": "Qwen/Qwen3-0.6B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": "0_6",
        "quantizations": [
          "fp8"
        ],
        "model_id": "Qwen/Qwen3-0.6B-FP8",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_6",
        "quantizations": [
          "Int8"
        ],
        "model_id": "Qwen/Qwen3-0.6B-GPTQ-Int8",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_6",
        "quantizations": [
          "Int4"
        ],
        "model_id": "JunHowie/Qwen3-0.6B-GPTQ-Int4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "0_6",
        "quantizations": [
          "3bit",
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_id": "mlx-community/Qwen3-0.6B-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_6",
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_1",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "BF16",
          "UD-IQ1_M",
          "UD-IQ1_S",
          "UD-IQ2_M",
          "UD-IQ2_XXS",
          "UD-IQ3_XXS",
          "UD-Q2_K_XL",
          "UD-Q3_K_XL",
          "UD-Q4_K_XL",
          "UD-Q5_K_XL",
          "UD-Q6_K_XL",
          "UD-Q8_K_XL",
          "IQ4_NL",
          "IQ4_XS"
        ],
        "model_id": "unsloth/Qwen3-0.6B-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "Qwen3-0.6B-{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_7",
        "quantizations": [
          "none"
        ],
        "model_id": "Qwen/Qwen3-1.7B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": "1_7",
        "quantizations": [
          "fp8"
        ],
        "model_id": "Qwen/Qwen3-1.7B-FP8",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_7",
        "quantizations": [
          "Int8"
        ],
        "model_id": "Qwen/Qwen3-1.7B-GPTQ-Int8",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_7",
        "quantizations": [
          "Int4"
        ],
        "model_id": "JunHowie/Qwen3-1.7B-GPTQ-Int4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "1_7",
        "quantizations": [
          "3bit",
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_id": "mlx-community/Qwen3-1.7B-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_7",
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_1",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "BF16",
          "UD-IQ1_M",
          "UD-IQ1_S",
          "UD-IQ2_M",
          "UD-IQ2_XXS",
          "UD-IQ3_XXS",
          "UD-Q2_K_XL",
          "UD-Q3_K_XL",
          "UD-Q4_K_XL",
          "UD-Q5_K_XL",
          "UD-Q6_K_XL",
          "UD-Q8_K_XL",
          "IQ4_NL",
          "IQ4_XS"
        ],
        "model_id": "unsloth/Qwen3-1.7B-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "Qwen3-1.7B-{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "none"
        ],
        "model_id": "Qwen/Qwen3-4B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 4,
        "quantizations": [
          "fp8"
        ],
        "model_id": "Qwen/Qwen3-4B-FP8",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 4,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen3-4B-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 4,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "JunHowie/Qwen3-4B-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 4,
        "quantizations": [
          "3bit",
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_id": "mlx-community/Qwen3-4B-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 4,
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_1",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "BF16",
          "UD-IQ1_M",
          "UD-IQ1_S",
          "UD-IQ2_M",
          "UD-IQ2_XXS",
          "UD-IQ3_XXS",
          "UD-Q2_K_XL",
          "UD-Q3_K_XL",
          "UD-Q4_K_XL",
          "UD-Q5_K_XL",
          "UD-Q6_K_XL",
          "UD-Q8_K_XL",
          "IQ4_NL",
          "IQ4_XS"
        ],
        "model_id": "unsloth/Qwen3-4B-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "Qwen3-4B-{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_id": "Qwen/Qwen3-8B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 8,
        "quantizations": [
          "fp8"
        ],
        "model_id": "Qwen/Qwen3-8B-FP8",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen3-8B-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "JunHowie/Qwen3-8B-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "quantizations": [
          "3bit",
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_id": "mlx-community/Qwen3-8B-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_1",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "BF16",
          "UD-IQ1_M",
          "UD-IQ1_S",
          "UD-IQ2_M",
          "UD-IQ2_XXS",
          "UD-IQ3_XXS",
          "UD-Q2_K_XL",
          "UD-Q3_K_XL",
          "UD-Q4_K_XL",
          "UD-Q5_K_XL",
          "UD-Q6_K_XL",
          "UD-Q8_K_XL",
          "IQ4_NL",
          "IQ4_XS"
        ],
        "model_id": "unsloth/Qwen3-8B-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "Qwen3-8B-{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none"
        ],
        "model_id": "Qwen/Qwen3-14B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 14,
        "quantizations": [
          "fp8"
        ],
        "model_id": "Qwen/Qwen3-14B-FP8",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen3-14B-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "JunHowie/Qwen3-14B-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 14,
        "quantizations": [
          "3bit",
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_id": "mlx-community/Qwen3-14B-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_1",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "BF16",
          "UD-IQ1_M",
          "UD-IQ1_S",
          "UD-IQ2_M",
          "UD-IQ2_XXS",
          "UD-IQ3_XXS",
          "UD-Q2_K_XL",
          "UD-Q3_K_XL",
          "UD-Q4_K_XL",
          "UD-Q5_K_XL",
          "UD-Q6_K_XL",
          "UD-Q8_K_XL",
          "IQ4_NL",
          "IQ4_XS"
        ],
        "model_id": "unsloth/Qwen3-14B-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "Qwen3-14B-{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "quantizations": [
          "none"
        ],
        "model_id": "Qwen/Qwen3-30B-A3B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "quantizations": [
          "fp8"
        ],
        "model_id": "Qwen/Qwen3-30B-A3B-FP8",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "quantizations": [
          "Int8"
        ],
        "model_id": "JunHowie/Qwen3-30B-A3B-GPTQ-Int8",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen3-30B-A3B-GPTQ-Int4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "quantizations": [
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_id": "mlx-community/Qwen3-30B-A3B-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 30,
        "activated_size_in_billions": 3,
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_1",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "BF16",
          "UD-IQ1_M",
          "UD-IQ1_S",
          "UD-IQ2_M",
          "UD-IQ2_XXS",
          "UD-IQ3_XXS",
          "UD-Q2_K_XL",
          "UD-Q3_K_XL",
          "UD-Q4_K_XL",
          "UD-Q5_K_XL",
          "UD-Q6_K_XL",
          "UD-Q8_K_XL",
          "IQ4_NL",
          "IQ4_XS"
        ],
        "quantization_parts": {
          "BF16": [
            "00001-of-00002",
            "00002-of-00002"
          ]
        },
        "model_id": "unsloth/Qwen3-30B-A3B-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "Qwen3-30B-A3B-{quantization}.gguf",
        "model_file_name_split_template": "BF16/Qwen3-30B-A3B-{quantization}-{part}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "none"
        ],
        "model_id": "Qwen/Qwen3-32B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 32,
        "quantizations": [
          "fp8"
        ],
        "model_id": "Qwen/Qwen3-32B-FP8",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen3-32B-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "JunHowie/Qwen3-32B-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 32,
        "quantizations": [
          "4bit",
          "6bit",
          "8bit",
          "bf16"
        ],
        "model_id": "mlx-community/Qwen3-32B-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_1",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "BF16",
          "UD-IQ1_M",
          "UD-IQ1_S",
          "UD-IQ2_M",
          "UD-IQ2_XXS",
          "UD-IQ3_XXS",
          "UD-Q2_K_XL",
          "UD-Q3_K_XL",
          "UD-Q4_K_XL",
          "UD-Q5_K_XL",
          "UD-Q6_K_XL",
          "UD-Q8_K_XL",
          "IQ4_NL",
          "IQ4_XS"
        ],
        "quantization_parts": {
          "BF16": [
            "00001-of-00002",
            "00002-of-00002"
          ]
        },
        "model_id": "unsloth/Qwen3-32B-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "Qwen3-32B-{quantization}.gguf",
        "model_file_name_split_template": "BF16/Qwen3-32B-{quantization}-{part}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "quantizations": [
          "none"
        ],
        "model_id": "Qwen/Qwen3-235B-A22B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "quantizations": [
          "fp8"
        ],
        "model_id": "Qwen/Qwen3-235B-A22B-FP8",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "quantizations": [
          "Int8"
        ],
        "model_id": "tclf90/Qwen3-235B-A22B-GPTQ-Int8",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen3-235B-A22B-GPTQ-Int4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "quantizations": [
          "3bit",
          "4bit",
          "8bit"
        ],
        "model_id": "mlx-community/Qwen3-235B-A22B-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 235,
        "activated_size_in_billions": 22,
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_1",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "BF16",
          "UD-Q2_K_XL",
          "UD-Q3_K_XL",
          "IQ4_NL",
          "IQ4_XS"
        ],
        "quantization_parts": {
          "BF16": [
            "00001-of-00010",
            "00002-of-00010",
            "00003-of-00010",
            "00004-of-00010",
            "00005-of-00010",
            "00006-of-00010",
            "00007-of-00010",
            "00008-of-00010",
            "00009-of-00010",
            "00010-of-00010"
          ],
          "IQ4_XS": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ],
          "Q2_K": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "Q2_K_L": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "Q3_K_S": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ],
          "Q4_0": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ],
          "Q4_1": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ],
          "Q5_K_M": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ],
          "Q6_K": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ],
          "Q8_0": [
            "00001-of-00006",
            "00002-of-00006",
            "00003-of-00006",
            "00004-of-00006",
            "00005-of-00006",
            "00006-of-00006"
          ],
          "UD-Q2_K_XL": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "UD-Q3_K_XL": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ]
        },
        "model_id": "unsloth/Qwen3-235B-A22B-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "Qwen3-235B-A22B-{quantization}.gguf",
        "model_file_name_split_template": "{quantization}/Qwen3-235B-A22B-{quantization}-{part}.gguf"
      }
    ],
    "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in message.content %}\n                {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n                {%- set reasoning_content = message.content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '<|im_start|>' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- '<think>\\n\\n</think>\\n\\n' }}\n    {%- endif %}\n{%- endif %}",
    "stop_token_ids": [
      151643,
      151644,
      151645
    ],
    "stop": [
      "<|endoftext|>",
      "<|im_start|>",
      "<|im_end|>"
    ],
    "reasoning_start_tag": "<think>",
    "reasoning_end_tag": "</think>",
    "virtualenv": {
      "packages": [
        "transformers>=4.51.0",
        "mlx-lm>=0.24.0 ; sys_platform=='darwin'",
        "numpy==1.26.4"
      ]
    }
  }
]
