export const llmAllDataKey = [
  'model_uid',
  'model_name',
  'model_type',
  'model_engine',
  'model_format',
  'model_size_in_billions',
  'quantization',
  'n_worker',
  'n_gpu',
  'n_gpu_layers',
  'replica',
  'request_limits',
  'worker_ip',
  'gpu_idx',
  'download_hub',
  'model_path',
  'reasoning_content',
  'gguf_quantization',
  'gguf_model_path',
  'lightning_version',
  'lightning_model_path',
  'cpu_offload',
  'peft_model_config',
  'quantization_config',
  'enable_thinking',
  'multimodal_projector',
  'enable_virtual_env',
  'virtual_env_packages',
  'envs',
]

export const additionalParameterTipList = {
  'transformers': ['torch_dtype', 'device', 'enable_flash_attn'],
  'llama.cpp': ['n_ctx', 'use_mmap', 'use_mlock'],
  'vllm': [
    'block_size',
    'gpu_memory_utilization',
    'max_num_seqs',
    'max_model_len',
    'guided_decoding_backend',
    'scheduling_policy',
    'tensor_parallel_size',
    'pipeline_parallel_size',
    'enable_prefix_caching',
    'enable_chunked_prefill',
    'enable_expert_parallel',
    'enforce_eager',
    'cpu_offload_gb',
    'disable_custom_all_reduce',
    'limit_mm_per_prompt',
    'model_quantization',
    'mm_processor_kwargs',
    'min_pixels',
    'max_pixels',
  ],
  'sglang': [
    'mem_fraction_static',
    'attention_reduce_in_fp32',
    'tp_size',
    'dp_size',
    'chunked_prefill_size',
    'cpu_offload_gb',
    'enable_dp_attention',
    'enable_ep_moe',
  ],
  'mlx': ['cache_limit_gb', 'max_kv_size'],
}

export const quantizationParametersTipList = [
  'load_in_8bit',
  'load_in_4bit',
  'llm_int8_threshold',
  'llm_int8_skip_modules',
  'llm_int8_enable_fp32_cpu_offload',
  'llm_int8_has_fp16_weight',
  'bnb_4bit_compute_dtype',
  'bnb_4bit_quant_type',
  'bnb_4bit_use_double_quant',
  'bnb_4bit_quant_storage',
]
