export const llmAllDataKey = [
  'model_uid',
  'model_name',
  'model_type',
  'model_engine',
  'model_format',
  'model_size_in_billions',
  'quantization',
  'n_gpu',
  'n_gpu_layers',
  'replica',
  'request_limits',
  'worker_ip',
  'gpu_idx',
  'download_hub',
  'model_path',
  'gguf_quantization',
  'gguf_model_path',
  'cpu_offload',
  'peft_model_config',
]

export const additionalParameterTipList = {
  'transformers': ['torch_dtype', 'device'],
  'llama.cpp': ['n_ctx', 'use_mmap', 'use_mlock'],
  'vllm': [
    'block_size',
    'gpu_memory_utilization',
    'max_num_seqs',
    'max_model_len',
    'guided_decoding_backend',
    'scheduling_policy',
    'tensor_parallel_size',
    'pipeline_parallel_size',
    'enable_prefix_cache',
    'enable_chunked_prefill',
    'enforce_eager',
    'cpu_offload_gb',
    'disable_custom_all_reduce',
  ],
  'sglang': [
    'mem_fraction_static',
    'attention_reduce_in_fp32',
    'tp_size',
    'dp_size',
    'chunked_prefill_size',
    'cpu_offload_gb',
    'enable_dp_attention',
    'enable_ep_moe',
  ],
  'mlx': ['cache_limit_gb', 'max_kv_size'],
}
